import argparse
import torch
import torch.nn.functional as F
import time
import psutil
import os
import csv
from datetime import datetime
from cts_model import CTSDualTowerModel
from cags_scheduler import CAGSStrategyLayer, CAGSCorrectionLayer
from real_sensor import RealSensor
from real_downloader import RealDownloader
import requests


# ==========================================
# 1. æ–°å¢ï¼šç®€æ˜“ç¼©æ”¾å™¨ (å¿…é¡»ä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒä¸€è‡´)
# ==========================================
class SimpleScaler:
    def transform(self, val, mean, std):
        return (val - mean) / (std + 1e-6)

    def inverse_transform(self, val, mean, std):
        return val * std + mean

# è¿™é‡Œçš„å‡å€¼å’Œæ–¹å·®æ˜¯åŸºäºä½ ä¹‹å‰ 4000 æ¡è®­ç»ƒæ•°æ®ä¼°ç®—çš„
# å¦‚æœä¸åŠ è¿™ä¸ªï¼ŒAI æ ¹æœ¬çœ‹ä¸æ‡‚è¾“å…¥çš„æ•°æ®
CLIENT_STATS = {
    'bandwidth_mbps': (20.0, 30.0),    # å¹³å‡å¸¦å®½ 20, æ³¢åŠ¨ 30
    'cpu_limit': (0.5, 0.3),     # CPU è´Ÿè½½
    'network_rtt': (50.0, 80.0),   # RTT
    'mem_limit_mb': (8.0, 4.0)      # å†…å­˜ (GB)
}
IMAGE_STATS = {
    'total_size_mb': (200.0, 150.0), 
    'avg_layer_entropy': (6.5, 1.0),
    'text_ratio': (0.1, 0.1),
    'layer_count': (10.0, 5.0),
    'zero_ratio': (0.05, 0.05)
}


def load_model(model_path):
    """
    åŠ è½½è®­ç»ƒå¥½çš„AIæ¨¡å‹
    """
    device = torch.device("cpu")
    model = CTSDualTowerModel(client_feats=4, image_feats=5, num_algos=10).to(device)
    
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            # å°è¯•å…¶ä»–å¯èƒ½çš„è·¯å¾„
            possible_paths = [
                model_path,
                '../ml_training/modeling/cts_best_model_full.pth',
                '../../ml_training/modeling/cts_best_model_full.pth',
                '../../../ml_training/modeling/cts_best_model_full.pth',
                os.path.join(os.path.dirname(__file__), '../ml_training/modeling/cts_best_model_full.pth')
            ]
            found_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    found_path = path
                    break
            
            if found_path is None:
                print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æ¼”ç¤º...")
                return None, device
            
            model_path = found_path
            print(f"ğŸ” æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
        
        # ä½¿ç”¨å®‰å…¨æ–¹å¼åŠ è½½æ¨¡å‹
        state_dict = torch.load(model_path, map_location=device, weights_only=True)
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, device
    except Exception as e:
        print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¼”ç¤º...")
        # ä¿®å¤å‚æ•°åç§°é”™è¯¯
        model = CTSDualTowerModel(client_feats=4, image_feats=5, num_algos=10).to(torch.device("cpu"))
        device = torch.device("cpu")
        return None, device


def calculate_uncertainty(beta, v, alpha):
    """
    è®¡ç®—ä¸ç¡®å®šæ€§ U = beta / (v * (alpha - 1))
    """
    return beta / (v * (alpha - 1) + 1e-6)  # é˜²æ­¢é™¤é›¶


def record_experiment_summary(mode, success, total_time, client_info, predicted_uncertainty=None, chunk_size=None, concurrency=None, output_file=None, top_algorithms=None):
    """
    è®°å½•å®éªŒæ‘˜è¦æ•°æ®åˆ°CSVæ–‡ä»¶
    """
    summary_file = "experiment_summary.csv"
    file_exists = os.path.isfile(summary_file)
    
    with open(summary_file, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        # å†™è¡¨å¤´
        if not file_exists:
            writer.writerow([
                "Timestamp", "Mode", "BW_Mbps", "RTT_ms", "CPU_Load", "Memory_GB",
                "Uncertainty", "Init_Chunk_MB", "Concurrency", "Total_Time_s", "Avg_Speed_MB_s", "Success",
                "Top_Algo_1", "Top_Algo_2", "Top_Algo_3"
            ])
        
        # æå–æ•°æ®
        bw = client_info['bandwidth_mbps'] if client_info else 0
        rtt = client_info['rtt_ms'] if client_info else 0
        cpu_load = client_info['cpu_load'] if client_info else 0
        memory_gb = client_info['memory_gb'] if client_info else 0
        uncert = predicted_uncertainty if predicted_uncertainty is not None else 0
        init_chunk_mb = chunk_size / (1024*1024) if chunk_size else 0
        avg_speed = 0
        
        # è·å–é¡¶çº§ç®—æ³•
        top_algo_1 = ""
        top_algo_2 = ""
        top_algo_3 = ""
        if top_algorithms:
            if len(top_algorithms) > 0:
                top_algo_1 = f"{top_algorithms[0][0]}({top_algorithms[0][1]:.2f}s)"
            if len(top_algorithms) > 1:
                top_algo_2 = f"{top_algorithms[1][0]}({top_algorithms[1][1]:.2f}s)"
            if len(top_algorithms) > 2:
                top_algo_3 = f"{top_algorithms[2][0]}({top_algorithms[2][1]:.2f}s)"
        
        # è®¡ç®—å¹³å‡é€Ÿåº¦
        if output_file and os.path.exists(output_file):
            try:
                file_size = os.path.getsize(output_file)
                avg_speed = (file_size / (1024*1024)) / total_time if total_time > 0 else 0
            except:
                avg_speed = 0
        
        writer.writerow([
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),  # Timestamp
            mode,  # Mode
            f"{bw:.2f}",  # BW_Mbps
            f"{rtt:.0f}",  # RTT_ms
            f"{cpu_load:.2f}",  # CPU_Load
            f"{memory_gb:.1f}",  # Memory_GB
            f"{uncert:.4f}",  # Uncertainty
            f"{init_chunk_mb:.2f}",  # Init_Chunk_MB
            concurrency or 0,  # Concurrency
            f"{total_time:.2f}",  # Total_Time_s
            f"{avg_speed:.2f}",  # Avg_Speed_MB_s
            "TRUE" if success else "FALSE",  # Success
            top_algo_1,  # Top_Algo_1
            top_algo_2,  # Top_Algo_2
            top_algo_3   # Top_Algo_3
        ])
    
    print(f"[Benchmark] ğŸ“ å®éªŒæ•°æ®å·²è®°å½•è‡³ {summary_file}")


def run_cags_mode(args, model, device):
    """
    CAGSè‡ªé€‚åº”ç­–ç•¥æ¨¡å¼
    """
    print("ğŸš€ è¿è¡Œ CAGS è‡ªé€‚åº”ç­–ç•¥æ¨¡å¼")
    
    # 1. æ„ŸçŸ¥ç¯å¢ƒ
    print("ğŸ” æ­£åœ¨æ„ŸçŸ¥ç½‘ç»œç¯å¢ƒ...")
    sensor = RealSensor(args.url)
    net_profile = sensor.get_network_profile()
    sys_profile = sensor.probe_system_info() # è·å–çœŸå®å†…å­˜ä¿¡æ¯
    # è·å–çœŸå®çš„CPUè´Ÿè½½
    real_cpu_load = psutil.cpu_percent(interval=None) / 100.0  # 0.0 ~ 1.0
    print(f"ğŸ“Š ç½‘ç»œæ¦‚å†µ: {net_profile}, CPUè´Ÿè½½: {real_cpu_load:.2f}")
    
    # è·å–æ–‡ä»¶çœŸå®å¤§å°ç”¨äºç‰¹å¾
    try:
        head_resp = requests.head(args.url, timeout=2)
        real_file_size_mb = int(head_resp.headers.get('Content-Length', 0)) / (1024*1024)
    except:
        real_file_size_mb = 100.0

    # 2. ç‰¹å¾æ„å»ºä¸æ ‡å‡†åŒ– (CRITICAL FIX)
    scaler = SimpleScaler()
    
    # Clientç‰¹å¾: [Bandwidth, CPU_Limit, RTT, Memory_Limit]
    raw_bw = net_profile['bandwidth_mbps']
    raw_rtt = net_profile['rtt_ms']
    raw_mem = sys_profile.get('total_memory_gb', 8.0)
    
    norm_bw = scaler.transform(raw_bw, CLIENT_STATS['bandwidth_mbps'][0], CLIENT_STATS['bandwidth_mbps'][1])
    norm_cpu = scaler.transform(real_cpu_load, CLIENT_STATS['cpu_limit'][0], CLIENT_STATS['cpu_limit'][1])
    norm_rtt = scaler.transform(raw_rtt, CLIENT_STATS['network_rtt'][0], CLIENT_STATS['network_rtt'][1])
    norm_mem = scaler.transform(raw_mem, CLIENT_STATS['mem_limit_mb'][0], CLIENT_STATS['mem_limit_mb'][1])
    
    client_vec = torch.FloatTensor([[
        norm_bw, 
        norm_cpu,   # ä½¿ç”¨åŠ¨æ€æ„ŸçŸ¥çš„CPUè´Ÿè½½
        norm_rtt, 
        norm_mem
    ]]).to(device)
    
    # Imageç‰¹å¾: [Total_Size, Entropy, Text, Layer, Zero]
    norm_size = scaler.transform(real_file_size_mb, IMAGE_STATS['total_size_mb'][0], IMAGE_STATS['total_size_mb'][1])
    image_vec = torch.FloatTensor([[
        norm_size, 
        0.0,  # å…¶ä»–ç‰¹å¾ç®€åŒ–å¤„ç†ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨çœŸå®çš„ä¼ æ„Ÿå™¨æ•°æ®
        0.0, 
        0.0, 
        0.0
    ]]).to(device)
    
    algo_vec = torch.LongTensor([0]).to(device)

    # AIæ¨ç†
    print("ğŸ¤– æ­£åœ¨è¿›è¡ŒAIæ¨ç†...")
    with torch.no_grad():
        preds = model(client_vec, image_vec, algo_vec)
        gamma, v, alpha, beta = preds[0]
        
        # è®¡ç®—ä¸ç¡®å®šæ€§
        uncertainty_val = calculate_uncertainty(beta, v, alpha)
        ai_uncertainty = min(1.0, max(0.0, uncertainty_val.item() / 10.0))
        
        # åæ ‡å‡†åŒ–é¢„æµ‹æ—¶é—´
        predicted_time_s = torch.expm1(gamma).item()
    
    print(f"ğŸ”® AIæ¨ç†ç»“æœ: é¢„æµ‹è€—æ—¶ {predicted_time_s:.2f}s, ä¸ç¡®å®šæ€§ {ai_uncertainty:.4f}")
    
    # æˆ˜ç•¥å±‚å†³ç­–
    print("ğŸ§  æ­£åœ¨è¿›è¡Œæˆ˜ç•¥å±‚å†³ç­–...")
    strategy = CAGSStrategyLayer()
    predicted_risk_prob = 0.05 if predicted_time_s > 60 else 0.01
    best_config, cost = strategy.optimize(
        raw_bw,  # ä½¿ç”¨åŸå§‹å¸¦å®½å€¼
        predicted_risk_prob, 
        real_cpu_load,  # ä½¿ç”¨çœŸå®çš„CPUè´Ÿè½½
        model_uncertainty=ai_uncertainty
    )
    chunk_size, concurrency = best_config
    
    print(f"ğŸ’¡ æˆ˜ç•¥å±‚å†³ç­–: å—å¤§å° {chunk_size/(1024*1024):.2f}MB, å¹¶å‘æ•° {concurrency}")
    
    # é¢„æµ‹ä¸åŒå‹ç¼©ç®—æ³•çš„æ—¶é—´å¹¶æ’åº
    client_profile = {
        'bandwidth_mbps': raw_bw,
        'cpu_score': 2000,  # å‡è®¾CPUè¯„åˆ†ä¸º2000
        'decompression_speed': 200  # å‡è®¾è§£å‹é€Ÿåº¦ä¸º200MB/s
    }
    image_profile = {
        'total_size_mb': real_file_size_mb,
        'avg_layer_entropy': 0.65
    }
    sorted_algorithms = strategy.predict_compression_times(client_profile, image_profile)
    
    print(f"[Benchmark] å‹ç¼©ç®—æ³•é¢„æµ‹æ—¶é—´æ’åº (å‰5): {sorted_algorithms[:5]}")
    
    # è·å–æ–‡ä»¶å¤§å°
    try:
        response = requests.head(args.url)
        file_size = int(response.headers.get('Content-Length', 0))
        if file_size == 0:
            print("âš ï¸ æ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä½¿ç”¨Rangeè¯·æ±‚è·å–")
            response = requests.get(args.url, headers={'Range': 'bytes=0-0'}, timeout=5)
            if response.status_code == 206:
                content_range = response.headers.get('Content-Range', '')
                if content_range:
                    file_size = int(content_range.split('/')[-1])
    except:
        print("âš ï¸ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨1GB")
        file_size = 1024 * 1024 * 1024  # 1GB
    
    # åˆå§‹åŒ–ä¿®æ­£å±‚
    correction = CAGSCorrectionLayer(initial_chunk_size=chunk_size)
    
    # æ‰§è¡Œä¸‹è½½
    downloader = RealDownloader(args.url, file_size, args.output_file)
    start_time = time.time()
    success, total_time = downloader.download_with_chunks(chunk_size, concurrency, correction)
    
    # è®°å½•å®éªŒæ•°æ®
    client_info = {
        'bandwidth_mbps': raw_bw,
        'rtt_ms': raw_rtt,
        'cpu_load': real_cpu_load,
        'memory_gb': raw_mem
    }
    record_experiment_summary(
        mode="CAGS", 
        success=success, 
        total_time=total_time, 
        client_info=client_info,
        predicted_uncertainty=ai_uncertainty,
        chunk_size=chunk_size,
        concurrency=concurrency,
        output_file=args.output_file,
        top_algorithms=sorted_algorithms[:3]  # è®°å½•å‰3ä¸ªæœ€ä½³ç®—æ³•
    )
    
    return success


def run_static_mode(args):
    """
    é™æ€ç­–ç•¥æ¨¡å¼ (æ¨¡æ‹ŸDocker)
    """
    print("ğŸ“¦ è¿è¡Œ Docker é™æ€ç­–ç•¥æ¨¡å¼ (å›ºå®š4MBå—ï¼Œ3å¹¶å‘)")
    
    # å›ºå®šå‚æ•°
    chunk_size = 4 * 1024 * 1024  # 4MB
    concurrency = 3
    
    # è·å–æ–‡ä»¶å¤§å°
    try:
        response = requests.head(args.url)
        file_size = int(response.headers.get('Content-Length', 0))
        if file_size == 0:
            print("âš ï¸ æ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä½¿ç”¨Rangeè¯·æ±‚è·å–")
            response = requests.get(args.url, headers={'Range': 'bytes=0-0'}, timeout=5)
            if response.status_code == 206:
                content_range = response.headers.get('Content-Range', '')
                if content_range:
                    file_size = int(content_range.split('/')[-1])
    except:
        print("âš ï¸ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨1GB")
        file_size = 1024 * 1024 * 1024  # 1GB
    
    # æ‰§è¡Œä¸‹è½½
    downloader = RealDownloader(args.url, file_size, args.output_file)
    start_time = time.time()
    success, total_time = downloader.download_with_chunks(chunk_size, concurrency)
    
    # è®°å½•å®éªŒæ•°æ®ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰
    client_info = {
        'bandwidth_mbps': 0,
        'rtt_ms': 0,
        'cpu_load': 0,
        'memory_gb': 0
    }
    record_experiment_summary(
        mode="STATIC", 
        success=success, 
        total_time=total_time, 
        client_info=client_info,
        chunk_size=chunk_size,
        concurrency=concurrency,
        output_file=args.output_file,
        top_algorithms=[('gzip-6', 10.0), ('zstd-3', 12.0)]  # é»˜è®¤ç®—æ³•
    )
    
    return success


def run_aimd_mode(args):
    """
    AIMDæ¨¡å¼ (å›ºå®šåˆå§‹å—å¤§å°ï¼ŒåŠ¨æ€è°ƒæ•´)
    """
    print("ğŸ”„ è¿è¡Œ AIMD åŠ¨æ€è°ƒæ•´æ¨¡å¼")
    
    # å›ºå®šåˆå§‹å‚æ•°
    chunk_size = 2 * 1024 * 1024  # 2MB
    concurrency = 4
    
    # è·å–æ–‡ä»¶å¤§å°
    try:
        response = requests.head(args.url)
        file_size = int(response.headers.get('Content-Length', 0))
        if file_size == 0:
            print("âš ï¸ æ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼Œå°è¯•ä½¿ç”¨Rangeè¯·æ±‚è·å–")
            response = requests.get(args.url, headers={'Range': 'bytes=0-0'}, timeout=5)
            if response.status_code == 206:
                content_range = response.headers.get('Content-Range', '')
                if content_range:
                    file_size = int(content_range.split('/')[-1])
    except:
        print("âš ï¸ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨1GB")
        file_size = 1024 * 1024 * 1024  # 1GB
    
    # åˆå§‹åŒ–ä¿®æ­£å±‚
    correction = CAGSCorrectionLayer(initial_chunk_size=chunk_size)
    
    # æ‰§è¡Œä¸‹è½½
    downloader = RealDownloader(args.url, file_size, args.output_file)
    start_time = time.time()
    success, total_time = downloader.download_with_chunks(chunk_size, concurrency, correction)
    
    # è®°å½•å®éªŒæ•°æ®
    client_info = {
        'bandwidth_mbps': 0,
        'rtt_ms': 0,
        'cpu_load': 0,
        'memory_gb': 0
    }
    record_experiment_summary(
        mode="AIMD", 
        success=success, 
        total_time=total_time, 
        client_info=client_info,
        chunk_size=chunk_size,
        concurrency=concurrency,
        output_file=args.output_file,
        top_algorithms=[('gzip-6', 10.0), ('lz4-fast', 11.0)]  # é»˜è®¤ç®—æ³•
    )
    
    return success


def main():
    parser = argparse.ArgumentParser(description='CAGS çœŸå®ç¯å¢ƒå®éªŒæ‰§è¡Œå™¨')
    parser.add_argument('--mode', choices=['cags', 'static', 'aimd'], 
                       required=True, help='è¿è¡Œæ¨¡å¼: cags(è‡ªé€‚åº”), static(é™æ€), aimd(AIMDåŠ¨æ€)')
    parser.add_argument('--url', type=str, required=True, 
                       help='ä¸‹è½½URL')
    parser.add_argument('--output-file', type=str, default='downloaded_file.bin',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model-path', type=str, 
                       default='../ml_training/modeling/cts_best_model_full.pth',
                       help='AIæ¨¡å‹è·¯å¾„')
    
    args = parser.parse_args()
    
    print(f"ğŸ¯ ç›®æ ‡URL: {args.url}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {args.output_file}")
    print(f"âš™ï¸  è¿è¡Œæ¨¡å¼: {args.mode}")
    
    # ç›‘æ§ç³»ç»Ÿèµ„æº
    def monitor_resources():
        while True:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            print(f"ğŸ–¥ï¸  CPU: {cpu_percent}%, å†…å­˜: {memory_percent}%")
            time.sleep(5)
    
    # å¯åŠ¨èµ„æºç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitor_resources, daemon=True)
    monitor_thread.start()
    
    if args.mode == 'cags':
        # åŠ è½½æ¨¡å‹
        model, device = load_model(args.model_path)
        if model is None:
            # å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„æ¨¡å‹ç”¨äºæ¼”ç¤º
            # ä¿®å¤å‚æ•°åç§°é”™è¯¯
            model = CTSDualTowerModel(client_feats=4, image_feats=5, num_algos=10).to(torch.device("cpu"))
            device = torch.device("cpu")
            print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
        success = run_cags_mode(args, model, device)
    elif args.mode == 'static':
        success = run_static_mode(args)
    elif args.mode == 'aimd':
        success = run_aimd_mode(args)
    
    if success:
        print("âœ… ä¸‹è½½æˆåŠŸå®Œæˆ!")
    else:
        print("âŒ ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜!")


if __name__ == "__main__":
    import threading
    main()