from flask import Flask, request, jsonify
import torch
import numpy as np
import sys
import os

# å¯¼å…¥ç°æœ‰çš„æ¨¡å‹å’Œè°ƒåº¦å™¨
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from cts_model import CTSDualTowerModel
from cags_scheduler import CAGSStrategyLayer

app = Flask(__name__)

# ==========================================
# 0. å…¨å±€é…ç½®
# ==========================================
# âš ï¸ æ›¿æ¢ä¸ºä½ çš„çœŸå®å…¬ç½‘ IP
MY_PUBLIC_IP = "47.121.137.243" 
DEFAULT_DOWNLOAD_URL = f"http://{MY_PUBLIC_IP}/real_test.bin"

# ==========================================
# 1. ç‰¹å¾æ ‡å‡†åŒ–å™¨
# ==========================================
class SimpleScaler:
    def transform(self, val, mean, std):
        return (val - mean) / (std + 1e-6)

# è¿™é‡Œçš„å‡å€¼å’Œæ–¹å·®æ˜¯åŸºäºè®­ç»ƒæ•°æ®ä¼°ç®—çš„
CLIENT_STATS = {
    'bandwidth_mbps': (20.0, 30.0), 
    'cpu_load': (0.5, 0.3),          
    'network_rtt': (50.0, 80.0),      
    'memory_gb': (8.0, 4.0)          
}
IMAGE_STATS = {
    'total_size_mb': (200.0, 150.0), 
    'avg_layer_entropy': (6.5, 1.0),
    'text_ratio': (0.1, 0.1),
    'layer_count': (10.0, 5.0),
    'zero_ratio': (0.05, 0.05)
}

# ==========================================
# 2. å…¨å±€æ¨¡å‹åŠ è½½
# ==========================================
print("ğŸ”„ æ­£åœ¨åŠ è½½AIæ¨¡å‹...")
device = torch.device("cpu")
# æ³¨æ„ï¼šç¡®ä¿å‚æ•°ä¸ train.py ä¸€è‡´ (client=4, image=5)
model = CTSDualTowerModel(client_feats=4, image_feats=5, num_algos=10).to(device)

try:
    # è‡ªåŠ¨å¯»æ‰¾æ¨¡å‹è·¯å¾„
    possible_paths = [
        "cts_best_model_full.pth",
        "../ml_training/modeling/cts_best_model_full.pth",
        "../../ml_training/modeling/cts_best_model_full.pth"
    ]
    model_path = next((p for p in possible_paths if os.path.exists(p)), None)
    
    if model_path:
        print(f"ğŸ“¥ åŠ è½½æƒé‡æ–‡ä»¶: {model_path}")
        # ä½¿ç”¨å®‰å…¨æ–¹å¼åŠ è½½æ¨¡å‹
        state_dict = torch.load(model_path, map_location=device, weights_only=True)
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        print("âœ… AIæ¨¡å‹åŠ è½½æˆåŠŸï¼")
    else:
        raise FileNotFoundError("æœªæ‰¾åˆ° .pth æ¨¡å‹æ–‡ä»¶")
        
except Exception as e:
    print(f"âŒ AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    print("âš ï¸  [æ¼”ç¤ºæ¨¡å¼] ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¨¡å‹ç»§ç»­è¿è¡Œ...")

def calculate_uncertainty(beta, v, alpha):
    """è®¡ç®—ä¸ç¡®å®šæ€§ U"""
    return beta / (v * (alpha - 1) + 1e-6) 

@app.route('/negotiate', methods=['POST'])
def negotiate_strategy():
    """
    æ ¸å¿ƒæ¥å£ï¼šæ¥æ”¶å®¢æˆ·ç«¯ä¿¡æ¯ -> AI æ¨ç† -> æˆ˜ç•¥å†³ç­– -> è¿”å›ç­–ç•¥
    """
    try:
        data = request.json
        client_info = data.get('client_info', {})
        image_info = data.get('image_info', {})
        server_info = data.get('server_info', {})
        
        print(f"\n[Server] æ”¶åˆ°è¯·æ±‚ | BW:{client_info.get('bandwidth_mbps')} | CPU:{client_info.get('cpu_load')}")
        
        # --- A. ç‰¹å¾é¢„å¤„ç† ---
        scaler = SimpleScaler()
        
        raw_bw = float(client_info.get('bandwidth_mbps', 10.0))
        raw_cpu = float(client_info.get('cpu_load', 0.5))
        raw_rtt = float(client_info.get('rtt_ms', 50.0))
        raw_mem = float(client_info.get('memory_gb', 4.0))
        
        # æ ‡å‡†åŒ–
        norm_bw = scaler.transform(raw_bw, *CLIENT_STATS['bandwidth_mbps'])
        norm_cpu = scaler.transform(raw_cpu, *CLIENT_STATS['cpu_load'])
        norm_rtt = scaler.transform(raw_rtt, *CLIENT_STATS['network_rtt'])
        norm_mem = scaler.transform(raw_mem, *CLIENT_STATS['memory_gb'])
        
        client_vec = torch.FloatTensor([[norm_bw, norm_cpu, norm_rtt, norm_mem]]).to(device)
        
        # Image ç‰¹å¾
        raw_size = float(image_info.get('size_mb', 100.0))
        norm_size = scaler.transform(raw_size, *IMAGE_STATS['total_size_mb'])
        # å…¶ä»–ç‰¹å¾æš‚æ—¶ç»™é»˜è®¤å€¼ï¼Œå¦‚æœ smart_client ä¼ äº†å¯ä»¥æ¥
        image_vec = torch.FloatTensor([[norm_size, 0.5, 0.1, 5.0, 0.05]]).to(device)
        algo_vec = torch.LongTensor([0]).to(device)

        # --- B. AI æ¨ç† ---
        with torch.no_grad():
            preds = model(client_vec, image_vec, algo_vec)
            gamma, v, alpha, beta = preds[0]
            
            uncertainty_val = calculate_uncertainty(beta, v, alpha)
            ai_uncertainty = min(1.0, max(0.0, uncertainty_val.item() / 10.0))
            predicted_time_s = torch.expm1(gamma).item()
        
        print(f"[AI Brain] é¢„æµ‹è€—æ—¶: {predicted_time_s:.2f}s | ä¸ç¡®å®šæ€§ U: {ai_uncertainty:.4f}")
        
        # --- C. æˆ˜ç•¥å±‚å†³ç­– (Chunk & Concurrency) ---
        strategy = CAGSStrategyLayer()
        
        # ä¼°ç®—ä¸¢åŒ…ç‡é£é™© (RTTè¶Šé«˜ï¼Œä¸¢åŒ…é£é™©è¶Šå¤§)
        predicted_risk_prob = 0.05 if predicted_time_s > 60 else 0.01
        
        best_config, cost = strategy.optimize(
            predicted_bw_mbps=raw_bw, 
            predicted_loss_rate=predicted_risk_prob, 
            client_cpu_load=raw_cpu, 
            model_uncertainty=ai_uncertainty
        )
        chunk_size, concurrency = best_config
        
        print(f"[Strategy] å†³ç­–: åˆ†ç‰‡ {chunk_size/1024:.0f}KB | å¹¶å‘ {concurrency}")
        
        # --- D. å‹ç¼©ç®—æ³•æ’åº (ä½ çš„æ–°åŠŸèƒ½) ---
        # æ„é€ ç”¨äºæ’åºçš„ profile
        c_profile = {'bandwidth_mbps': raw_bw, 'cpu_score': 2000, 'decompression_speed': 200}
        i_profile = {'total_size_mb': raw_size, 'avg_layer_entropy': 0.65}
        
        # æ£€æŸ¥ strategy æ˜¯å¦æœ‰è¿™ä¸ªæ–¹æ³• (é˜²æ­¢æŠ¥é”™)
        if hasattr(strategy, 'predict_compression_times'):
            sorted_algorithms = strategy.predict_compression_times(c_profile, i_profile)
        else:
            sorted_algorithms = [('default', 0.0)] # Fallback
        
        # --- E. è¿”å›å“åº” ---
        response_data = {
            'target_url': server_info.get('download_url', DEFAULT_DOWNLOAD_URL),
            'strategy': {
                'initial_chunk_size': int(chunk_size),
                'concurrency': int(concurrency)
            },
            'meta_info': {
                'predicted_time_s': predicted_time_s,
                'uncertainty': ai_uncertainty,
                'cost': float(cost),
                'top_algorithms': sorted_algorithms[:5] # å‰5å
            }
        }
        
        return jsonify(response_data)
    
    except Exception as e:
        print(f"âŒ [Server] å†³ç­–å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()
        # è¿”å›ä¿åº•ç­–ç•¥
        return jsonify({
            'target_url': DEFAULT_DOWNLOAD_URL,
            'strategy': {'initial_chunk_size': 1024*1024, 'concurrency': 2},
            'meta_info': {'error': str(e), 'uncertainty': 1.0}
        })

if __name__ == '__main__':
    # ç›‘å¬ 0.0.0.0 æ‰èƒ½è¢«å…¬ç½‘è®¿é—®
    app.run(host='0.0.0.0', port=5000, debug=False)