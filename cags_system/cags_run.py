import torch
import torch.nn as nn
import torch.nn.functional as F

class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
    def forward(self, x):
        return x.unsqueeze(-1) * self.weights + self.biases

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4,
            batch_first=True, dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, x):
        tokens = self.tokenizer(x)
        batch_size = x.shape[0]
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        tokens = torch.cat((cls_tokens, tokens), dim=1)
        out = self.transformer(tokens)
        return out[:, 0, :]

class CTSDualTowerModel(nn.Module):
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = TransformerTower(client_feats, embed_dim)
        self.image_tower = TransformerTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
        fusion_input_dim = embed_dim * 3 
        self.hidden = nn.Sequential(
            nn.Linear(fusion_input_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        self.head = nn.Linear(64, 4) 

    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        hidden = self.hidden(combined)
        out = self.head(hidden)
        
        gamma = out[:, 0]
        v     = F.softplus(out[:, 1]) + 1e-6
        alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
        beta  = F.softplus(out[:, 3]) + 1e-6
        
        return torch.stack([gamma, v, alpha, beta], dim=1)
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
    def forward(self, x):
        return x.unsqueeze(-1) * self.weights + self.biases

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4,
            batch_first=True, dropout=0.2
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, x):
        tokens = self.tokenizer(x)
        batch_size = x.shape[0]
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        tokens = torch.cat((cls_tokens, tokens), dim=1)
        out = self.transformer(tokens)
        return out[:, 0, :]

class CTSDualTowerModel(nn.Module):
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = TransformerTower(client_feats, embed_dim)
        self.image_tower = TransformerTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
        fusion_input_dim = embed_dim * 3 
        self.hidden = nn.Sequential(
            nn.Linear(fusion_input_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        # è¾“å‡ºå±‚å‡çº§ä¸º 4 ä¸ªç¥ç»å…ƒ (Gamma, v, Alpha, Beta)
        self.head = nn.Linear(64, 4) 

    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        hidden = self.hidden(combined)
        out = self.head(hidden)
        
        # æ–½åŠ æ•°å­¦çº¦æŸ (Softplus)
        gamma = out[:, 0]
        v     = F.softplus(out[:, 1]) + 1e-6
        alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
        beta  = F.softplus(out[:, 3]) + 1e-6
        
        return torch.stack([gamma, v, alpha, beta], dim=1)
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
import os
import sys

# === 1. å¼•å…¥ CAGS è°ƒåº¦å™¨æ ¸å¿ƒç±» ===
from cags_scheduler import CAGSStrategyLayer, CAGSTacticalLayer, CAGSCorrectionLayer

# ==============================================================================
# === 2. ç²˜è´´æ¨¡å‹å®šä¹‰ (AIéƒ¨åˆ†) - å¿…é¡»ä¸ train.py å’Œ experiment_graph_ai.py 100% ä¸€è‡´ ===
# ==============================================================================
class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
    def forward(self, x):
        return x.unsqueeze(-1) * self.weights + self.biases

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4,
            batch_first=True, dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, x):
        tokens = self.tokenizer(x)
        batch_size = x.shape[0]
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        tokens = torch.cat((cls_tokens, tokens), dim=1)
        out = self.transformer(tokens)
        return out[:, 0, :]

class CTSDualTowerModel(nn.Module):
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = TransformerTower(client_feats, embed_dim)
        self.image_tower = TransformerTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
        fusion_input_dim = embed_dim * 3 
        self.hidden = nn.Sequential(
            nn.Linear(fusion_input_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        # [ä¿®æ”¹ç‚¹ 1] è¾“å‡ºå±‚å‡çº§ä¸º 4 ä¸ªç¥ç»å…ƒ (Gamma, v, Alpha, Beta)
        self.head = nn.Linear(64, 4) 

    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        hidden = self.hidden(combined)
        out = self.head(hidden)
        
        # [ä¿®æ”¹ç‚¹ 2] æ–½åŠ æ•°å­¦çº¦æŸ (Softplus)
        gamma = out[:, 0]
        v     = F.softplus(out[:, 1]) + 1e-6
        alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
        beta  = F.softplus(out[:, 3]) + 1e-6
        
        return torch.stack([gamma, v, alpha, beta], dim=1)

# ==============================================================================
# ğŸš€ ä¸»ç¨‹åºï¼šAI é©±åŠ¨çš„ CAGS ä»¿çœŸ (å«ä¸ç¡®å®šæ€§æ¼”ç¤º)
# ==============================================================================
def run_cags_simulation():
    print("="*60)
    print("ğŸš€ å¯åŠ¨ CAGS è‡ªé€‚åº”æµæ°´çº¿ä¼ è¾“ç³»ç»Ÿ (Uncertainty-Aware AI Mode)")
    print("="*60)

    # ---------------------------------------------------------
    # Step 1: åŠ è½½è®­ç»ƒå¥½çš„å¤§è„‘ (CFT-Net)
    # ---------------------------------------------------------
    device = torch.device("cpu") 
    
    # ---------------------------------------------------------
    # Step 1: åŠ è½½è®­ç»ƒå¥½çš„å¤§è„‘ (CFT-Net)
    # ---------------------------------------------------------
    device = torch.device("cpu") 
    
    # æ›´æ–°å¹¶æ‰©å±•å¯èƒ½çš„æ¨¡å‹è·¯å¾„
    possible_paths = [
        "cts_best_model_full.pth",
        "ml_training/modeling/cts_best_model_full.pth",      # æ–°å¢ï¼šé¡¹ç›®æ ¹ç›®å½•ä¸‹
        "../ml_training/modeling/cts_best_model_full.pth",   # åŸæœ‰
        "../../ml_training/modeling/cts_best_model_full.pth", # æ–°å¢ï¼šæ›´æ·±ä¸€å±‚
        "../cags_system/ml_training/modeling/cts_best_model_full.pth" # å¸¸è§IDEç»“æ„
    ]
    
    # ä½¿ç”¨ next() å’Œç”Ÿæˆå™¨è¡¨è¾¾å¼ç®€åŒ–è·¯å¾„æŸ¥æ‰¾
    model_path = next((p for p in possible_paths if os.path.exists(p)), None)

    # ç»Ÿä¸€çš„æ¨¡å‹åˆå§‹åŒ–
    model = CTSDualTowerModel(client_feats=4, image_feats=5, num_algos=10).to(device)
    ai_uncertainty = 0.5  # é»˜è®¤ä¸ç¡®å®šæ€§

    if model_path:
        print(f"ğŸ“¥ æ­£åœ¨åŠ è½½ AI æ¨¡å‹: {model_path} ...")
        try:
            state_dict = torch.load(model_path, map_location=device)
            # ç»´åº¦æ£€æŸ¥
            if state_dict['head.weight'].shape[0] == 4:
                model.load_state_dict(state_dict)
                model.eval()
                print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼EDL ä¸ç¡®å®šæ€§æ¨æ–­å·²å°±ç»ªã€‚")
            else:
                print("âš ï¸ æ£€æµ‹åˆ°æ—§æ¨¡å‹æƒé‡ã€‚åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼ã€‚")
                ai_uncertainty = 0.8
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨éšæœºæƒé‡æ¼”ç¤º...")
            ai_uncertainty = 0.8
    else:
        print("âš ï¸ [æ¼”ç¤ºæ¨¡å¼] æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œä½¿ç”¨éšæœºæƒé‡æ¼”ç¤º...")

    # ---------------------------------------------------------
    # Step 2: æ„é€ è¾“å…¥åœºæ™¯
    # ---------------------------------------------------------
    print("\nğŸŒ [ç¯å¢ƒæ„ŸçŸ¥] æ­£åœ¨é‡‡é›†ä¸Šä¸‹æ–‡ç‰¹å¾...")
    # æ„é€ ï¼šå¸¦å®½ä½ (5Mbps) + æ–‡ä»¶å¤§ (1.5GB) -> é¢„æœŸé£é™©è¾ƒé«˜
    client_vec = torch.FloatTensor([[5.0, 0.8, 200.0, 1024.0]])
    image_vec = torch.FloatTensor([[1500.0, 0.8, 0.1, 5.0, 0.1]])
    algo_vec = torch.LongTensor([0])

    # ---------------------------------------------------------
    # Step 3: AI æ¨ç† (è®¡ç®—ä¸ç¡®å®šæ€§)
    # ---------------------------------------------------------
    with torch.no_grad():
        preds = model(client_vec, image_vec, algo_vec)
        gamma, v, alpha, beta = preds[0]
        
        # [ä¿®æ”¹ç‚¹ 3] è®¡ç®—ä¸ç¡®å®šæ€§
        # Uncertainty = Beta / (v * (Alpha - 1))
        uncertainty_val = beta / (v * (alpha - 1))
        predicted_time_s = torch.expm1(gamma).item()
        
        # æ›´æ–° AI ä¸ç¡®å®šæ€§
        ai_uncertainty = min(1.0, max(0.0, uncertainty_val.item() / 10.0)) # å½’ä¸€åŒ–

    predicted_risk_prob = 0.05 if predicted_time_s > 60 else 0.01
    predicted_bw = 5.0 
    
    print(f"ğŸ¤– [AI æ¨ç†ç»“æœ]")
    print(f"   ğŸ‘‰ é¢„æµ‹è€—æ—¶: {predicted_time_s:.2f} ç§’")
    print(f"   ğŸ‘‰ æ¨¡å‹ä¸ç¡®å®šæ€§ (Uncertainty): {ai_uncertainty:.4f}") # æ‰“å°å‡ºæ¥ç»™è€å¸ˆçœ‹

    # ---------------------------------------------------------
    # Step 4: æˆ˜ç•¥å±‚å†³ç­– (ä¼ å…¥ä¸ç¡®å®šæ€§)
    # ---------------------------------------------------------
    strategy = CAGSStrategyLayer()
    
    # [ä¿®æ”¹ç‚¹ 4] ä¼ å…¥ model_uncertainty
    # å³ä½¿ risk_prob å¾ˆä½ï¼Œå¦‚æœ uncertainty å¾ˆé«˜ï¼Œä¹Ÿä¼šè§¦å‘é£é™©æ”¾å¤§
    best_config, cost = strategy.optimize(predicted_bw, predicted_risk_prob, 0.8, model_uncertainty=ai_uncertainty)
    chunk_size, concurrency = best_config

    print(f"\nğŸ’¡ [æˆ˜ç•¥å±‚] åŸºäº AI é¢„æµ‹ (å«ä¸ç¡®å®šæ€§åŠ æƒ) çš„æ•ˆç”¨å†³ç­–")
    print(f"   ğŸ‘‰ æœ€ä¼˜åˆ‡ç‰‡: {chunk_size/1024} KB")
    print(f"   ğŸ‘‰ æœ€ä¼˜å¹¶å‘: {concurrency} çº¿ç¨‹")
    
    if ai_uncertainty > 0.5:
        print(f"   ğŸ‘‰ å†³ç­–ç†ç”±: æ¨¡å‹ä¸ç¡®å®šæ€§è¾ƒé«˜ ({ai_uncertainty:.2f})ï¼Œç³»ç»Ÿè‡ªåŠ¨å¯ç”¨äº†ã€é£é™©æ”¾å¤§æœºåˆ¶ã€‘ï¼Œå€¾å‘äºä¿å®ˆé…ç½®ã€‚")
    elif predicted_risk_prob > 0.02:
        print(f"   ğŸ‘‰ å†³ç­–ç†ç”±: AI é¢„æµ‹è€—æ—¶è¿‡é•¿ï¼Œåˆ¤å®šä¸ºé«˜é£é™©ç¯å¢ƒï¼Œå¼ºåˆ¶é€‰æ‹©ç¨³å¥ç²’åº¦ã€‚")
    else:
        print(f"   ğŸ‘‰ å†³ç­–ç†ç”±: æ¨¡å‹ç½®ä¿¡åº¦é«˜ä¸”é¢„æµ‹é£é™©ä½ï¼Œå¯ç”¨æ¿€è¿›é…ç½®ä»¥æå‡ååã€‚")

    # ---------------------------------------------------------
    # Step 5: æˆ˜æœ¯æ‰§è¡Œ
    # ---------------------------------------------------------
    tactical = CAGSTacticalLayer()
    correction = CAGSCorrectionLayer(initial_chunk_size=chunk_size)
    
    print("\nğŸ”„ [æˆ˜æœ¯å±‚ & ä¿®æ­£å±‚] å¯åŠ¨è‡ªé€‚åº”ä¼ è¾“æµæ°´çº¿...")
    print("-" * 60)

    for i in range(10):
        # æ¨¡æ‹Ÿï¼šä¸­é—´å‘ç”Ÿç½‘ç»œæŠ–åŠ¨
        is_jitter = (i == 3 or i == 4 or i == 5)
        status = 'TIMEOUT' if is_jitter else 'SUCCESS'
        
        # A. ä¿®æ­£å±‚ (AIMD)
        current_size = correction.feedback(status, rtt_ms=200) 
        
        # B. æˆ˜æœ¯å±‚ (ä¹±åºæ¨¡æ‹Ÿ)
        actual_id = i
        if i == 1: actual_id = 2
        if i == 2: actual_id = 1
        
        if status == 'SUCCESS':
            tactical.on_download_complete(actual_id, current_size/1024)
        
        time.sleep(0.1)

    print("-" * 60)
    print("âœ… ä»¿çœŸç»“æŸï¼ç³»ç»Ÿå±•ç¤ºäº†ä» [ä¸ç¡®å®šæ€§æ„ŸçŸ¥] åˆ° [AIMDè‡ªæ„ˆ] çš„å®Œæ•´é—­ç¯ã€‚")

if __name__ == "__main__":
    run_cags_simulation()