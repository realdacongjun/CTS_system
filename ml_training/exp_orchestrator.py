"""
å®éªŒç¼–æ’æ¨¡å—
åŠŸèƒ½ï¼šè‡ªåŠ¨åŒ–æ§åˆ¶å¤§è§„æ¨¡å®éªŒæµç¨‹ï¼Œç®¡ç†å®¹å™¨ç¯å¢ƒå’Œæ•°æ®æ”¶é›†
è¾“å…¥ï¼šå®éªŒé…ç½®å‚æ•°
è¾“å‡ºï¼šå®éªŒç»“æœæ•°æ®
"""

import os
import subprocess
import time
import json
import uuid
import docker
from typing import Dict, Any, List
from .config import get_client_capabilities, get_image_profiles, get_compression_config
import threading
import signal
import sys
from pathlib import Path
import shutil
import sqlite3
import tempfile
import subprocess


class ExperimentOrchestrator:
    CLIENT_PROFILES = {
        "C1": {"cpu": 0.2, "memory": "512m", "bandwidth": "2mbit", "rtt": "200ms", "disk_io": "5mb/s"},
        "C2": {"cpu": 0.5, "memory": "1g", "bandwidth": "20mbit", "rtt": "50ms", "disk_io": "10mb/s"},
        "C3": {"cpu": 1.0, "memory": "2g", "bandwidth": "5mbit", "rtt": "100ms", "disk_io": "50mb/s"},
        "C4": {"cpu": 1.5, "memory": "2g", "bandwidth": "50mbit", "rtt": "20ms", "disk_io": "150mb/s"},
        "C5": {"cpu": 0.8, "memory": "2g", "bandwidth": "100mbit", "rtt": "10ms", "disk_io": "200mb/s"},
        "C6": {"cpu": 4.0, "memory": "4g", "bandwidth": "500mbit", "rtt": "5ms", "disk_io": "500mb/s"}
    }
    
    def __init__(self, cloud_mode=False):
        self.cloud_mode = cloud_mode
        self.docker_client = docker.from_env()
        self.check_environment()

    def check_environment(self):
        """ç¯å¢ƒä¾èµ–é¢„æ£€ - æ ¹æ®é¡¹ç›®è§„èŒƒå¿…é¡»æ‰§è¡Œ"""
        # æ£€æŸ¥tcå‘½ä»¤
        if not self.cloud_mode and not shutil.which("tc"):
            print("è­¦å‘Š: tcå‘½ä»¤ä¸å¯ç”¨ï¼Œå°†è‡ªåŠ¨åˆ‡æ¢åˆ°äº‘æ¨¡å¼")
            self.cloud_mode = True

        # æ£€æŸ¥sudoæƒé™
        if not self.cloud_mode:
            try:
                subprocess.run(["sudo", "-n", "true"], check=True, capture_output=True)
            except subprocess.CalledProcessError:
                print("è­¦å‘Š: æ— sudoæƒé™ï¼Œæ— æ³•ä½¿ç”¨tcã€‚åˆ‡æ¢åˆ°äº‘æ¨¡å¼")
                self.cloud_mode = True

        # æ£€æŸ¥Pumbaï¼ˆäº‘æ¨¡å¼å¿…éœ€ï¼‰
        if self.cloud_mode and not shutil.which("pumba"):
            raise RuntimeError("äº‘æ¨¡å¼éœ€è¦Pumbaï¼Œä½†æœªå®‰è£…ã€‚è¯·å®‰è£…: https://github.com/alexei-led/pumba")

    def run_experiment(self, client_type, image_name, algorithm):
        """æ‰§è¡Œå•ä¸ªå®éªŒï¼Œç¡®ä¿èµ„æºæ¸…ç†é—­ç¯"""
        container = None
        temp_dir = None
        try:
            # 1. æ¸…ç†ç¼“å­˜ - ä¿éšœæ€§èƒ½æµ‹é‡å‡†ç¡®æ€§
            subprocess.run(["sync"], check=True)
            with open('/proc/sys/vm/drop_caches', 'w') as f:
                f.write('3')
            
            # 2. å¯åŠ¨å®¹å™¨å¹¶æŒ‚è½½ä¸´æ—¶ç›®å½•
            container, temp_dir = self.start_container(client_type)
            
            # 3. åº”ç”¨ç½‘ç»œé™åˆ¶
            if not self.cloud_mode:
                self.apply_tc_rules(container, client_type)
            else:
                self.apply_pumba_rules(container, client_type)
            
            # 4. æ‰§è¡Œå®éªŒ
            result = self.execute_pull(container, image_name, algorithm)
            
            # 5. éªŒè¯æ•°æ®è´¨é‡
            return self.validate_experiment_data(result)
        finally:
            # 6. æ¸…ç†èµ„æºï¼ˆç¡®ä¿æ‰§è¡Œï¼‰
            if container:
                self.cleanup_container(container)
            if temp_dir:
                shutil.rmtree(temp_dir, ignore_errors=True)
            if not self.cloud_mode:
                self.cleanup_tc_rules(client_type)

    def validate_experiment_data(self, result):
        """æ•°æ®è´¨é‡æ ¡éªŒ - æ ¹æ®å®éªŒæ•°æ®è´¨é‡æ§åˆ¶è§„èŒƒ"""
        target_bandwidth = self.CLIENT_PROFILES[result['client_type']]['bandwidth']
        actual_bandwidth = result.get('actual_bandwidth', 0)
        
        # å¸¦å®½åå·®æ£€æŸ¥
        if actual_bandwidth < 0.5 * target_bandwidth or actual_bandwidth > 1.5 * target_bandwidth:
            result['is_noisy_data'] = True
            result['noise_reason'] = f"å¸¦å®½åå·®è¿‡å¤§: ç›®æ ‡{target_bandwidth}, å®æµ‹{actual_bandwidth}"

        # å¸¦å®½æ³¢åŠ¨ç‡æ£€æŸ¥
        bandwidth_std = result.get('bandwidth_std', 0)
        bandwidth_mean = result.get('bandwidth_mean', 0)
        if bandwidth_mean > 0:
            cv = bandwidth_std / bandwidth_mean
            if cv > 0.3:
                result['is_noisy_data'] = True
                result['noise_reason'] = f"å¸¦å®½æ³¢åŠ¨ç‡è¿‡é«˜: CV={cv:.2f}"
        
        return result

    def start_container(self, client_type):
        """å¯åŠ¨å®¹å™¨å¹¶æŒ‚è½½ä¸´æ—¶ç›®å½• - è§£å†³å­˜å‚¨ç®¡ç†é—®é¢˜"""
        profile = self.CLIENT_PROFILES[client_type]
        temp_dir = tempfile.mkdtemp(prefix=f"exp_{client_type}_")
        
        container = self.docker_client.containers.run(
            "cts-system/client-agent:latest",
            detach=True,
            volumes={temp_dir: {'bind': '/tmp/experiment', 'mode': 'rw'}},
            nano_cpus=int(profile['cpu'] * 1e9),
            mem_limit=profile['memory'],
            # å…¶ä»–èµ„æºé™åˆ¶...
        )
        return container, temp_dir

    def apply_tc_rules(self, container, client_type):
        """åº”ç”¨tcè§„åˆ™ - æœ¬åœ°æ¨¡å¼"""
        veth_name = self.get_container_veth_interface(container.id)
        if not veth_name:
            raise RuntimeError("æ— æ³•æ‰¾åˆ°å®¹å™¨çš„vethæ¥å£")
        
        subprocess.run([
            "sudo", "tc", "qdisc", "del", "dev", veth_name, "root"
        ], check=True)
        
        subprocess.run([
            "sudo", "tc", "qdisc", "add", "dev", veth_name, "root", "netem",
            "rate", self.CLIENT_PROFILES[client_type]['bandwidth'],
            "delay", self.CLIENT_PROFILES[client_type]['rtt']
        ], check=True)

    def apply_pumba_rules(self, container, client_type):
        """åº”ç”¨Pumbaè§„åˆ™ - äº‘æ¨¡å¼"""
        subprocess.run([
            "pumba", "netem", "--duration", "60s", "delay",
            "--time", self.CLIENT_PROFILES[client_type]['rtt'],
            "--rate", self.CLIENT_PROFILES[client_type]['bandwidth'],
            "container", container.id
        ], check=True)

    def execute_pull(self, container, image_name, algorithm):
        """æ‰§è¡Œæ‹‰å–æ“ä½œ - æ ¸å¿ƒå®éªŒé€»è¾‘"""
        result = container.exec_run(
            f"python3 /app/client_pull_script.py {image_name} --method {algorithm}",
            stdout=True,
            stderr=True,
            detach=False
        )
        
        output = result.output.decode('utf-8')
        exit_code = result.exit_code
        
        if exit_code != 0:
            raise RuntimeError(f"æ‹‰å–å¤±è´¥: {output}")
        
        # è§£æè¾“å‡º
        for line in reversed(output.strip().split('\n')):
            if line.startswith('{') and line.endswith('}'):
                try:
                    return json.loads(line)
                except:
                    continue
        
        raise RuntimeError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾“å‡º")

    def cleanup_container(self, container):
        """æ¸…ç†å®¹å™¨"""
        container.stop()
        container.remove()

    def cleanup_tc_rules(self, client_type):
        """æ¸…ç†tcè§„åˆ™"""
        veth_name = self.get_container_veth_interface(container.id)
        if veth_name:
            subprocess.run([
                "sudo", "tc", "qdisc", "del", "dev", veth_name, "root"
            ], check=True)

    def get_container_veth_interface(self, container_id):
        """
        é€šç”¨ã€ç¨³å¥çš„æ–¹æ³•ï¼šé€šè¿‡ iflink ç´¢å¼•æŸ¥æ‰¾å®¹å™¨åœ¨å®¿ä¸»æœºä¾§å¯¹åº”çš„ veth ç½‘å¡åç§°
        """
        try:
            # 1. åœ¨å®¹å™¨å†…éƒ¨è·å– eth0 å¯¹åº”çš„å®¿ä¸»æœºç½‘å¡ç´¢å¼•å· (iflink)
            # æ‰§è¡Œ: cat /sys/class/net/eth0/iflink
            cmd_get_iflink = f"docker exec {container_id} cat /sys/class/net/eth0/iflink"
            iflink_idx = subprocess.check_output(cmd_get_iflink, shell=True).decode().strip()
            
            # 2. åœ¨å®¿ä¸»æœºä¾§éå†æ‰€æœ‰ç½‘ç»œæ¥å£ï¼Œå¯»æ‰¾ç´¢å¼•å·åŒ¹é…çš„é‚£ä¸ª
            # å®¿ä¸»æœºçš„ç½‘å¡ç´¢å¼•ä¿¡æ¯å­˜æ”¾åœ¨ /sys/class/net/<iface>/ifindex
            for iface in os.listdir('/sys/class/net/'):
                ifindex_path = f'/sys/class/net/{iface}/ifindex'
                if os.path.exists(ifindex_path):
                    with open(ifindex_path, 'r') as f:
                        if f.read().strip() == iflink_idx:
                            # æ’é™¤æ‰å›ç¯ç½‘å¡ç­‰å¹²æ‰°
                            if iface != 'lo':
                                return iface
                                
            return None
        except Exception as e:
            print(f"âŒ å®šä½å®¹å™¨ {container_id} çš„ veth æ¥å£å¤±è´¥: {e}")
            return None

    def _setup_tc_limit(self, veth_name: str, bandwidth_mbps: int, network_rtt: int):
        """
        ä½¿ç”¨ netem ç›´æ¥æ§åˆ¶ç½‘å¡é€Ÿç‡å’Œå»¶è¿Ÿ (æ¯” TBF æ›´é€‚åˆäº‘ç¯å¢ƒè™šæ‹Ÿç½‘å¡)
        """
        try:
            # æ¸…ç†æ—§è§„åˆ™
            subprocess.run(f"sudo tc qdisc del dev {veth_name} root", shell=True, capture_output=True)
            
            # æ ¸å¿ƒä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨ netem çš„ rate å‚æ•°ï¼Œç®€å•ç¨³å¥
            # è®¡ç®—å»¶è¿Ÿçš„ä¸€åŠä½œä¸ºå¹³æ»‘å€¼ (optional)
            tc_cmd = (f"sudo tc qdisc add dev {veth_name} root netem "
                      f"rate {bandwidth_mbps}mbit "
                      f"delay {network_rtt}ms 2ms distribution normal")
            
            subprocess.run(tc_cmd, shell=True, check=True)
            print(f"æˆåŠŸè®¾ç½®ç½‘ç»œé™åˆ¶: {bandwidth_mbps}Mbps, {network_rtt}mså»¶è¿Ÿ")
            return True
        except Exception as e:
            print(f"âŒ Tc è®¾ç½®å¤±è´¥ [{veth_name}]: {e}")
            return False

    def _setup_pumba_limit(self, container_id: str, bandwidth_mbps: int, network_rtt: int):
        """
        ä½¿ç”¨Pumbaè®¾ç½®ç½‘ç»œé™åˆ¶ï¼ˆäº‘æ¨¡å¼ä¸‹ï¼‰
        """
        try:
            # åœæ­¢å¯èƒ½å­˜åœ¨çš„pumbaè¿›ç¨‹
            subprocess.run(['pkill', '-f', f'pumba.*{container_id}'], capture_output=True)
            
            # ä½¿ç”¨pumbaè®¾ç½®ç½‘ç»œé™åˆ¶
            cmd = [
                'pumba', 'netem', '--duration', '60s', 'delay',
                f'--time={network_rtt}ms', '--jitter=2ms',
                '--rate', f'{bandwidth_mbps}mbit',
                'container', container_id
            ]
            
            subprocess.Popen(cmd)
            print(f"æˆåŠŸè®¾ç½®Pumbaç½‘ç»œé™åˆ¶: {bandwidth_mbps}Mbps, {network_rtt}mså»¶è¿Ÿ")
            return True
        except Exception as e:
            print(f"âŒ Pumba è®¾ç½®å¤±è´¥ [{container_id}]: {e}")
            return False

    def _get_container_veth_safe(self, container_id):
        """æ”¹è¿›çš„ veth æŸ¥æ‰¾ï¼šå¢åŠ äº†é‡è¯•æœºåˆ¶ï¼Œé˜²æ­¢å®¹å™¨å¯åŠ¨ç¬é—´ç½‘å¡æœªæŒ‚è½½"""
        for _ in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
            veth = self.get_container_veth_interface(container_id)
            if veth: 
                return veth
            time.sleep(1)
        return None

    def __init__(self, 
                 registry_url: str = "localhost:5000",
                 data_dir: str = "/tmp/exp_data",
                 container_image: str = "cts_client:latest",
                 cloud_mode: bool = False):
        """
        åˆå§‹åŒ–å®éªŒç¼–æ’å™¨
        
        Args:
            registry_url: é•œåƒä»“åº“åœ°å€
            data_dir: å®éªŒæ•°æ®ç›®å½•
            container_image: å®¢æˆ·ç«¯å®¹å™¨é•œåƒ
            cloud_mode: æ˜¯å¦åœ¨äº‘æœåŠ¡å™¨æ¨¡å¼ä¸‹è¿è¡Œ
        """
        self.registry_url = registry_url
        self.data_dir = data_dir
        self.container_image = container_image
        self.client = docker.from_env()
        self.cloud_mode = cloud_mode  # äº‘æœåŠ¡å™¨æ¨¡å¼æ ‡å¿—
        
        # æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§
        self._check_environment()
        
        # ä»é…ç½®è·å–å®éªŒå‚æ•°
        self.client_profiles = get_client_capabilities()['profiles']
        self.target_images = get_image_profiles()
        self.compression_methods = get_compression_config()['algorithms']
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        Path(self.data_dir).mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå®éªŒè®°å½•æ•°æ®åº“
        self.db_path = os.path.join(self.data_dir, "experiment_manifest.db")
        self._init_database()
        
        # è®°å½•å·²å®Œæˆçš„å®éªŒ
        self.completed_experiments = self._load_completed_experiments()
        
        # ç”¨äºä¼˜é›…é€€å‡º
        self.running = True
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _check_environment(self):
        """æ£€æŸ¥ç¯å¢ƒå…¼å®¹æ€§"""
        import platform
        
        # æ£€æŸ¥tcå‘½ä»¤æ˜¯å¦å­˜åœ¨ (ä»…åœ¨Linuxç¯å¢ƒä¸‹)
        if platform.system().lower() == "linux" and not self.cloud_mode:
            result = subprocess.run(['which', 'tc'], capture_output=True, text=True)
            if result.returncode != 0:
                print("âš ï¸ æœªæ‰¾åˆ°tcå‘½ä»¤ï¼Œå¦‚æœåœ¨äº‘ç¯å¢ƒè¿è¡Œè¯·å¯ç”¨cloud_mode=True")
        elif not self.cloud_mode:
            print("âš ï¸ å½“å‰ç³»ç»Ÿä¸æ˜¯Linuxï¼Œè·³è¿‡tcå‘½ä»¤æ£€æŸ¥")
        
        # æ£€æŸ¥Pumbaå‘½ä»¤æ˜¯å¦å­˜åœ¨
        if platform.system().lower() == "linux":
            result = subprocess.run(['which', 'pumba'], capture_output=True, text=True)
            if result.returncode != 0 and self.cloud_mode:
                print("âš ï¸ æœªæ‰¾åˆ°Pumbaå‘½ä»¤ï¼Œäº‘æ¨¡å¼ä¸‹ç½‘ç»œä»¿çœŸå¯èƒ½å¤±è´¥")
        elif self.cloud_mode:
            print("âš ï¸ å½“å‰ç³»ç»Ÿä¸æ˜¯Linuxï¼Œè·³è¿‡Pumbaå‘½ä»¤æ£€æŸ¥")
        
        # æ£€æŸ¥iperf3å‘½ä»¤æ˜¯å¦å­˜åœ¨ (ä»…åœ¨Linuxç¯å¢ƒä¸‹)
        if platform.system().lower() == "linux" and not self.cloud_mode:
            result = subprocess.run(['which', 'iperf3'], capture_output=True, text=True)
            if result.returncode != 0:
                print("âš ï¸ æœªæ‰¾åˆ°iperf3å‘½ä»¤ï¼Œç½‘ç»œæ ¡å‡†åŠŸèƒ½å¯èƒ½å—é™")
        elif not self.cloud_mode:
            print("âš ï¸ å½“å‰ç³»ç»Ÿä¸æ˜¯Linuxï¼Œè·³è¿‡iperf3å‘½ä»¤æ£€æŸ¥")
        
        # æ£€æŸ¥æ˜¯å¦å…·æœ‰sudoæƒé™ (ä»…åœ¨Linuxç¯å¢ƒä¸‹)
        if platform.system().lower() == "linux" and not self.cloud_mode:
            result = subprocess.run(['sudo', '-n', 'true'], capture_output=True, text=True)
            if result.returncode != 0:
                print("âš ï¸ å½“å‰ç”¨æˆ·æ²¡æœ‰sudoæƒé™ï¼Œç½‘ç»œä»¿çœŸå¯èƒ½å¤±è´¥")
        elif not self.cloud_mode:
            print("âš ï¸ å½“å‰ç³»ç»Ÿä¸æ˜¯Linuxï¼Œè·³è¿‡sudoæƒé™æ£€æŸ¥")
    
    def _init_database(self):
        """åˆå§‹åŒ–å®éªŒè®°å½•æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºå®éªŒè®°å½•è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS experiments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                uuid TEXT UNIQUE,
                profile_id TEXT,
                image_name TEXT,
                method TEXT,
                replication INTEGER,
                status TEXT,
                start_time REAL,
                end_time REAL,
                cost_total REAL,
                host_cpu_load REAL,
                host_memory_usage REAL,
                host_disk_io REAL,
                decompression_time REAL,
                error_msg TEXT,
                is_noisy_data INTEGER,
                actual_bandwidth REAL,
                bandwidth_std REAL,
                avg_cpu_usage REAL,
                peak_memory REAL,
                compressed_size INTEGER,
                uncompressed_size INTEGER,
                layer_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # åˆ›å»ºå®éªŒé…ç½®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS experiment_configs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                config_key TEXT UNIQUE,
                config_data TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        print(f"æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
        self.running = False
        # æ¸…ç†èµ„æº
        self._cleanup_containers()
    
    def _cleanup_containers(self):
        """æ¸…ç†æ‰€æœ‰å®éªŒå®¹å™¨"""
        try:
            containers = self.client.containers.list(all=True, filters={"ancestor": self.container_image})
            for container in containers:
                try:
                    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„tcè§„åˆ™
                    if not self.cloud_mode:
                        veth = self.get_container_veth_interface(container.id)
                        if veth:
                            subprocess.run(f"sudo tc qdisc del dev {veth} root 2>/dev/null", shell=True)
                    
                    # æ¸…ç†pumbaè¿›ç¨‹
                    subprocess.run(['pkill', '-f', f'pumba.*{container.id}'], capture_output=True)
                    
                    container.stop()
                    container.remove()
                    print(f"å·²æ¸…ç†å®¹å™¨: {container.short_id}")
                except:
                    pass
        except Exception as e:
            print(f"æ¸…ç†å®¹å™¨æ—¶å‡ºé”™: {e}")
    
    def _load_completed_experiments(self) -> set:
        """ä»æ•°æ®åº“åŠ è½½å·²å®Œæˆçš„å®éªŒè®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute("SELECT uuid FROM experiments WHERE status IN ('SUCCESS', 'ABNORMAL')")
            completed = {row[0] for row in cursor.fetchall()}
            print(f"å·²åŠ è½½ {len(completed)} ä¸ªå·²å®Œæˆçš„å®éªŒè®°å½•")
            return completed
        except Exception as e:
            print(f"åŠ è½½å·²å®Œæˆå®éªŒè®°å½•å¤±è´¥: {e}")
            return set()
        finally:
            conn.close()
    
    def _setup_emulated_container(self, profile: Dict[str, Any]) -> docker.models.containers.Container:
        """æ ¹æ®é…ç½®å¯åŠ¨æ¨¡æ‹Ÿå®¢æˆ·ç«¯å®¹å™¨"""
        profile_name = profile['name']
        print(f"æ­£åœ¨å¯åŠ¨å®¹å™¨ç¯å¢ƒ: {profile_name}")
        
        # æ„å»ºå®¹å™¨å¯åŠ¨å‚æ•°
        container_params = {
            'image': self.container_image,
            'volumes': {
                self.data_dir: {'bind': '/experiment_data', 'mode': 'rw'},
                '/var/run/docker.sock': {'bind': '/var/run/docker.sock', 'mode': 'rw'}
            },
            'network_mode': 'bridge',
            'environment': {
                'CLIENT_PROFILE_NAME': profile_name,
                'CPU_SCORE': str(profile.get('cpu_limit', profile.get('cpu_score', 1000))),
                'BANDWIDTH_MBPS': str(profile.get('bw_rate', profile.get('bandwidth_mbps', 10))),
                'NETWORK_RTT': str(profile.get('latency', profile.get('network_rtt', 100))),
                'DISK_IO_SPEED': str(profile.get('disk_read', profile.get('disk_io_speed', 50))),
                'MEMORY_SIZE': str(profile.get('mem_limit', profile.get('memory_size', 1))),
                'DECOMP_SPEED_GZIP': str(profile.get('decompression_speed', {}).get('gzip', 100)),
                'DECOMP_SPEED_ZSTD': str(profile.get('decompression_speed', {}).get('zstd', 100)),
                'DECOMP_SPEED_LZ4': str(profile.get('decompression_speed', {}).get('lz4', 100)),
                'LATENCY_REQUIREMENT': str(profile.get('latency_requirement', 100)),
                'REGISTRY_URL': self.registry_url
            },
            'mem_limit': f"{profile.get('mem_limit', profile.get('memory_size', '512m'))}",
            'detach': True,
            'tty': True,
            'stdin_open': True
        }
        
        # Windows Dockerç¯å¢ƒéœ€è¦ç‰¹æ®Šå¤„ç†CPUå‚æ•°
        import platform
        if platform.system().lower() != "windows":
            # éWindowsç³»ç»Ÿä½¿ç”¨nano_cpusé™åˆ¶
            container_params['nano_cpus'] = int(profile.get('cpu_limit', profile.get('cpu_score', 1000)) / 1000 * 1e9)
        else:
            # Windowsç³»ç»Ÿä½¿ç”¨ä¸åŒçš„CPUé™åˆ¶æ–¹å¼
            cpu_limit = profile.get('cpu_limit', profile.get('cpu_score', 1000))
            # å°†CPUåˆ†æ•°è½¬æ¢ä¸ºDockerå…¼å®¹çš„CPUå‘¨æœŸæ•°
            # è¿™é‡Œæˆ‘ä»¬æŒ‰æ¯”ä¾‹è½¬æ¢ï¼Œ1000åˆ†å¯¹åº”1ä¸ªCPUæ ¸å¿ƒ
            # æ·»åŠ æœ€å°å€¼é™åˆ¶ï¼Œé¿å…CPUé…é¢è¿‡å°å¯¼è‡´çš„é”™è¯¯
            container_params['cpu_period'] = 100000  # 100ms
            container_params['cpu_quota'] = max(1000, int((cpu_limit / 1000) * 100000))  # æŒ‰æ¯”ä¾‹åˆ†é…CPUæ—¶é—´ï¼Œæœ€å°ä¸º1000
        
        # å¯åŠ¨å®¹å™¨
        container = self.client.containers.run(**container_params)
        
        # åº”ç”¨ç½‘ç»œé™åˆ¶
        if not self.cloud_mode:
            self._apply_network_limit(container.id, profile)
        else:
            # äº‘æ¨¡å¼ä¸‹ä½¿ç”¨Pumba
            profile_bw = profile.get('bw_rate', profile.get('bandwidth_mbps', 10))
            profile_rtt = profile.get('latency', profile.get('network_rtt', 100))
            self._setup_pumba_limit(container.id, profile_bw, profile_rtt)
        
        print(f"å®¹å™¨ {container.id[:12]} å·²å¯åŠ¨ï¼Œåº”ç”¨é…ç½®: {profile_name}")
        return container

    def _apply_network_limit(self, container_id: str, profile: Dict[str, Any]):
        """åº”ç”¨ç½‘ç»œé™åˆ¶ï¼ˆå¸¦å®½ã€å»¶è¿Ÿç­‰ï¼‰"""
        try:
            # è·å–å®¹å™¨çš„vethæ¥å£åç§°
            veth_name = self._get_container_veth_safe(container_id)
            
            if veth_name:
                # è®¾ç½®ç½‘ç»œé™åˆ¶
                success = self._setup_tc_limit(
                    veth_name,
                    profile['bandwidth_mbps'],
                    profile['network_rtt']
                )
                
                if success:
                    print(f"ä¸ºå®¹å™¨ {container_id[:12]} æˆåŠŸåº”ç”¨ç½‘ç»œé™åˆ¶: {profile['bandwidth_mbps']}Mbps, {profile['network_rtt']}ms")
                else:
                    print(f"ä¸ºå®¹å™¨ {container_id[:12]} åº”ç”¨ç½‘ç»œé™åˆ¶å¤±è´¥")
            else:
                print(f"æ— æ³•æ‰¾åˆ°å®¹å™¨ {container_id[:12]} çš„vethæ¥å£ï¼Œè·³è¿‡ç½‘ç»œé™åˆ¶è®¾ç½®")
        except Exception as e:
            print(f"åº”ç”¨ç½‘ç»œé™åˆ¶å¤±è´¥: {e}")

    def _clear_system_cache(self):
        """æ¸…ç†ç³»ç»Ÿç¼“å­˜ï¼Œç¡®ä¿å®éªŒå‡†ç¡®æ€§"""
        try:
            # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿç¼“å­˜
            subprocess.run(['sync'], check=True)
            with open('/proc/sys/vm/drop_caches', 'w') as f:
                f.write('3')
            print("ç³»ç»Ÿç¼“å­˜å·²æ¸…ç†")
        except Exception as e:
            print(f"æ¸…ç†ç³»ç»Ÿç¼“å­˜å¤±è´¥: {e}")
    
    def _save_experiment_record(self, experiment_record: Dict[str, Any]):
        """ä¿å­˜å®éªŒè®°å½•åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        try:
            cursor.execute('''
                INSERT OR REPLACE INTO experiments 
                (uuid, profile_id, image_name, method, replication, status, 
                 start_time, end_time, cost_total, host_cpu_load, host_memory_usage, 
                 host_disk_io, decompression_time, error_msg, is_noisy_data,
                 actual_bandwidth, bandwidth_std, avg_cpu_usage, peak_memory,
                 compressed_size, uncompressed_size, layer_count)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                experiment_record.get('uuid'),
                experiment_record.get('profile_id'),
                experiment_record.get('image_name'),
                experiment_record.get('method'),
                experiment_record.get('replication'),
                experiment_record.get('status', 'PENDING'),
                experiment_record.get('start_time'),
                experiment_record.get('end_time'),
                experiment_record.get('cost_total'),
                experiment_record.get('host_cpu_load'),
                experiment_record.get('host_memory_usage'),
                experiment_record.get('host_disk_io'),
                experiment_record.get('decompression_time'),
                experiment_record.get('error_msg'),
                experiment_record.get('is_noisy_data', 0),
                experiment_record.get('actual_bandwidth'),
                experiment_record.get('bandwidth_std'),
                experiment_record.get('avg_cpu_usage'),
                experiment_record.get('peak_memory'),
                experiment_record.get('compressed_size'),
                experiment_record.get('uncompressed_size'),
                experiment_record.get('layer_count')
            ))
            
            conn.commit()
        except Exception as e:
            print(f"ä¿å­˜å®éªŒè®°å½•å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def _get_host_runtime_metrics(self):
        """è·å–å®¿ä¸»æœºè¿è¡Œæ—¶æŒ‡æ ‡"""
        return {
            'host_cpu_load': self._get_host_cpu_load(),
            'host_memory_usage': self._get_host_memory_usage(),
            'host_disk_io': self._get_host_disk_io()
        }
    
    def _get_host_disk_io(self) -> float:
        """è·å–å®¿ä¸»æœºç£ç›˜IOä½¿ç”¨ç‡"""
        try:
            # ä½¿ç”¨iostatè·å–ç£ç›˜IOä¿¡æ¯ï¼ˆå¦‚æœæ²¡æœ‰å®‰è£…iostatï¼Œåˆ™è¿”å›0ï¼‰
            result = subprocess.run(['iostat', '-d', '1', '2'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                # è§£ææœ€åä¸€éƒ¨åˆ†æ•°æ®
                for line in reversed(lines):
                    if line and not line.startswith('Device') and not line.startswith('Linux'):
                        parts = line.split()
                        if len(parts) >= 3:
                            # è¿”å›ç£ç›˜IOçš„å¹³å‡å€¼ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                            return float(0.0)  # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„è§£æ
            return 0.0
        except Exception:
            return 0.0

    def _collect_experiment_metrics(self, 
                        profile_id: str, 
                        image_name: str, 
                        method: str, 
                        rep: int, 
                        start_time: float, 
                        end_time: float,
                        decompression_time: float = 0,
                        error_msg: str = None,
                        monitoring_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """æ”¶é›†å®éªŒæŒ‡æ ‡"""
        # ç”Ÿæˆå”¯ä¸€å®éªŒID
        exp_uuid = f"{profile_id}_{image_name}_{method}_rep{rep}_{uuid.uuid4().hex[:8]}"
        
        # é»˜è®¤ç›‘æ§æ•°æ®
        if monitoring_data is None:
            monitoring_data = {
                'actual_bandwidth': 0,
                'bandwidth_std': 0,
                'avg_cpu_usage': 0,
                'peak_memory': 0,
                'is_noisy_data': False,
                'compressed_size': 0,
                'uncompressed_size': 0,
                'layer_count': 0
            }
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°æ•°æ®
        target_bandwidth = next((p['bandwidth_mbps'] for p in self.client_profiles if p['name'] == profile_id), 0)
        bandwidth_deviation = abs(monitoring_data['actual_bandwidth'] - target_bandwidth) / target_bandwidth if target_bandwidth > 0 else 0
        cv = monitoring_data['bandwidth_std'] / monitoring_data['actual_bandwidth'] if monitoring_data['actual_bandwidth'] > 0 else 0
        is_noisy = bandwidth_deviation > 0.5 or cv > 0.3
        
        # åˆ›å»ºå®éªŒè®°å½•
        experiment_record = {
            'uuid': exp_uuid,
            'profile_id': profile_id,
            'image_name': image_name,
            'method': method,
            'replication': rep,
            'start_time': start_time,
            'end_time': end_time,
            'cost_total': end_time - start_time,
            'timestamp': time.time(),
            'host_cpu_load': self._get_host_cpu_load(),
            'host_memory_usage': self._get_host_memory_usage(),
            'host_disk_io': self._get_host_disk_io(),
            'decompression_time': decompression_time,
            'decompression_performance': {
                'decompression_time': decompression_time,
                'cpu_usage': 0.0,
                'memory_usage': 0.0,
                'disk_io': 0.0,
                'method': method
            },
            'error_msg': error_msg,
            'status': 'ABNORMAL' if error_msg or decompression_time < 0.01 else 'SUCCESS',  # è§£å‹æ—¶é—´å°äº10msæ ‡è®°ä¸ºå¼‚å¸¸
            'is_noisy_data': is_noisy,
            'actual_bandwidth': monitoring_data['actual_bandwidth'],
            'bandwidth_std': monitoring_data['bandwidth_std'],
            'avg_cpu_usage': monitoring_data['avg_cpu_usage'],
            'peak_memory': monitoring_data['peak_memory'],
            'compressed_size': monitoring_data['compressed_size'],
            'uncompressed_size': monitoring_data['uncompressed_size'],
            'layer_count': monitoring_data['layer_count']
        }
        
        # ä¿å­˜å®éªŒæ•°æ®åˆ°ç‹¬ç«‹æ–‡ä»¶
        filename = f"{exp_uuid}.json"
        filepath = os.path.join(self.data_dir, filename)
        
        with open(filepath, 'w') as f:
            json.dump(experiment_record, f, indent=2)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        self._save_experiment_record(experiment_record)
        
        return experiment_record
    
    def collect_experiment_metrics(self, 
                        profile_id: str, 
                        image_name: str, 
                        method: str, 
                        rep: int, 
                        start_time: float, 
                        end_time: float,
                        decompression_time: float = 0,
                        error_msg: str = None,
                        monitoring_data: Dict[str, Any] = None) -> Dict[str, Any]:
        """å…¬å…±æ–¹æ³•ï¼šæ”¶é›†å®éªŒæŒ‡æ ‡"""
        return self._collect_experiment_metrics(
            profile_id, image_name, method, rep, start_time, end_time, decompression_time, error_msg, monitoring_data
        )

    def _get_host_cpu_load(self) -> float:
        """è·å–å®¿ä¸»æœºCPUè´Ÿè½½"""
        try:
            # è·å–CPUä½¿ç”¨ç‡
            with open('/proc/loadavg', 'r') as f:
                loadavg = f.read().strip().split()[0]
                return float(loadavg)
        except Exception:
            return 0.0
    
    def _get_host_memory_usage(self) -> float:
        """è·å–å®¿ä¸»æœºå†…å­˜ä½¿ç”¨ç‡"""
        try:
            import psutil
            return psutil.virtual_memory().percent
        except ImportError:
            return 0.0
    
    def _monitor_container_resources(self, container_id: str, duration: float, interval: float = 0.5):
        """
        å®æ—¶ç›‘æ§å®¹å™¨èµ„æºä½¿ç”¨æƒ…å†µ
        """
        # åˆå§‹åŒ–å­˜å‚¨æ•°æ®çš„åˆ—è¡¨
        bandwidth_data = []
        cpu_usage_data = []
        memory_usage_data = []
        
        # è·å–åˆå§‹ç½‘ç»œç»Ÿè®¡æ•°æ®
        initial_stats = self.client.containers.get(container_id).stats(stream=False)
        initial_net = initial_stats.get('networks', {})
        initial_rx_bytes = sum([net.get('rx_bytes', 0) for net in initial_net.values()])
        
        start_time = time.time()
        while time.time() - start_time < duration:
            try:
                # è·å–å½“å‰æ—¶é—´ç‚¹çš„å®¹å™¨ç»Ÿè®¡ä¿¡æ¯
                current_stats = self.client.containers.get(container_id).stats(stream=False)
                
                # è®¡ç®—å½“å‰å¸¦å®½ä½¿ç”¨æƒ…å†µ
                current_net = current_stats.get('networks', {})
                current_rx_bytes = sum([net.get('rx_bytes', 0) for net in current_net.values()])
                
                # è®¡ç®—å®é™…å¸¦å®½ (Mbps)
                time_delta = interval
                bytes_delta = current_rx_bytes - initial_rx_bytes
                bandwidth_mbps = (bytes_delta * 8) / (1024 * 1024) / time_delta  # è½¬æ¢ä¸ºMbps
                
                # æ›´æ–°åˆå§‹å€¼
                initial_rx_bytes = current_rx_bytes
                
                # è·å–CPUä½¿ç”¨ç‡
                cpu_percent = 0.0
                cpu_stats = current_stats.get('cpu_stats', {})
                precpu_stats = current_stats.get('precpu_stats', {})
                
                cpu_delta = cpu_stats.get('cpu_usage', {}).get('total_usage', 0) - precpu_stats.get('cpu_usage', {}).get('total_usage', 0)
                system_delta = cpu_stats.get('system_cpu_usage', 0) - precpu_stats.get('system_cpu_usage', 0)
                online_cpus = cpu_stats.get('online_cpus', len(cpu_stats.get('cpu_usage', {}).get('percpu_usage', [1])))
                
                if system_delta > 0:
                    cpu_percent = (cpu_delta / system_delta) * online_cpus * 100.0
                
                # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ
                mem_stats = current_stats.get('memory_stats', {})
                mem_usage = mem_stats.get('usage', 0) / (1024 * 1024)  # è½¬æ¢ä¸ºMB
                
                # å­˜å‚¨æ•°æ®
                bandwidth_data.append(bandwidth_mbps)
                cpu_usage_data.append(cpu_percent)
                memory_usage_data.append(mem_usage)
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªé‡‡æ ·ç‚¹
                time.sleep(interval)
                
            except Exception as e:
                print(f"ç›‘æ§å®¹å™¨ {container_id} æ—¶å‡ºé”™: {e}")
                break
        
        # è®¡ç®—ç»Ÿè®¡ç‰¹å¾
        if bandwidth_data:
            avg_bandwidth = sum(bandwidth_data) / len(bandwidth_data)
            bandwidth_std = (sum((x - avg_bandwidth) ** 2 for x in bandwidth_data) / len(bandwidth_data)) ** 0.5 if len(bandwidth_data) > 1 else 0
        else:
            avg_bandwidth = 0
            bandwidth_std = 0
        
        if cpu_usage_data:
            avg_cpu = sum(cpu_usage_data) / len(cpu_usage_data)
        else:
            avg_cpu = 0
        
        if memory_usage_data:
            peak_memory = max(memory_usage_data)
        else:
            peak_memory = 0
        
        return {
            'actual_bandwidth': avg_bandwidth,
            'bandwidth_std': bandwidth_std,
            'avg_cpu_usage': avg_cpu,
            'peak_memory': peak_memory
        }
    
    def run_profiled_experiment(self, 
                              container: docker.models.containers.Container, 
                              image_name: str, 
                              method: str,
                              profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿è¡Œå¸¦ç›‘æ§çš„å®éªŒ
        """
        try:
            # å‡†å¤‡å‘½ä»¤
            if self.cloud_mode:
                print(f"äº‘æ¨¡å¼: ä½¿ç”¨å½“å‰ç¯å¢ƒæ‰§è¡Œå®éªŒ - é•œåƒ: {image_name}, æ–¹æ³•: {method}")
                command = f"python3 /app/client_pull_script.py {self.registry_url}/{image_name} --method {method}"
            else:
                command = f"python3 /app/client_pull_script.py {self.registry_url}/{image_name} --method {method}"
            
            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            monitoring_result = {}
            monitor_thread = threading.Thread(
                target=self._monitor_container_resources,
                args=(container.id, 60, 0.5)
            )
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            monitor_thread.start()
            
            # æ‰§è¡Œæ‹‰å–å®éªŒ
            result = container.exec_run(
                cmd=command,
                stdout=True,
                stderr=True,
                detach=False
            )
            
            # è§£ææ‰§è¡Œç»“æœ
            output = result.output.decode('utf-8')
            exit_code = result.exit_code
            
            # ç­‰å¾…ç›‘æ§çº¿ç¨‹å®Œæˆ
            monitor_thread.join(timeout=5)  # æœ€å¤šç­‰å¾…5ç§’
            
            # è·å–ç›‘æ§æ•°æ®
            monitoring_data = self._monitor_container_resources(container.id, 0.1, 0.5)  # çŸ­æ—¶é—´å¿«é€Ÿé‡‡æ ·è·å–æœ€ç»ˆæ•°æ®
            
            # è·å–é•œåƒé™æ€ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦ä»é…ç½®ä¸­è·å–ï¼‰
            image_features = self._get_image_features(image_name)
            
            # å¢åŠ ï¼šä»æ ‡å‡†è¾“å‡ºä¸­ç²¾å‡†æå–æœ€åä¸€è¡Œ JSON
            perf_data = {}
            for line in reversed(output.strip().split('\n')):
                if line.startswith('{') and line.endswith('}'):
                    try:
                        perf_data = json.loads(line)
                        break
                    except: 
                        continue
            
            if not perf_data:
                return {
                    "status": "ABNORMAL", 
                    "error": "No JSON found in output", 
                    "raw": output,
                    "actual_bandwidth": monitoring_data['actual_bandwidth'],
                    "bandwidth_std": monitoring_data['bandwidth_std'],
                    "avg_cpu_usage": monitoring_data['avg_cpu_usage'],
                    "peak_memory": monitoring_data['peak_memory'],
                    "is_noisy_data": False,
                    "compressed_size": image_features['compressed_size'],
                    "uncompressed_size": image_features['uncompressed_size'],
                    "layer_count": image_features['layer_count'],
                    "image_name": image_name,  # ç¡®ä¿åŒ…å«image_nameå­—æ®µ
                    "method": method,  # ç¡®ä¿åŒ…å«methodå­—æ®µ
                    "profile_id": profile['name']  # ç¡®ä¿åŒ…å«profile_idå­—æ®µ
                }
                
            # è®°å½•é¢å¤–çš„è§£å‹æ€§èƒ½ï¼ˆå®¿ä¸»æœºè§†è§’ï¼‰
            host_metrics = self._get_host_runtime_metrics()
            perf_data.update(host_metrics)
            
            # åˆå¹¶ç›‘æ§æ•°æ®
            perf_data.update(monitoring_data)
            perf_data.update(image_features)
            
            # ç¡®ä¿åŒ…å«å…³é”®å­—æ®µ
            perf_data['image_name'] = image_name
            perf_data['method'] = method
            perf_data['profile_id'] = profile['name']
            
            # è®¡ç®—æ˜¯å¦ä¸ºå™ªå£°æ•°æ®
            target_bandwidth = profile.get('bandwidth_mbps', 0)
            bandwidth_deviation = abs(monitoring_data['actual_bandwidth'] - target_bandwidth) / target_bandwidth if target_bandwidth > 0 else 0
            cv = monitoring_data['bandwidth_std'] / monitoring_data['actual_bandwidth'] if monitoring_data['actual_bandwidth'] > 0 else 0
            perf_data['is_noisy_data'] = bandwidth_deviation > 0.5 or cv > 0.3
            
            return perf_data
            
        except Exception as e:
            return {
                'status': 'ABNORMAL',
                'error': str(e),
                'raw_output': '',
                'image_name': image_name,  # ç¡®ä¿åŒ…å«image_nameå­—æ®µ
                'method': method,  # ç¡®ä¿åŒ…å«methodå­—æ®µ
                'profile_id': profile['name']  # ç¡®ä¿åŒ…å«profile_idå­—æ®µ
            }
    
    def _get_image_features(self, image_name: str) -> Dict[str, Any]:
        """
        è·å–é•œåƒé™æ€ç‰¹å¾
        """
        # æ¨¡æ‹Ÿè·å–é•œåƒç‰¹å¾ï¼Œå®é™…åº”ç”¨ä¸­åº”ä»é…ç½®æˆ–é•œåƒåˆ†æä¸­è·å–
        return {
            'compressed_size': 100000000,  # 100MB
            'uncompressed_size': 250000000,  # 250MB
            'layer_count': 5
        }
    
    def run_experiment_matrix(self, 
                            replications: int = 3, 
                            parallel: bool = False) -> List[Dict[str, Any]]:
        """
        è¿è¡Œå®éªŒçŸ©é˜µ
        å®ç°ä¸‰çº§å¾ªç¯ï¼šå®¢æˆ·ç«¯é…ç½® -> ç›®æ ‡é•œåƒ -> å‹ç¼©æ–¹æ³• -> é‡å¤æ¬¡æ•°
        """
        print("å¼€å§‹è¿è¡Œå®éªŒçŸ©é˜µ...")
        print(f"å®¢æˆ·ç«¯é…ç½®æ•°: {len(self.client_profiles)}")
        print(f"ç›®æ ‡é•œåƒæ•°: {len(self.target_images)}")
        print(f"å‹ç¼©æ–¹æ³•æ•°: {len(self.compression_methods)}")
        print(f"é‡å¤æ¬¡æ•°: {replications}")
        total = len(self.client_profiles) * len(self.target_images) * len(self.compression_methods) * replications
        print(f"ğŸš€ å¯åŠ¨æ­£å¼å®éªŒçŸ©é˜µ | é¢„ä¼°æ€»é‡: {total}")
        
        all_results = []
        completed_count = 0
        total_experiments = total
        
        for profile in self.client_profiles:
            if not self.running:
                break
                
            print(f"\n=== å¼€å§‹å¤„ç†å®¢æˆ·ç«¯é…ç½®: {profile['name']} ({profile['description']}) ===")
            
            # 1. å¯åŠ¨å®¹å™¨å‰ï¼Œå…ˆæ¸…ç†ä¸€é Docker ç³»ç»Ÿç¼“å­˜
            os.system("docker system prune -f > /dev/null")
            
            # å¯åŠ¨æ¨¡æ‹Ÿå®¹å™¨
            container = self._setup_emulated_container(profile)
            if not container: 
                print(f"å®¹å™¨å¯åŠ¨å¤±è´¥: {profile['name']}")
                continue
            
            # 2. è·å– veth å¹¶è®¾ç½®ç½‘ç»œé™åˆ¶
            if not self.cloud_mode:
                veth = self._get_container_veth_safe(container.id)
                if veth:
                    self._setup_tc_limit(veth, profile['bandwidth_mbps'], profile['network_rtt'])
            else:
                # äº‘æ¨¡å¼ä¸‹ä½¿ç”¨Pumba
                self._setup_pumba_limit(container.id, profile['bandwidth_mbps'], profile['network_rtt'])
            
            try:
                for image in self.target_images:
                    if not self.running:
                        break
                        
                    for method in self.compression_methods:
                        if not self.running:
                            break
                            
                        for rep in range(replications):
                            if not self.running:
                                break
                                
                            # ç”Ÿæˆå®éªŒUUID
                            exp_id = f"{profile['name']}_{image['name']}_{method}_rep{rep}"
                            
                            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
                            if exp_id in self.completed_experiments:
                                print(f"è·³è¿‡å·²å®Œæˆå®éªŒ: {exp_id}")
                                completed_count += 1
                                continue
                            
                            print(f"æ‰§è¡Œå®éªŒ {completed_count + 1}/{total_experiments}: {exp_id}")
                            
                            # 3. æ ¸å¿ƒï¼šæ¯æ¬¡æ‹‰å–å‰å¿…é¡»åˆ é™¤å®¹å™¨å†…çš„é•œåƒç¼“å­˜
                            # æ¨¡æ‹Ÿ"å†·å¯åŠ¨"æ‹‰å–
                            img_url = f"{self.registry_url}/{image['name']}"
                            try:
                                container.exec_run(f"docker rmi -f {img_url}", detach=False)
                            except:
                                pass  # å¿½ç•¥æ¸…ç†é”™è¯¯
                            
                            # 4. æ¸…ç†ç³»ç»Ÿç¼“å­˜
                            self._clear_system_cache()
                            
                            # æ‰§è¡Œå¸¦ç›‘æ§çš„å®éªŒ
                            result = self.run_profiled_experiment(container, img_url, method, profile)
                            
                            # è®°å½•å®éªŒç»“æœ
                            experiment_record = self._collect_experiment_metrics(
                                profile['name'], 
                                image['name'], 
                                method, 
                                rep, 
                                time.time(),  # start_time
                                time.time(),  # end_time
                                result.get('decompression_time', 0),
                                result.get('error'),
                                result  # ä¼ é€’ç›‘æ§æ•°æ®
                            )
                            
                            # åˆå¹¶ç»“æœ
                            experiment_record.update(result)
                            
                            # æ ‡è®°å®éªŒå®Œæˆ
                            self.completed_experiments.add(exp_id)
                            all_results.append(experiment_record)
                            
                            completed_count += 1
                            
                            # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
                            time.sleep(0.5)
                            
            finally:
                # åœæ­¢å¹¶åˆ é™¤å®¹å™¨
                try:
                    container.stop()
                    container.remove()
                    print(f"å®¹å™¨å·²æ¸…ç†: {profile['name']}")
                except:
                    pass
        
        print(f"\nå®éªŒçŸ©é˜µå®Œæˆï¼å…±æ‰§è¡Œ {len(all_results)} ä¸ªå®éªŒ")
        return all_results
    
    def aggregate_results(self) -> Dict[str, Any]:
        """èšåˆå®éªŒç»“æœ"""
        print("æ­£åœ¨èšåˆå®éªŒç»“æœ...")
        
        results = []
        data_files = [f for f in os.listdir(self.data_dir) if f.endswith('.json') and f != 'completed_experiments.json']
        
        for filename in data_files:
            filepath = os.path.join(self.data_dir, filename)
            try:
                with open(filepath, 'r') as f:
                    data = json.load(f)
                    results.append(data)
            except Exception as e:
                print(f"è¯»å–æ–‡ä»¶ {filename} å¤±è´¥: {e}")
        
        # æŒ‰å®éªŒé…ç½®åˆ†ç»„
        grouped_results = {}
        for result in results:
            key = f"{result.get('profile_id', 'unknown')}_{result.get('image_name', 'unknown')}_{result.get('method', 'unknown')}"
            if key not in grouped_results:
                grouped_results[key] = []
            grouped_results[key].append(result)
        
        # è®¡ç®—æ¯ç»„çš„ç»Ÿè®¡ä¿¡æ¯
        aggregated = {}
        for key, group in grouped_results.items():
            # ä¿®å¤åŸä»£ç ä¸­çš„bugï¼šä½¿ç”¨cost_totalè€Œä¸æ˜¯æœªå®šä¹‰çš„duration
            durations = [r['cost_total'] for r in group if r.get('status') == 'SUCCESS']
            
            if durations and len(durations) > 0:
                mean_duration = sum(durations) / len(durations)
                
                # è®¡ç®—æ–¹å·®ï¼Œç”¨äºæ£€æµ‹å¼‚å¸¸æ•°æ®
                variance = sum((x - mean_duration) ** 2 for x in durations) / len(durations) if len(durations) > 1 else 0
                std_dev = variance ** 0.5
                
                # è®¡ç®—å˜å¼‚ç³»æ•°
                cv = (std_dev / mean_duration) * 100 if mean_duration > 0 else 0  # å˜å¼‚ç³»æ•°
                
                # æ ‡è®°å˜å¼‚ç³»æ•°è¿‡å¤§çš„ç»„
                high_cv = cv > 15  # å˜å¼‚ç³»æ•°è¶…è¿‡15%
                
                # ä¿®å¤åŸä»£ç ä¸­çš„bugï¼šå®šä¹‰valid_durations
                valid_durations = durations
                
                aggregated[key] = {
                    'count': len(valid_durations),
                    'mean_duration': mean_duration,
                    'min_duration': min(valid_durations),
                    'max_duration': max(valid_durations),
                    'std_deviation': std_dev,
                    'variance': variance,
                    'cv': cv,  # å˜å¼‚ç³»æ•°
                    'high_cv': high_cv,  # é«˜å˜å¼‚ç³»æ•°æ ‡è®°
                    'replications': valid_durations
                }
        
        print(f"èšåˆäº† {len(aggregated)} ç»„å®éªŒç»“æœ")
        return aggregated


def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œå®éªŒçŸ©é˜µ"""
    print("å¯åŠ¨å®éªŒç¼–æ’ç³»ç»Ÿ...")
    
    # åˆ›å»ºç¼–æ’å™¨
    orchestrator = ExperimentOrchestrator(cloud_mode=True)  # é»˜è®¤ä½¿ç”¨äº‘æ¨¡å¼
    
    # è¿è¡Œå®éªŒçŸ©é˜µ
    results = orchestrator.run_experiment_matrix(replications=3)
    
    # èšåˆç»“æœ
    aggregated = orchestrator.aggregate_results()
    
    # ä¿å­˜èšåˆç»“æœ
    output_file = os.path.join(orchestrator.data_dir, "aggregated_results.json")
    with open(output_file, 'w') as f:
        json.dump(aggregated, f, indent=2)
    
    print(f"èšåˆç»“æœå·²ä¿å­˜è‡³: {output_file}")
    print("å®éªŒç¼–æ’å®Œæˆï¼")


if __name__ == "__main__":
    main()