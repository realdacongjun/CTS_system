import os
import json
import math
import shutil
import tarfile
import docker
import time
import pandas as pd
import numpy as np
from collections import Counter
from tqdm import tqdm

# ==============================================================================
# 1. é…ç½®åŒºåŸŸï¼šä½ çš„ç›®æ ‡é•œåƒåˆ—è¡¨
# ==============================================================================
TARGET_IMAGES = [
    'quay.io/centos/centos:stream9', 
    'fedora:latest', 
    'ubuntu:latest',
    'mongo:latest', 
    'mysql:latest', 
    'postgres:latest',
    'rust:latest', 
    'ruby:latest', 
    'python:latest',
    'nginx:latest', 
    'httpd:latest', 
    'rabbitmq:latest', 
    'wordpress:latest', 
    'nextcloud:latest',
    'gradle:latest', 
    'node:latest'
]


TEMP_DIR = "temp_feature_extraction_v2"
SAMPLE_SIZE_BYTES = 20 * 1024 * 1024  # å¢åŠ é‡‡æ ·åˆ° 20MB ä»¥è·å¾—æ›´å‡†çš„åˆ†å¸ƒ
MAX_LAYERS_TO_KEEP = 3  # æå– Top-3 å¤§å±‚çš„ç‹¬ç«‹ç‰¹å¾ç»™ Attention ç”¨

class FeatureExtractor:
    def __init__(self):
        try:
            self.client = docker.from_env()
            print("âœ… Docker Client è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Docker è¿æ¥å¤±è´¥: {e}")
            exit(1)
        
        if os.path.exists(TEMP_DIR):
            shutil.rmtree(TEMP_DIR)
        os.makedirs(TEMP_DIR)

    def process_all(self, image_list):
        results = []
        print(f"ğŸš€ å¼€å§‹æ·±åº¦ç‰¹å¾æå–ï¼ˆé’ˆå¯¹ Attention ä¼˜åŒ–ç‰ˆï¼‰...")
        
        for img in tqdm(image_list, desc="Processing Images"):
            try:
                self._ensure_image(img)
                features = self._analyze_image(img)
                if features:
                    results.append(features)
            except Exception as e:
                print(f"\nâŒ å¤„ç† {img} å¤±è´¥: {e}")
        
        self._save_results(results)

    def _ensure_image(self, image_name):
        try:
            self.client.images.get(image_name)
        except docker.errors.ImageNotFound:
            print(f"\nâ¬‡ï¸ æ‹‰å– {image_name}...")
            self.client.images.pull(image_name)

    def _analyze_image(self, image_name):
        clean_name = image_name.replace("/", "_").replace(":", "_")
        extract_path = os.path.join(TEMP_DIR, clean_name)
        os.makedirs(extract_path, exist_ok=True)
        
        try:
            # 1. å¯¼å‡ºä¸è§£å‹
            img_obj = self.client.images.get(image_name)
            tar_stream = img_obj.save()
            tar_file = os.path.join(extract_path, "image.tar")
            
            with open(tar_file, 'wb') as f:
                for chunk in tar_stream:
                    f.write(chunk)
            
            with tarfile.open(tar_file) as tar:
                tar.extractall(path=extract_path)
            os.remove(tar_file)

            # 2. è§£æ Manifest
            manifest_path = os.path.join(extract_path, "manifest.json")
            if not os.path.exists(manifest_path): return None
            
            with open(manifest_path) as f:
                manifest = json.load(f)[0]
            
            # 3. é€å±‚æ·±åº¦åˆ†æ
            layer_stats = []
            blobs_dir = os.path.join(extract_path, "blobs", "sha256")
            
            for layer_file in manifest.get('Layers', []):
                layer_path = self._find_layer(extract_path, blobs_dir, layer_file)
                if layer_path:
                    stat = self._compute_advanced_features(layer_path)
                    layer_stats.append(stat)

            # 4. ç”Ÿæˆé«˜ç»´ç‰¹å¾å‘é‡
            return self._construct_high_dim_features(image_name, layer_stats)

        finally:
            if os.path.exists(extract_path):
                shutil.rmtree(extract_path)

    def _find_layer(self, root, blobs, filename):
        p1 = os.path.join(root, filename)
        if os.path.exists(p1): return p1
        p2 = os.path.join(blobs, os.path.basename(filename))
        if os.path.exists(p2): return p2
        return None

    def _compute_advanced_features(self, filepath):
        """è®¡ç®—æ›´ä¸°å¯Œçš„å•å±‚ç‰©ç†å±æ€§"""
        size = os.path.getsize(filepath)
        if size == 0:
            return {'size': 0, 'entropy': 0, 'text_ratio': 0, 'is_compressed': 0, 'header_type': 0}

        read_len = min(size, SAMPLE_SIZE_BYTES)
        with open(filepath, 'rb') as f:
            data = f.read(read_len)
        
        # A. åŸºç¡€ç†µ
        counts = Counter(data)
        total = len(data)
        entropy = 0
        for count in counts.values():
            p = count / total
            entropy -= p * math.log2(p)
        entropy /= 8.0 

        # B. å­—èŠ‚åˆ†å¸ƒæŒ‡çº¹ (Byte Histogram Focus)
        # ç»Ÿè®¡ä¸å¯è§å­—ç¬¦æ¯”ä¾‹ï¼ˆäºŒè¿›åˆ¶ç‰¹å¾å¼ºï¼‰
        binary_chars = sum(1 for b in data if not (32 <= b <= 126 or b in (9, 10, 13)))
        binary_ratio = binary_chars / total

        # C. ç®€å•æ–‡ä»¶å¤´æ£€æµ‹ (Magic Number)
        # 0: Unknown, 1: Gzip (Already compressed), 2: ELF (Binary), 3: Text (Script)
        header_type = 0
        if len(data) > 4:
            if data.startswith(b'\x1f\x8b'): # Gzip
                header_type = 1
            elif data.startswith(b'\x7fELF'): # Linux Binary
                header_type = 2
            elif binary_ratio < 0.1: # Mostly text
                header_type = 3

        return {
            'size': size,
            'entropy': entropy,
            'text_ratio': 1.0 - binary_ratio,
            'header_type': header_type # è¿™æ˜¯ä¸€ä¸ªç±»åˆ«ç‰¹å¾ï¼Œé€‚åˆ Embedding
        }

    def _construct_high_dim_features(self, image_name, stats):
        """
        æ ¸å¿ƒå‡çº§ï¼šæ„é€ é€‚åˆ Attention çš„é«˜ç»´ç‰¹å¾
        """
        if not stats: return None

        # 1. å…¨å±€èšåˆç‰¹å¾ (ä¿æŒåŸæœ‰çš„ï¼Œä½œä¸º Base)
        total_size = sum(s['size'] for s in stats)
        avg_entropy = np.mean([s['entropy'] for s in stats])
        
        # æ–°å¢ï¼šå¼‚æ„æ€§ç‰¹å¾ (æ–¹å·®)
        # ç†µçš„æ–¹å·®å¤§ï¼Œè¯´æ˜å±‚ä¸å±‚ä¹‹é—´å·®å¼‚å¤§ï¼ˆAttention å–œæ¬¢è¿™ä¸ªï¼‰
        entropy_std = np.std([s['entropy'] for s in stats])
        size_std = np.std([s['size'] for s in stats])

        # 2. å±‚çº§ç‰¹å¾ (Layer-wise Features)
        # æˆ‘ä»¬æŒ‰å¤§å°æ’åºï¼Œå–æœ€å¤§çš„ N å±‚
        # ç†ç”±ï¼šæœ€å¤§çš„å±‚å†³å®šäº†è§£å‹æ€§èƒ½ç“¶é¢ˆï¼ŒAttention åº”è¯¥å…³æ³¨å®ƒä»¬
        sorted_stats = sorted(stats, key=lambda x: x['size'], reverse=True)
        
        feature_dict = {
            "image_name": image_name,
            "total_size_mb": round(total_size / (1024**2), 2),
            "layer_count": len(stats),
            "avg_layer_entropy": round(avg_entropy, 4),
            "entropy_std": round(entropy_std, 4), # æ–°ç‰¹å¾ï¼šç†µæ³¢åŠ¨
            "size_std_mb": round(size_std / (1024**2), 2),   # æ–°ç‰¹å¾ï¼šå¤§å°æ³¢åŠ¨
        }

        # 3. å±•å¹³ Top-N å±‚çš„ç‰¹å¾ (Feature Flattening)
        # è¿™å°±åƒç»™äº† Attention æ¨¡å‹ 3 ä¸ªå…·ä½“çš„â€œè§‚å¯Ÿç‚¹â€
        for i in range(MAX_LAYERS_TO_KEEP):
            prefix = f"L{i+1}"
            if i < len(sorted_stats):
                s = sorted_stats[i]
                feature_dict[f"{prefix}_size_mb"] = round(s['size'] / (1024**2), 2)
                feature_dict[f"{prefix}_entropy"] = round(s['entropy'], 4)
                feature_dict[f"{prefix}_type"] = s['header_type'] # ç±»åˆ«ç‰¹å¾
            else:
                # å¦‚æœå±‚æ•°ä¸å¤Ÿï¼Œå¡« 0 (Padding)
                feature_dict[f"{prefix}_size_mb"] = 0
                feature_dict[f"{prefix}_entropy"] = 0
                feature_dict[f"{prefix}_type"] = 0

        return feature_dict

    def _save_results(self, results):
        df = pd.DataFrame(results)
        print("\n" + "="*50)
        print("ğŸ“Š é«˜ç»´ç‰¹å¾åˆ†æç»“æœé¢„è§ˆ:")
        print("="*50)
        print(df.head().to_string())
        print(f"\nç‰¹å¾ç»´åº¦: {df.shape[1]} åˆ— (åŸç‰ˆæœ¬çº¦ 5 åˆ—)")
        
        csv_path = "image_features_database.csv"
        df.to_csv(csv_path, index=False)
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(csv_path)}")

if __name__ == "__main__":
    extractor = FeatureExtractor()
    extractor.process_all(TARGET_IMAGES)

# # ä¸´æ—¶å·¥ä½œç›®å½• (ç”¨å®Œä¼šåˆ é™¤)
# TEMP_DIR = "temp_feature_extraction"
# # é‡‡æ ·å¤§å° (è¯»å–æ¯å±‚çš„å‰ 10MB è¿›è¡Œåˆ†æï¼Œè¶³ä»¥ä»£è¡¨æ•´ä½“)
# SAMPLE_SIZE_BYTES = 10 * 1024 * 1024 

# class FeatureExtractor:
#     def __init__(self):
#         try:
#             self.client = docker.from_env()
#             print("âœ… Docker Client è¿æ¥æˆåŠŸ")
#         except Exception as e:
#             print(f"âŒ Docker è¿æ¥å¤±è´¥ï¼Œè¯·ç¡®ä¿ docker æœåŠ¡å·²å¯åŠ¨: {e}")
#             exit(1)
        
#         # ç¡®ä¿ç¯å¢ƒå¹²å‡€
#         if os.path.exists(TEMP_DIR):
#             shutil.rmtree(TEMP_DIR)
#         os.makedirs(TEMP_DIR)

#     def process_all(self, image_list):
#         results = []
#         print(f"ğŸš€ å¼€å§‹åˆ†æ {len(image_list)} ä¸ªé•œåƒçš„ç‰©ç†ç‰¹å¾...")
        
#         # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
#         for img in tqdm(image_list, desc="Processing Images"):
#             try:
#                 # 1. ç¡®ä¿é•œåƒå­˜åœ¨
#                 self._ensure_image(img)
#                 # 2. æå–ç‰¹å¾
#                 features = self._analyze_image(img)
#                 if features:
#                     results.append(features)
#             except Exception as e:
#                 print(f"\nâŒ å¤„ç† {img} å¤±è´¥: {e}")
        
#         # 3. ä¿å­˜ç»“æœ
#         self._save_results(results)

#     def _ensure_image(self, image_name):
#         """å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œå…ˆæ‹‰å–"""
#         try:
#             self.client.images.get(image_name)
#         except docker.errors.ImageNotFound:
#             print(f"\nâ¬‡ï¸ æ­£åœ¨æ‹‰å– {image_name} (è¿™å¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´)...")
#             self.client.images.pull(image_name)

#     def _analyze_image(self, image_name):
#         """æ ¸å¿ƒåˆ†æé€»è¾‘"""
#         clean_name = image_name.replace("/", "_").replace(":", "_")
#         extract_path = os.path.join(TEMP_DIR, clean_name)
#         os.makedirs(extract_path, exist_ok=True)
        
#         try:
#             # A. å¯¼å‡ºé•œåƒ (docker save)
#             img_obj = self.client.images.get(image_name)
#             tar_stream = img_obj.save()
#             tar_file = os.path.join(extract_path, "image.tar")
            
#             with open(tar_file, 'wb') as f:
#                 for chunk in tar_stream:
#                     f.write(chunk)
            
#             # B. è§£å‹
#             with tarfile.open(tar_file) as tar:
#                 tar.extractall(path=extract_path)
#             os.remove(tar_file) # çœç©ºé—´

#             # C. è§£æ Manifest æ‰¾ Layers
#             manifest_path = os.path.join(extract_path, "manifest.json")
#             if not os.path.exists(manifest_path):
#                 return None
            
#             with open(manifest_path) as f:
#                 manifest = json.load(f)[0]
            
#             # D. é€å±‚åˆ†æ
#             layer_stats = []
#             blobs_dir = os.path.join(extract_path, "blobs", "sha256")
            
#             for layer_file in manifest.get('Layers', []):
#                 # å…¼å®¹ä¸åŒçš„å­˜å‚¨è·¯å¾„ç»“æ„
#                 layer_path = self._find_layer(extract_path, blobs_dir, layer_file)
#                 if layer_path:
#                     stat = self._compute_file_features(layer_path)
#                     layer_stats.append(stat)

#             # E. èšåˆç‰¹å¾ (Weighted Average)
#             return self._aggregate_features(image_name, layer_stats)

#         finally:
#             # æ¸…ç†ï¼Œé˜²æ­¢ç£ç›˜çˆ†æ»¡ (Rust è§£å‹å‡ºæ¥å¾ˆå¤§)
#             if os.path.exists(extract_path):
#                 shutil.rmtree(extract_path)

#     def _find_layer(self, root, blobs, filename):
#         p1 = os.path.join(root, filename)
#         if os.path.exists(p1): return p1
#         p2 = os.path.join(blobs, os.path.basename(filename))
#         if os.path.exists(p2): return p2
#         return None

#     def _compute_file_features(self, filepath):
#         """è®¡ç®—å•ä¸ªæ–‡ä»¶çš„ç‰©ç†å±æ€§"""
#         size = os.path.getsize(filepath)
#         if size == 0:
#             return {'size': 0, 'entropy': 0, 'text_ratio': 0, 'zero_ratio': 0}

#         # é‡‡æ ·è¯»å–
#         read_len = min(size, SAMPLE_SIZE_BYTES)
#         with open(filepath, 'rb') as f:
#             data = f.read(read_len)
        
#         # 1. ç†µ (Entropy)
#         entropy = 0
#         if data:
#             counts = Counter(data)
#             total = len(data)
#             for count in counts.values():
#                 p = count / total
#                 entropy -= p * math.log2(p)
#             entropy /= 8.0 # å½’ä¸€åŒ– 0-1

#         # 2. æ–‡æœ¬ç‡ (Text Ratio)
#         text_chars = sum(1 for b in data if 32 <= b <= 126 or b in (9, 10, 13))
#         text_ratio = text_chars / len(data) if data else 0

#         # 3. ç¨€ç–ç‡ (Zero Ratio) - å¾ˆå¤šäºŒè¿›åˆ¶æ–‡ä»¶åŒ…å«å¤§é‡ç©ºæ´
#         zero_count = data.count(b'\x00')
#         zero_ratio = zero_count / len(data) if data else 0

#         return {
#             'size': size,
#             'entropy': entropy,
#             'text_ratio': text_ratio,
#             'zero_ratio': zero_ratio
#         }

#     def _aggregate_features(self, image_name, stats):
#         """åŠ æƒèšåˆï¼šå¤§å±‚çš„ç‰¹å¾å†³å®šæ•´ä½“ç‰¹å¾"""
#         total_size = sum(s['size'] for s in stats)
#         if total_size == 0: return None

#         # åŠ æƒå¹³å‡
#         avg_entropy = sum(s['entropy'] * s['size'] for s in stats) / total_size
#         avg_text = sum(s['text_ratio'] * s['size'] for s in stats) / total_size
#         avg_zero = sum(s['zero_ratio'] * s['size'] for s in stats) / total_size

#         return {
#             "image_name": image_name,
#             "total_size_mb": round(total_size / (1024**2), 2),
#             "layer_count": len(stats),
#             "avg_layer_entropy": round(avg_entropy, 4),
#             "text_ratio": round(avg_text, 4),
#             "zero_ratio": round(avg_zero, 4)  # ç¨€ç–åº¦
#         }

#     def _save_results(self, results):
#         df = pd.DataFrame(results)
#         print("\n" + "="*50)
#         print("ğŸ“Š ç‰¹å¾åˆ†æç»“æœé¢„è§ˆ:")
#         print("="*50)
#         print(df.to_string())
        
#         # ä¿å­˜ä¸º CSVï¼Œæ–¹ä¾¿åç»­ Dataset ç±»è¯»å–
#         csv_path = "image_features_database.csv"
#         df.to_csv(csv_path, index=False)
#         print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {os.path.abspath(csv_path)}")
#         print("ğŸ’¡ ä¸‹ä¸€æ­¥ï¼šåœ¨è®­ç»ƒä»£ç ä¸­åŠ è½½æ­¤æ–‡ä»¶ï¼Œä½¿ç”¨ 'pd.merge' å°†å…¶åˆå¹¶åˆ° experiments è®°å½•ä¸­ã€‚")

# if __name__ == "__main__":
#     extractor = FeatureExtractor()
#     extractor.process_all(TARGET_IMAGES)