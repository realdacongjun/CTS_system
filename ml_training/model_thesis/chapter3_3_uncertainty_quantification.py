import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import roc_auc_score, average_precision_score
import warnings
import os
import platform

warnings.filterwarnings('ignore')

# --- å­—ä½“è‡ªåŠ¨é…ç½® ---
system_name = platform.system()
if system_name == 'Windows':
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
else:
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ==============================================================================
# æ¨¡å‹å®šä¹‰ (ä¿æŒä¸€è‡´)
# ==============================================================================
class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
    def forward(self, x):
        return x.unsqueeze(-1) * self.weights + self.biases

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4, batch_first=True, dropout=0.1)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, x):
        tokens = self.tokenizer(x)
        batch_size = x.shape[0]
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        tokens = torch.cat((cls_tokens, tokens), dim=1)
        out = self.transformer(tokens)
        return out[:, 0, :]

class CTSDualTowerModel(nn.Module):
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = TransformerTower(client_feats, embed_dim)
        self.image_tower = TransformerTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        fusion_input_dim = embed_dim * 3 
        self.hidden = nn.Sequential(
            nn.Linear(fusion_input_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        self.head = nn.Linear(64, 4) 
    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        hidden = self.hidden(combined)
        out = self.head(hidden)
        gamma = out[:, 0]
        v     = F.softplus(out[:, 1]) + 1e-6
        alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
        beta  = F.softplus(out[:, 3]) + 1e-6
        return torch.stack([gamma, v, alpha, beta], dim=1)

# ==============================================================================
# ä¿®æ­£åçš„è¯„ä¼°å™¨
# ==============================================================================
class UncertaintyEvaluatorFixed:
    
    def __init__(self):
        self.model = None
        self.scaler_c = StandardScaler()
        self.scaler_i = StandardScaler()
        self.enc_algo = LabelEncoder()
        # ç‰¹å¾åˆ—å®šä¹‰
        self.col_client = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
        self.col_image = ['total_size_mb', 'avg_layer_entropy', 'text_ratio', 'layer_count', 'zero_ratio']
        
    def _find_file(self, filename):
        for path in [filename, os.path.join('..', 'modeling', filename), os.path.join('ml_training', 'modeling', filename)]:
            if os.path.exists(path): return path
        return filename

    def load_resources(self):
        """åŠ è½½æ•°æ®å’Œæ¨¡å‹"""
        print("æ­£åœ¨åŠ è½½èµ„æº...")
        data_path = self._find_file('cts_data.xlsx')
        feat_path = self._find_file('image_features_database.csv')
        model_path = self._find_file('cts_best_model_full_modified.pth')
        
        # 1. æ•°æ®å¤„ç†
        df_exp = pd.read_excel(data_path)
        df_feat = pd.read_csv(feat_path)
        
        rename_map = {"image": "image_name", "method": "algo_name", "network_bw": "bandwidth_mbps", "network_delay": "network_rtt", "mem_limit": "mem_limit_mb"}
        df_exp = df_exp.rename(columns=rename_map)
        if 'total_time' not in df_exp.columns:
            cols = [c for c in df_exp.columns if 'total_tim' in c]
            if cols: df_exp = df_exp.rename(columns={cols[0]: 'total_time'})
        
        df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
        if 'mem_limit_mb' not in df_exp.columns: df_exp['mem_limit_mb'] = 1024.0
        
        self.full_df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
        
        # 2. æ‹ŸåˆScaler
        self.scaler_c.fit(self.full_df[self.col_client].values)
        self.scaler_i.fit(self.full_df[self.col_image].values)
        self.enc_algo.fit(self.full_df['algo_name'].values)
        
        # 3. åŠ è½½æ¨¡å‹
        self.model = CTSDualTowerModel(
            client_feats=len(self.col_client),
            image_feats=len(self.col_image),
            num_algos=len(self.enc_algo.classes_),
            embed_dim=32
        )
        self.model.load_state_dict(torch.load(model_path, map_location='cpu'))
        self.model.eval()
        print("âœ… æ¨¡å‹ä¸æ•°æ®åŠ è½½å®Œæˆ")

    def create_calibrated_ood(self):
        """
        æ„é€ æ›´åˆç†çš„ OOD æ•°æ®ï¼š
        ä¸æ˜¯é€šè¿‡æ•°å€¼çˆ†ç‚¸ï¼Œè€Œæ˜¯é€šè¿‡'é€»è¾‘å†²çª'å’Œ'å™ªå£°æ‰°åŠ¨'
        """
        print("æ„é€  OOD æµ‹è¯•é›†...")
        
        # 1. ID æ•°æ® (In-Distribution): ä»çœŸå®æ•°æ®ä¸­é‡‡æ ·
        id_df = self.full_df.sample(n=1000, random_state=42).copy()
        id_df['is_ood'] = 0
        id_df['ood_type'] = 'ID (Normal)'
        
        # 2. OOD-1: å™ªå£°æ‰°åŠ¨ (Noisy Features)
        # ç»™ç‰¹å¾åŠ ä¸Šå¼ºé«˜æ–¯å™ªå£°ï¼Œä½¿å…¶è„±ç¦»åŸå§‹åˆ†å¸ƒï¼Œä½†ä¸è¿‡åˆ†
        ood_noise = self.full_df.sample(n=500, random_state=101).copy()
        for col in self.col_client + self.col_image:
            std = ood_noise[col].std()
            # åŠ ä¸Š 3å€æ ‡å‡†å·®çš„å™ªå£° -> ç»Ÿè®¡å­¦ä¸Šçš„å¼‚å¸¸å€¼
            ood_noise[col] = ood_noise[col] + np.random.normal(0, 3 * std, len(ood_noise))
        ood_noise['is_ood'] = 1
        ood_noise['ood_type'] = 'Noisy Input'
        
        # 3. OOD-2: é€»è¾‘å†²çª (Conflicting Features)
        # ä¾‹å¦‚ï¼šæé«˜å¸¦å®½(10000) ä½† RTT æé«˜(5000ms) -> ç‰©ç†ä¸ŠçŸ›ç›¾
        ood_conflict = self.full_df.sample(n=500, random_state=102).copy()
        ood_conflict['bandwidth_mbps'] = 10000.0  # è¶…å¿«ç½‘
        ood_conflict['network_rtt'] = 5000.0      # è¶…é«˜å»¶è¿Ÿ
        ood_conflict['total_size_mb'] = 0.1       # æå°æ–‡ä»¶
        ood_conflict['avg_layer_entropy'] = 0.01  # æä½ç†µ
        ood_conflict['is_ood'] = 1
        ood_conflict['ood_type'] = 'Logic Conflict'
        
        return pd.concat([id_df, ood_noise, ood_conflict], ignore_index=True)

    def predict(self, df):
        X_c = self.scaler_c.transform(df[self.col_client].values)
        X_i = self.scaler_i.transform(df[self.col_image].values)
        # å¤„ç†å¯èƒ½çš„æœªçŸ¥ç®—æ³•æ ‡ç­¾
        try:
            X_a = self.enc_algo.transform(df['algo_name'].values)
        except:
            # å¦‚æœOODæ„é€ å‡ºäº†æœªçŸ¥ç®—æ³•ï¼Œé»˜è®¤ç”¨0
            X_a = np.zeros(len(df), dtype=int)
            
        cx = torch.FloatTensor(X_c)
        ix = torch.FloatTensor(X_i)
        ax = torch.LongTensor(X_a)
        
        with torch.no_grad():
            preds = self.model(cx, ix, ax)
            gamma, v, alpha, beta = preds[:, 0], preds[:, 1], preds[:, 2], preds[:, 3]
            
            # [å…³é”®ä¿®æ”¹] ä½¿ç”¨æ€»æ–¹å·® (Total Variance)
            # Var[y] = Beta * (1 + v) / (v * (Alpha - 1))
            # åŒ…å« Aleatoric (æ•°æ®å™ªéŸ³) + Epistemic (æ¨¡å‹ä¸æ‡‚)
            # è¿™ç§æŒ‡æ ‡å¯¹ OOD æ£€æµ‹æ›´é²æ£’
            uncertainty = (beta * (1 + v)) / (v * (alpha - 1))
            
            pred_time = np.expm1(gamma.numpy())
            
        return pred_time, uncertainty.numpy()

    def run_evaluation(self):
        self.load_resources()
        test_df = self.create_calibrated_ood()
        
        preds, unc = self.predict(test_df)
        test_df['uncertainty'] = unc
        test_df['pred_time'] = preds
        
        # 1. æ‰“å°ç»Ÿè®¡æ•°æ®
        id_u = test_df[test_df['is_ood']==0]['uncertainty'].mean()
        ood_u = test_df[test_df['is_ood']==1]['uncertainty'].mean()
        auroc = roc_auc_score(test_df['is_ood'], test_df['uncertainty'])
        
        print("\n" + "="*40)
        print("ğŸ“Š ä¿®æ­£åçš„ä¸ç¡®å®šæ€§ç»Ÿè®¡")
        print("="*40)
        print(f"ID æ ·æœ¬å¹³å‡ä¸ç¡®å®šæ€§  : {id_u:.4f}")
        print(f"OOD æ ·æœ¬å¹³å‡ä¸ç¡®å®šæ€§ : {ood_u:.4f}")
        print(f"OOD æ£€æµ‹ AUROC       : {auroc:.4f}")
        print("="*40)
        
        if ood_u > id_u:
            print("âœ… éªŒè¯æˆåŠŸï¼šå¼‚å¸¸æ ·æœ¬çš„ä¸ç¡®å®šæ€§æ˜¾è‘—é«˜äºæ­£å¸¸æ ·æœ¬ï¼")
        else:
            print("âš ï¸ è­¦å‘Šï¼šOODä¸ç¡®å®šæ€§ä»æœªè¶…è¿‡IDï¼Œå¯èƒ½æ˜¯æ¨¡å‹å¯¹Logç©ºé—´æ–¹å·®çš„ç†è§£é—®é¢˜ã€‚")

        # 2. ç»˜åˆ¶å›¾è¡¨
        self.plot_results(test_df)
        
        # ä¿å­˜æ•°æ®
        stats = {
            'id_uncertainty': float(id_u),
            'ood_uncertainty': float(ood_u),
            'ood_auroc': float(auroc)
        }
        with open('chapter3_3_statistics.json', 'w') as f:
            json.dump(stats, f, indent=2)

    def plot_results(self, df):
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))
        
        # (a) åˆ†å¸ƒç›´æ–¹å›¾ (KDE)
        # æˆªæ–­æå€¼ä»¥ä¾¿ç»˜å›¾å¥½çœ‹ (å–95åˆ†ä½æ•°)
        limit = np.percentile(df['uncertainty'], 95)
        plot_data = df[df['uncertainty'] < limit]
        
        sns.kdeplot(data=plot_data[plot_data['is_ood']==0], x='uncertainty', fill=True, ax=axes[0], color='blue', label='ID (æ­£å¸¸)')
        sns.kdeplot(data=plot_data[plot_data['is_ood']==1], x='uncertainty', fill=True, ax=axes[0], color='red', label='OOD (å¼‚å¸¸)')
        axes[0].set_title('(a) è®¤çŸ¥ä¸ç¡®å®šæ€§åˆ†å¸ƒå¯†åº¦ä¼°è®¡', fontsize=14)
        axes[0].set_xlabel('æ€»é¢„æµ‹ä¸ç¡®å®šæ€§ (Total Variance)', fontsize=12)
        axes[0].legend()
        
        # (b) è¯¯å·®ç›¸å…³æ€§ (åªçœ‹ ID æ•°æ®ï¼Œè¯æ˜æ¨¡å‹çŸ¥é“è‡ªå·±å“ªé‡Œä¸å‡†)
        id_df = df[df['is_ood']==0].copy()
        id_df['abs_error'] = np.abs(id_df['total_time'] - id_df['pred_time'])
        # åˆ†ç®±è®¡ç®—
        id_df['unc_bin'] = pd.qcut(id_df['uncertainty'], q=10, duplicates='drop')
        bin_stats = id_df.groupby('unc_bin').agg({'abs_error': 'mean', 'uncertainty': 'mean'}).reset_index()
        
        sns.regplot(data=bin_stats, x='uncertainty', y='abs_error', ax=axes[1], 
                    scatter_kws={'s':100, 'alpha':0.7}, line_kws={'color':'red'})
        axes[1].set_title('(b) é¢„æµ‹è¯¯å·®ä¸ä¸ç¡®å®šæ€§çš„ç›¸å…³æ€§ (AUSEéªŒè¯)', fontsize=14)
        axes[1].set_xlabel('å¹³å‡ä¸ç¡®å®šæ€§ (Bin)', fontsize=12)
        axes[1].set_ylabel('å¹³å‡ç»å¯¹è¯¯å·® (MAE)', fontsize=12)
        axes[1].grid(True, linestyle=':', alpha=0.6)
        
        plt.tight_layout()
        plt.savefig('figure_3_5_uncertainty_analysis_fixed.png', dpi=300)
        print("âœ… å›¾è¡¨å·²ç”Ÿæˆ: figure_3_5_uncertainty_analysis_fixed.png")

if __name__ == "__main__":
    evaluator = UncertaintyEvaluatorFixed()
    evaluator.run_evaluation()