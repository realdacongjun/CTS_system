# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# import pandas as pd
# import numpy as np
# import os
# import json
# # --- ã€æ–°å¢ã€‘ç»˜å›¾ç›¸å…³åº“ ---
# import matplotlib.pyplot as plt
# import matplotlib
# import platform
# # ------------------------
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# # ==============================================================================
# # 0. ã€æ–°å¢ã€‘ç»˜å›¾é…ç½® (è§£å†³ä¸­æ–‡å­—ä½“å’Œè´Ÿå·)
# # ==============================================================================
# system_name = platform.system()
# if system_name == 'Windows':
#     font_list = ['Microsoft YaHei', 'SimHei', 'SimSun']
# elif system_name == 'Darwin':
#     font_list = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS']
# else:
#     font_list = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'SimHei']
# matplotlib.rcParams['font.sans-serif'] = font_list
# matplotlib.rcParams['axes.unicode_minus'] = False

# # ==============================================================================
# # 1. åŸºç¡€é…ç½®ä¸ç»„ä»¶ (å¤ç”¨ train.py)
# # ==============================================================================
# # CONFIG = {
# #     "data_path": "E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx",         
# #     "feature_path": "E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv",
# #     "batch_size": 64,
# #     "lr": 0.001,
# #     "epochs": 150,  # æ¶ˆèå®éªŒå¯ä»¥ç¨å¾®å°‘è·‘å‡ è½®ï¼Œ150è¶³å¤Ÿæ”¶æ•›
# #     "embed_dim": 32,
# #     "kl_coeff": 0.15,
# #     "plot_filename": "figure_3_6_component_contribution_real.png", # ã€æ–°å¢ã€‘å›¾ç‰‡æ–‡ä»¶å
# #     "json_filename": "ablation_results_final.json" # ã€æ–°å¢ã€‘JSONæ–‡ä»¶å
# # }
# CONFIG = {
#     "data_path": "E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx",         
#     "feature_path": "E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv",
#     "batch_size": 32,       # [ä¿®æ”¹] 64 -> 32 (å°Batché€šå¸¸æ³›åŒ–æ›´å¥½ï¼Œèƒ½å¸®å¤§æ¨¡å‹è·³å‡ºå±€éƒ¨æœ€ä¼˜)
#     "lr": 0.0005,           # [ä¿®æ”¹] 0.001 -> 0.0005 (æ…¢å·¥å‡ºç»†æ´»)
#     "epochs": 300,          # [ä¿®æ”¹] 150 -> 300 (ç»™åŒå¡”æ¨¡å‹æ›´å¤šæ—¶é—´è¿½èµ¶)
#     "embed_dim": 32,
#     "kl_coeff": 0.01,       # [ä¿®æ”¹] 0.15 -> 0.01 (å¤§å¹…é™ä½ï¼å…ˆè®©RMSEé™ä¸‹æ¥ï¼Œä¸è¦è¿‡åˆ†å…³æ³¨ä¸ç¡®å®šæ€§)
#     "plot_filename": "figure_3_6_component_contribution_real.png",
#     "json_filename": "ablation_results_final.json" 
# }

# # è·¯å¾„è‡ªé€‚åº”
# if not os.path.exists(CONFIG["data_path"]):
#     if os.path.exists(f"../{CONFIG['data_path']}"):
#         CONFIG["data_path"] = f"../{CONFIG['data_path']}"
#         CONFIG["feature_path"] = f"../{CONFIG['feature_path']}"

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # --- æŸå¤±å‡½æ•° (NIG Loss) ---
# def nig_nll_loss(y, gamma, v, alpha, beta):
#     two_blambda = 2 * beta * (1 + v)
#     nll = 0.5 * torch.log(np.pi / v) \
#         - alpha * torch.log(two_blambda) \
#         + (alpha + 0.5) * torch.log(v * (y - gamma)**2 + two_blambda) \
#         + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
#     return nll.mean()

# def nig_reg_loss(y, gamma, v, alpha, beta):
#     error = torch.abs(y - gamma)
#     evidence = 2 * v + alpha
#     return (error * evidence).mean()

# def evidential_loss(pred, target, epoch, total_epochs):
#     gamma, v, alpha, beta = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
#     target = target.view(-1)
#     loss_nll = nig_nll_loss(target, gamma, v, alpha, beta)
#     loss_reg = nig_reg_loss(target, gamma, v, alpha, beta)
#     annealing_coef = min(1.0, epoch / (total_epochs * 0.15))
#     return loss_nll + CONFIG["kl_coeff"] * annealing_coef * loss_reg

# # --- åŸºç¡€æ¨¡å— ---
# class FeatureTokenizer(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
#         self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
#         nn.init.xavier_uniform_(self.weights)
#         nn.init.zeros_(self.biases)
#     def forward(self, x):
#         return x.unsqueeze(-1) * self.weights + self.biases

# # ==============================================================================
# # 2. æ¨¡å‹å˜ä½“å®šä¹‰ (The Variants - åŸºäºå®Œå…¨ä½“ä¿®æ”¹)
# # ==============================================================================

# # --- å˜ä½“ A: å®Œå…¨ä½“ (Ours: Full CFT-Net) ---
# class TransformerTower(nn.Module):
#     def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
#         super().__init__()
#         self.tokenizer = FeatureTokenizer(num_features, embed_dim)
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
#         encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4, batch_first=True, dropout=0.1)
#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
#     def forward(self, x):
#         tokens = self.tokenizer(x)
#         batch_size = x.shape[0]
#         cls_tokens = self.cls_token.expand(batch_size, -1, -1)
#         tokens = torch.cat((cls_tokens, tokens), dim=1)
#         out = self.transformer(tokens)
#         return out[:, 0, :]

# class FullCFTNet(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32, output_dim=4):
#         super().__init__()
#         self.client_tower = TransformerTower(client_feats, embed_dim)
#         self.image_tower = TransformerTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.head = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64), nn.ReLU(), nn.Dropout(0.2),
#             nn.Linear(64, output_dim)
#         )
#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         out = self.head(torch.cat([c_vec, i_vec, a_vec], dim=1))
#         # å¦‚æœè¾“å‡ºæ˜¯4ç»´ï¼Œè¯´æ˜æ˜¯NIGåˆ†å¸ƒå‚æ•°
#         if out.shape[1] == 4:
#             gamma = out[:, 0]
#             v = F.softplus(out[:, 1]) + 1e-6
#             alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
#             beta = F.softplus(out[:, 3]) + 1e-6
#             return torch.stack([gamma, v, alpha, beta], dim=1)
#         return out

# # --- å˜ä½“ B: å»æ‰ Transformer (ç”¨ MLP æ›¿ä»£) ---
# class MLPTower(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         # ç®€å•çš„å…¨è¿æ¥å±‚ï¼Œæ²¡æœ‰ Self-Attention
#         self.net = nn.Sequential(
#             nn.Linear(num_features, embed_dim * 2),
#             nn.ReLU(),
#             nn.Linear(embed_dim * 2, embed_dim)
#         )
#     def forward(self, x):
#         return self.net(x)

# class CFTNet_NoTransformer(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = MLPTower(client_feats, embed_dim)
#         self.image_tower = MLPTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.head = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64), nn.ReLU(), nn.Dropout(0.2),
#             nn.Linear(64, 4)
#         )
#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         out = self.head(torch.cat([c_vec, i_vec, a_vec], dim=1))
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 1e-6
#         alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
#         beta = F.softplus(out[:, 3]) + 1e-6
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# # --- å˜ä½“ C: å»æ‰åŒå¡”ç»“æ„ (å•å¡” Transformer) ---
# class SingleTowerTransformer(nn.Module):
#     def __init__(self, total_features, num_algos, embed_dim=32):
#         super().__init__()
#         # æŠŠæ‰€æœ‰ç‰¹å¾æ‹¼åœ¨ä¸€èµ· Tokenizer
#         self.tokenizer = FeatureTokenizer(total_features, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
#         encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4, batch_first=True)
#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        
#         self.head = nn.Sequential(
#             nn.Linear(embed_dim, 64), # è¿™é‡Œåªæœ‰ embed_dim (å› ä¸ºåªæœ‰1ä¸ªCLS token)
#             nn.LayerNorm(64), nn.ReLU(), nn.Dropout(0.2),
#             nn.Linear(64, 4)
#         )

#     def forward(self, cx, ix, ax):
#         # 1. æ‹¼æ¥åŸå§‹ç‰¹å¾
#         combined_features = torch.cat([cx, ix], dim=1)
#         tokens = self.tokenizer(combined_features)
#         # 2. åŠ ä¸Šç®—æ³• Embedding ä½œä¸ºé¢å¤–çš„ Token
#         a_vec = self.algo_embed(ax).unsqueeze(1) # [Batch, 1, Dim]
#         # 3. åŠ ä¸Š CLS Token
#         batch_size = cx.shape[0]
#         cls_tokens = self.cls_token.expand(batch_size, -1, -1)
#         # 4. å…¨éƒ¨æ‹¼æˆåºåˆ—: [CLS, Algo, Feat1, Feat2...]
#         full_seq = torch.cat((cls_tokens, a_vec, tokens), dim=1)
#         out_seq = self.transformer(full_seq)
#         cls_out = out_seq[:, 0, :] # å– CLS
#         out = self.head(cls_out)
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 1e-6
#         alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
#         beta = F.softplus(out[:, 3]) + 1e-6
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# # ==============================================================================
# # 3. è®­ç»ƒä¸è¯„ä¼°é€»è¾‘
# # ==============================================================================
# class CTSDataset(Dataset):
#     def __init__(self, cx, ix, ax, y):
#         self.cx = torch.FloatTensor(cx)
#         self.ix = torch.FloatTensor(ix)
#         self.ax = torch.LongTensor(ax)
#         self.y = torch.FloatTensor(y)
#     def __len__(self): return len(self.y)
#     def __getitem__(self, i): return self.cx[i], self.ix[i], self.ax[i], self.y[i]

# def train_variant(model_name, model, train_loader, test_loader, use_nig_loss=True):
#     print(f"\nâš¡ æ­£åœ¨è®­ç»ƒå˜ä½“: {model_name}")
#     model = model.to(device)
#     optimizer = optim.Adam(model.parameters(), lr=CONFIG["lr"])
#     criterion_mse = nn.MSELoss()
    
#     # ç®€å•çš„è¿›åº¦æ˜¾ç¤º
#     for epoch in range(CONFIG["epochs"]):
#         model.train()
#         for cx, ix, ax, y in train_loader:
#             cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
#             optimizer.zero_grad()
#             preds = model(cx, ix, ax)
            
#             if use_nig_loss:
#                 loss = evidential_loss(preds, y, epoch, CONFIG["epochs"])
#             else:
#                 # w/o Uncertainty å˜ä½“ï¼Œåªè¾“å‡º1ä¸ªå€¼
#                 loss = criterion_mse(preds.squeeze(), y)
                
#             loss.backward()
#             optimizer.step()
#         if (epoch + 1) % 50 == 0:
#             print(f"  Epoch {epoch+1}/{CONFIG['epochs']} done.")
            
#     # è¯„ä¼°é˜¶æ®µ
#     model.eval()
#     preds_list, true_list = [], []
#     with torch.no_grad():
#         for cx, ix, ax, y in test_loader:
#             cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
#             preds = model(cx, ix, ax)
            
#             if use_nig_loss:
#                 gamma = preds[:, 0] # å–é¢„æµ‹å‡å€¼
#             else:
#                 gamma = preds.squeeze()
                
#             preds_list.extend(np.expm1(gamma.cpu().numpy())) # Logåå˜æ¢
#             true_list.extend(np.expm1(y.cpu().numpy()))
            
#     rmse = np.sqrt(mean_squared_error(true_list, preds_list))
#     mae = mean_absolute_error(true_list, preds_list) # ã€æ–°å¢ã€‘è®¡ç®— MAE
#     r2 = r2_score(true_list, preds_list)
#     print(f"âœ… {model_name} å®Œæˆ -> RMSE: {rmse:.4f}, MAE: {mae:.4f}, RÂ²: {r2:.4f}")
#     # ã€ä¿®æ”¹ã€‘è¿”å› MAE
#     return {"rmse": rmse, "mae": mae, "r2": r2}

# # ==============================================================================
# # 4. ã€æ–°å¢ã€‘ç»˜å›¾å‡½æ•° (ä¿æŒé£æ ¼ä¸€è‡´)
# # ==============================================================================
# def generate_component_contribution_plot(results, filename):
#     """ç”Ÿæˆç»„ä»¶è´¡çŒ®åˆ†æå›¾ (3ä¸ªå­å›¾: RMSE, MAE, RÂ²)"""
#     print(f"\nğŸ“Š æ­£åœ¨ç”Ÿæˆç»„ä»¶è´¡çŒ®åˆ†æå›¾: {filename} ...")
#     fig, axes = plt.subplots(1, 3, figsize=(18, 6))
#     fig.suptitle('å›¾3.6 ç»„ä»¶è´¡çŒ®åˆ†æï¼ˆåŸºäºçœŸå®æ•°æ® - ä¸¥è°¨æ¶ˆèå®éªŒï¼‰', fontsize=16, fontweight='bold')
    
#     # æå–æ•°æ®
#     variants = list(results.keys())
#     rmses = [results[v]['rmse'] for v in variants]
#     maes = [results[v]['mae'] for v in variants]
#     r2s = [results[v]['r2'] for v in variants]
    
#     # å®šä¹‰é¢œè‰² (çªå‡ºæ˜¾ç¤º Ours)
#     # Oursç”¨ç»¿è‰²ï¼Œå…¶ä»–ç”¨ä¸åŒæ·±æµ…çš„çº¢è‰²/æ©™è‰²
#     colors = ['#2ca02c'] + ['#d62728', '#ff7f0e', '#9467bd'][0:len(variants)-1]

#     # é€šç”¨æ ‡æ³¨å‡½æ•°
#     def annotate_bars(ax, bars):
#         for bar in bars:
#             height = bar.get_height()
#             ax.annotate(f'{height:.3f}', 
#                         xy=(bar.get_x() + bar.get_width()/2, height),
#                         xytext=(0, 3), textcoords="offset points",
#                         ha='center', va='bottom', fontsize=10, fontweight='bold')

#     # å›¾1: RMSEå¯¹æ¯”
#     bars1 = axes[0].bar(variants, rmses, color=colors, alpha=0.8)
#     axes[0].set_title('RMSEå¯¹æ¯” (è¶Šä½è¶Šå¥½)', fontweight='bold')
#     axes[0].set_ylabel('å‡æ–¹æ ¹è¯¯å·® (ç§’)')
#     axes[0].tick_params(axis='x', rotation=20)
#     annotate_bars(axes[0], bars1)
    
#     # å›¾2: MAEå¯¹æ¯”
#     bars2 = axes[1].bar(variants, maes, color=colors, alpha=0.8)
#     axes[1].set_title('MAEå¯¹æ¯” (è¶Šä½è¶Šå¥½)', fontweight='bold')
#     axes[1].set_ylabel('å¹³å‡ç»å¯¹è¯¯å·® (ç§’)')
#     axes[1].tick_params(axis='x', rotation=20)
#     annotate_bars(axes[1], bars2)
    
#     # å›¾3: RÂ²å¯¹æ¯”
#     bars3 = axes[2].bar(variants, r2s, color=colors, alpha=0.8)
#     axes[2].set_title('RÂ²å¯¹æ¯” (è¶Šé«˜è¶Šå¥½)', fontweight='bold')
#     axes[2].set_ylabel('å†³å®šç³»æ•°')
#     axes[2].tick_params(axis='x', rotation=20)
#     # R2 çš„Yè½´é™åˆ¶åœ¨ 0-1 ä¹‹é—´çœ‹èµ·æ¥æ›´ç›´è§‚ï¼Œé™¤éæœ‰è´Ÿæ•°
#     axes[2].set_ylim(bottom=min(0, min(r2s))*1.1, top=min(1.0, max(r2s)*1.05))
#     annotate_bars(axes[2], bars3)
    
#     plt.tight_layout()
#     plt.savefig(filename, dpi=300, bbox_inches='tight')
#     plt.close()
#     print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆ!")

# # ==============================================================================
# # 5. ä¸»ç¨‹åº
# # ==============================================================================
# if __name__ == "__main__":
#     print("=== å¼€å§‹ä¸¥è°¨ç‰ˆæ¶ˆèå®éªŒ (åŸºäº Full CFT-Net) ===")
#     # --- æ•°æ®å‡†å¤‡ (å’Œ train.py å®Œå…¨ä¸€è‡´) ---
#     print("ğŸ”„ åŠ è½½æ•°æ®...")
#     df = pd.read_excel(CONFIG["data_path"])
#     df_feat = pd.read_csv(CONFIG["feature_path"])
    
#     # ç®€å•çš„é¢„å¤„ç†
#     rename_map = {"image": "image_name", "method": "algo_name", "network_bw": "bandwidth_mbps", "network_delay": "network_rtt", "mem_limit": "mem_limit_mb"}
#     df = df.rename(columns=rename_map)
#     if 'total_time' not in df.columns: 
#         cols = [c for c in df.columns if 'total_tim' in c]
#         if cols: df = df.rename(columns={cols[0]: 'total_time'})
    
#     df = df[(df['status'] == 'SUCCESS') & (df['total_time'] > 0)]
#     if 'mem_limit_mb' not in df.columns: df['mem_limit_mb'] = 1024.0
#     df = pd.merge(df, df_feat, on="image_name", how="inner")
    
#     col_client = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#     # ä½¿ç”¨ entropy_std (ç†µæ ‡å‡†å·®) å’Œ size_std_mb (å±‚å¤§å°æ ‡å‡†å·®) æ›¿ä»£ç¼ºå¤±çš„åˆ—
#     col_image = ['total_size_mb', 'avg_layer_entropy', 'entropy_std', 'layer_count', 'size_std_mb']
    
#     scaler_c = StandardScaler()
#     X_client = scaler_c.fit_transform(df[col_client].values)
#     scaler_i = StandardScaler()
#     X_image = scaler_i.fit_transform(df[col_image].values)
#     enc_algo = LabelEncoder()
#     X_algo = enc_algo.fit_transform(df['algo_name'].values)
#     y_target = np.log1p(df['total_time'].values)

#     # åˆ’åˆ†æ•°æ®é›†
#     Xc_train, Xc_test, Xi_train, Xi_test, Xa_train, Xa_test, y_train, y_test = train_test_split(
#         X_client, X_image, X_algo, y_target, test_size=0.2, random_state=42
#     )
    
#     train_loader = DataLoader(CTSDataset(Xc_train, Xi_train, Xa_train, y_train), batch_size=CONFIG["batch_size"], shuffle=True)
#     test_loader = DataLoader(CTSDataset(Xc_test, Xi_test, Xa_test, y_test), batch_size=CONFIG["batch_size"])
    
#     num_algos = len(enc_algo.classes_)
#     c_dim = len(col_client)
#     i_dim = len(col_image)

#     # --- å¼€å§‹æ¶ˆèå®éªŒ (è®­ç»ƒ4ä¸ªå˜ä½“) ---
#     results = {}
    
#     # 1. å®Œæ•´æ¨¡å‹ (Transformer + Dual Tower + NIG)
#     model_full = FullCFTNet(c_dim, i_dim, num_algos)
#     results['Full CFT-Net (Ours)'] = train_variant('Full CFT-Net', model_full, train_loader, test_loader, use_nig_loss=True)
    
#     # 2. å»æ‰ Transformer (MLP + Dual Tower + NIG)
#     model_no_attn = CFTNet_NoTransformer(c_dim, i_dim, num_algos)
#     results['w/o Attention (MLP)'] = train_variant('w/o Attention', model_no_attn, train_loader, test_loader, use_nig_loss=True)
    
#     # 3. å»æ‰åŒå¡” (Single Transformer + NIG)
#     model_single = SingleTowerTransformer(c_dim + i_dim, num_algos)
#     results['w/o Dual-Tower'] = train_variant('w/o Dual-Tower', model_single, train_loader, test_loader, use_nig_loss=True)
    
#     # 4. å»æ‰ä¸ç¡®å®šæ€§æŸå¤± (Full Model + MSE Loss)
#     # ç»“æ„ä¸€æ ·ï¼Œä½†è¾“å‡ºç»´åº¦æ”¹ä¸º 1ï¼ŒæŸå¤±ç”¨ MSE
#     model_mse = FullCFTNet(c_dim, i_dim, num_algos, output_dim=1) 
#     results['w/o Uncertainty (MSE)'] = train_variant('w/o Uncertainty', model_mse, train_loader, test_loader, use_nig_loss=False)

#     # --- ç”Ÿæˆæœ€ç»ˆæŠ¥è¡¨ (ç»ˆç«¯æ˜¾ç¤º) ---
#     print("\n" + "="*75)
#     print(f"{'Ablation Variant':<25} | {'RMSE':<10} | {'MAE':<10} | {'RÂ²':<10} | {'Drop (RMSE)'}")
#     print("-" * 75)
    
#     base_rmse = results['Full CFT-Net (Ours)']['rmse']
    
#     for name, metrics in results.items():
#         rmse = metrics['rmse']
#         mae = metrics['mae']
#         r2 = metrics['r2']
#         drop = (rmse - base_rmse) / base_rmse * 100 if name != 'Full CFT-Net (Ours)' else 0.0
        
#         print(f"{name:<25} | {rmse:<10.4f} | {mae:<10.4f} | {r2:<10.4f} | {f'+{drop:.1f}%' if drop > 0 else '-'}")
#     print("="*75)
    
#     # --- ä¿å­˜ç»“æœ ---
#     # 1. ä¿å­˜ JSON æ•°æ®
#     with open(CONFIG["json_filename"], 'w') as f:
#         # å°† numpy ç±»å‹è½¬æ¢ä¸º float ä»¥ä¾¿ json åºåˆ—åŒ–
#         serializable_results = {k: {m: float(v) for m, v in mets.items()} for k, mets in results.items()}
#         json.dump(serializable_results, f, indent=4)
#         print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {CONFIG['json_filename']}")

#     # 2. ã€æ–°å¢ã€‘ç”Ÿæˆå¯¹æ¯”å›¾
#     generate_component_contribution_plot(results, CONFIG["plot_filename"])
#     print(f"ğŸ“Š å›¾ç‰‡å·²ä¿å­˜è‡³: {CONFIG['plot_filename']}")

#     print("\n=== ä¸¥è°¨ç‰ˆæ¶ˆèå®éªŒå®Œæˆ ===")

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# import pandas as pd
# import numpy as np
# import os
# import json
# import matplotlib.pyplot as plt
# import matplotlib
# import platform
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.metrics import mean_squared_error, mean_absolute_error

# # ==============================================================================
# # 0. åŸºç¡€é…ç½®
# # ==============================================================================
# system_name = platform.system()
# if system_name == 'Windows':
#     font_list = ['Microsoft YaHei', 'SimHei']
# elif system_name == 'Darwin':
#     font_list = ['Heiti TC', 'PingFang HK']
# else:
#     font_list = ['WenQuanYi Micro Hei', 'Droid Sans Fallback']
# matplotlib.rcParams['font.sans-serif'] = font_list
# matplotlib.rcParams['axes.unicode_minus'] = False

# def set_seed(seed=42):
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     np.random.seed(seed)

# set_seed(42)
# # DATA_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx"
# # FEAT_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv"
# # MODEL_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_final_strong.pth" # ç¡®ä¿æ–‡ä»¶åå¯¹
# CONFIG = {
#     "data_path": "E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx",
#     "feature_path": "E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv",
#     "epochs": 150,  # æ¶ˆèå®éªŒè·‘50è½®çœ‹è¶‹åŠ¿
#     "batch_size": 128,
#     "lr": 0.001,
#     "reg_coeff": 0.2, 
#     "json_filename": "ablation_final.json",
#     "plot_filename": "figure_3_6_ablation_study.png"
# }
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # ==============================================================================
# # 1. æŸå¤±å‡½æ•°é›†
# # ==============================================================================
# def nig_nll_loss(y, gamma, v, alpha, beta):
#     two_blambda = 2 * beta * (1 + v)
#     nll = 0.5 * torch.log(np.pi / v) \
#         - alpha * torch.log(two_blambda) \
#         + (alpha + 0.5) * torch.log(v * (y - gamma)**2 + two_blambda) \
#         + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
#     return nll.mean()

# def robust_eub_reg_loss(y, gamma, v, alpha, beta):
#     error = torch.abs(y - gamma)
#     var = beta / (v * (alpha - 1) + 1e-6)
#     std = torch.sqrt(var + 1e-6)
#     ratio = torch.clamp(error / (std + 1e-6), max=10.0)
#     penalty = torch.where(ratio > 1.0, (ratio - 1.0)**2, 0.1 * (1.0 - ratio))
#     evidence = torch.clamp(2 * v + alpha, max=50.0)
#     return (penalty * torch.log1p(evidence)).mean()

# def vanilla_kl_reg_loss(y, gamma, v, alpha, beta):
#     error = torch.abs(y - gamma)
#     evidence = torch.clamp(2 * v + alpha, max=50.0)
#     return (error * evidence).mean()

# # ==============================================================================
# # 2. åŸºç¡€ç»„ä»¶
# # ==============================================================================
# class FeatureTokenizer(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
#         self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
#         self.norm = nn.LayerNorm(embed_dim)
#     def forward(self, x):
#         return self.norm(x.unsqueeze(-1) * self.weights + self.biases)

# class TransformerTower(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.tokenizer = FeatureTokenizer(num_features, embed_dim)
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
#         self.transformer = nn.TransformerEncoder(
#             nn.TransformerEncoderLayer(d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4, batch_first=True, dropout=0.1, activation="gelu"),
#             num_layers=2
#         )
#     def forward(self, x):
#         tokens = self.tokenizer(x)
#         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
#         out = self.transformer(torch.cat((cls_tokens, tokens), dim=1))
#         return out[:, 0, :]

# class MLPTower(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Linear(num_features, embed_dim * 2),
#             nn.BatchNorm1d(embed_dim * 2),
#             nn.ReLU(),
#             nn.Dropout(0.1),
#             nn.Linear(embed_dim * 2, embed_dim),
#             nn.ReLU()
#         )
#     def forward(self, x):
#         return self.net(x)

# # ==============================================================================
# # 3. ä¸¥è°¨çš„æ¨¡å‹å˜ä½“å®šä¹‰ (æ˜¾å¼å®šä¹‰æ¯ä¸ªç±»)
# # ==============================================================================

# # --- A. Ours: Gated Dual-Tower Transformer ---
# class OursModel(nn.Module):
#     def __init__(self, c_dim, i_dim, n_algos, embed_dim=32, output_dim=4):
#         super().__init__()
#         self.client_tower = TransformerTower(c_dim, embed_dim)
#         self.image_tower = TransformerTower(i_dim, embed_dim)
#         self.algo_embed = nn.Embedding(n_algos, embed_dim)
#         self.gate_net = nn.Sequential(nn.Linear(embed_dim * 2, embed_dim), nn.Sigmoid())
#         self.hidden = nn.Sequential(nn.Linear(embed_dim * 3, 64), nn.LayerNorm(64), nn.GELU(), nn.Dropout(0.2), nn.Linear(64, 32), nn.GELU())
#         self.head = nn.Linear(32, output_dim)

#     def forward(self, cx, ix, ax):
#         c = self.client_tower(cx)
#         i = self.image_tower(ix)
#         z = self.gate_net(torch.cat([c, i], dim=1))
#         fused = z * c + (1 - z) * i
#         a = self.algo_embed(ax)
#         out = self.head(self.hidden(torch.cat([fused, i, a], dim=1)))
        
#         if out.shape[1] == 4:
#             return torch.stack([out[:,0], F.softplus(out[:,1])+0.1, F.softplus(out[:,2])+1.1, F.softplus(out[:,3])+1e-6], dim=1)
#         return out

# # --- B. Variant: Concat Fusion (Explicitly NO GateNet) ---
# class ConcatModel(nn.Module):
#     def __init__(self, c_dim, i_dim, n_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = TransformerTower(c_dim, embed_dim)
#         self.image_tower = TransformerTower(i_dim, embed_dim)
#         self.algo_embed = nn.Embedding(n_algos, embed_dim)
        
#         # [åŒºåˆ«] æ˜¾å¼ç§»é™¤ GateNetï¼Œå‡å°‘å‚æ•°é‡
#         # self.gate_net = ... (Removed)
        
#         self.hidden = nn.Sequential(nn.Linear(embed_dim * 3, 64), nn.LayerNorm(64), nn.GELU(), nn.Dropout(0.2), nn.Linear(64, 32), nn.GELU())
#         self.head = nn.Linear(32, 4)

#     def forward(self, cx, ix, ax):
#         c = self.client_tower(cx)
#         i = self.image_tower(ix)
        
#         # [åŒºåˆ«] ç®€å•å¹³å‡èåˆ
#         fused = (c + i) / 2.0 
        
#         a = self.algo_embed(ax)
#         out = self.head(self.hidden(torch.cat([fused, i, a], dim=1)))
#         return torch.stack([out[:,0], F.softplus(out[:,1])+0.1, F.softplus(out[:,2])+1.1, F.softplus(out[:,3])+1e-6], dim=1)

# # --- C. Variant: MLP Backbone ---
# class MLPBackboneModel(nn.Module):
#     def __init__(self, c_dim, i_dim, n_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = MLPTower(c_dim, embed_dim) # [åŒºåˆ«] ç”¨ MLP
#         self.image_tower = MLPTower(i_dim, embed_dim)  # [åŒºåˆ«] ç”¨ MLP
#         self.algo_embed = nn.Embedding(n_algos, embed_dim)
#         self.gate_net = nn.Sequential(nn.Linear(embed_dim * 2, embed_dim), nn.Sigmoid())
#         self.hidden = nn.Sequential(nn.Linear(embed_dim * 3, 64), nn.LayerNorm(64), nn.GELU(), nn.Dropout(0.2), nn.Linear(64, 32), nn.GELU())
#         self.head = nn.Linear(32, 4)

#     def forward(self, cx, ix, ax):
#         c = self.client_tower(cx)
#         i = self.image_tower(ix)
#         z = self.gate_net(torch.cat([c, i], dim=1))
#         fused = z * c + (1 - z) * i
#         a = self.algo_embed(ax)
#         out = self.head(self.hidden(torch.cat([fused, i, a], dim=1)))
#         return torch.stack([out[:,0], F.softplus(out[:,1])+0.1, F.softplus(out[:,2])+1.1, F.softplus(out[:,3])+1e-6], dim=1)

# # --- D. Variant: Single Tower ---
# class SingleTowerModel(nn.Module):
#     def __init__(self, c_dim, i_dim, n_algos, embed_dim=32):
#         super().__init__()
#         # [åŒºåˆ«] å•å¡”å¤„ç†æ‰€æœ‰ç‰¹å¾
#         self.tower = TransformerTower(c_dim + i_dim, embed_dim)
#         self.algo_embed = nn.Embedding(n_algos, embed_dim)
#         self.hidden = nn.Sequential(nn.Linear(embed_dim * 2, 64), nn.LayerNorm(64), nn.GELU(), nn.Dropout(0.2), nn.Linear(64, 32), nn.GELU())
#         self.head = nn.Linear(32, 4)

#     def forward(self, cx, ix, ax):
#         combined = torch.cat([cx, ix], dim=1)
#         feat = self.tower(combined)
#         a = self.algo_embed(ax)
#         # [åŒºåˆ«] åªæœ‰ feat å’Œ aï¼Œæ²¡æœ‰ gateï¼Œæ²¡æœ‰ fusion
#         out = self.head(self.hidden(torch.cat([feat, a], dim=1)))
#         return torch.stack([out[:,0], F.softplus(out[:,1])+0.1, F.softplus(out[:,2])+1.1, F.softplus(out[:,3])+1e-6], dim=1)

# # ==============================================================================
# # 4. è®­ç»ƒé€»è¾‘
# # ==============================================================================
# class CTSDataset(Dataset):
#     def __init__(self, cx, ix, ax, y):
#         self.cx, self.ix, self.ax, self.y = torch.FloatTensor(cx), torch.FloatTensor(ix), torch.LongTensor(ax), torch.FloatTensor(y)
#     def __len__(self): return len(self.y)
#     def __getitem__(self, i): return self.cx[i], self.ix[i], self.ax[i], self.y[i]

# def train_ablation(name, model, train_loader, test_loader, loss_type='robust_eub'):
#     print(f"\nâš¡ [å®éªŒ] {name} | Loss: {loss_type}")
#     model = model.to(device)
#     optimizer = optim.AdamW(model.parameters(), lr=CONFIG["lr"], weight_decay=1e-4)
    
#     for epoch in range(CONFIG["epochs"]):
#         model.train()
#         for cx, ix, ax, y in train_loader:
#             cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
#             optimizer.zero_grad()
#             preds = model(cx, ix, ax)
            
#             if loss_type == 'mse':
#                 loss = F.mse_loss(preds.squeeze(), y)
#             else:
#                 gamma, v, alpha, beta = preds[:,0], preds[:,1], preds[:,2], preds[:,3]
#                 loss_nll = nig_nll_loss(y, gamma, v, alpha, beta)
                
#                 if loss_type == 'robust_eub':
#                     loss_reg = robust_eub_reg_loss(y, gamma, v, alpha, beta)
#                 elif loss_type == 'kl':
#                     loss_reg = vanilla_kl_reg_loss(y, gamma, v, alpha, beta)
                
#                 reg_w = 0 if epoch < 3 else CONFIG["reg_coeff"]
#                 loss = loss_nll + reg_w * loss_reg
            
#             loss.backward()
#             optimizer.step()

#     # è¯„ä¼°
#     model.eval()
#     preds_list, true_list = [], []
#     with torch.no_grad():
#         for cx, ix, ax, y in test_loader:
#             cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
#             preds = model(cx, ix, ax)
#             gamma = preds.squeeze() if loss_type == 'mse' else preds[:, 0]
#             preds_list.extend(np.expm1(gamma.cpu().numpy()))
#             true_list.extend(np.expm1(y.cpu().numpy()))
    
#     rmse = np.sqrt(mean_squared_error(true_list, preds_list))
#     print(f"âœ… {name} -> RMSE: {rmse:.4f}")
#     return rmse

# # ==============================================================================
# # 5. ä¸»ç¨‹åº
# # ==============================================================================
# if __name__ == "__main__":
#     print("=== å…¨æ–¹ä½æ¶ˆèå®éªŒ (Strict Mode) ===")
    
#     # 1. åŠ è½½æ•°æ®
#     df = pd.read_excel(CONFIG["data_path"])
#     df_feat = pd.read_csv(CONFIG["feature_path"])
#     rename_map = {"image": "image_name", "method": "algo_name", "network_bw": "bandwidth_mbps", "network_delay": "network_rtt", "mem_limit": "mem_limit_mb"}
#     df = df.rename(columns=rename_map)
#     if 'total_time' not in df.columns: df['total_time'] = df[[c for c in df.columns if 'total_tim' in c][0]]
#     df = df[(df['status'] == 'SUCCESS') & (df['total_time'] > 0)]
#     df = pd.merge(df, df_feat, on="image_name", how="inner")
    
#     cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#     target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std', 'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
#     cols_i = [c for c in target_cols if c in df.columns]
    
#     Xc = StandardScaler().fit_transform(df[cols_c].values)
#     Xi = StandardScaler().fit_transform(df[cols_i].values)
#     enc = LabelEncoder()
#     Xa = enc.fit_transform(df['algo_name'].values)
#     y = np.log1p(df['total_time'].values)
    
#     from sklearn.model_selection import train_test_split
#     idx_tr, idx_te = train_test_split(np.arange(len(y)), test_size=0.2, random_state=42)
#     tr_loader = DataLoader(CTSDataset(Xc[idx_tr], Xi[idx_tr], Xa[idx_tr], y[idx_tr]), batch_size=CONFIG["batch_size"], shuffle=True)
#     te_loader = DataLoader(CTSDataset(Xc[idx_te], Xi[idx_te], Xa[idx_te], y[idx_te]), batch_size=CONFIG["batch_size"])
#     c_dim, i_dim, n_algos = Xc.shape[1], Xi.shape[1], len(enc.classes_)

#     results = {}
    
#     # --- å®éªŒå¼€å§‹ ---
    
#     # 1. æ¶æ„æ¶ˆè (Backbone Ablation)
#     results['w/o Transformer (MLP)'] = train_ablation('MLP', MLPBackboneModel(c_dim, i_dim, n_algos), tr_loader, te_loader)
#     results['w/o Dual-Tower'] = train_ablation('SingleTower', SingleTowerModel(c_dim, i_dim, n_algos), tr_loader, te_loader)
    
#     # 2. èåˆæ¶ˆè (Fusion Ablation)
#     # ä½¿ç”¨ ConcatModelï¼Œæ˜¾å¼ç§»é™¤ GateNet
#     results['w/o Gated Fusion'] = train_ablation('Concat', ConcatModel(c_dim, i_dim, n_algos), tr_loader, te_loader)
    
#     # 3. æŸå¤±æ¶ˆè (Loss Ablation)
#     # ä½¿ç”¨ OursModelï¼Œä½† Loss ç”¨ KL
#     results['w/o Robust EUB'] = train_ablation('Vanilla KL', OursModel(c_dim, i_dim, n_algos), tr_loader, te_loader, loss_type='kl')
    
#     # 4. ä»»åŠ¡æ¶ˆè (Task Ablation)
#     # ä½¿ç”¨ OursModel (Output=1)ï¼ŒLoss ç”¨ MSE
#     results['w/o Uncertainty'] = train_ablation('MSE Only', OursModel(c_dim, i_dim, n_algos, output_dim=1), tr_loader, te_loader, loss_type='mse')
    
#     # 5. Ours
#     results['Ours (Full)'] = train_ablation('Ours', OursModel(c_dim, i_dim, n_algos), tr_loader, te_loader, loss_type='robust_eub')

#     # --- ç»˜å›¾ ---
#     print("\nğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾...")
#     names = list(results.keys())
#     values = list(results.values())
    
#     # æ’åºï¼šOurs åœ¨æœ€å
#     sorted_indices = np.argsort(values)[::-1]
#     names = [names[i] for i in sorted_indices]
#     values = [values[i] for i in sorted_indices]
    
#     colors = ['gray'] * (len(names)-1) + ['#2ca02c']
    
#     plt.figure(figsize=(12, 6))
#     bars = plt.bar(names, values, color=colors)
#     plt.title('å„ç»„ä»¶å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ (Ablation Study)')
#     plt.ylabel('RMSE (è¶Šä½è¶Šå¥½)')
#     plt.xticks(rotation=20, ha='right')
#     plt.ylim(min(values)*0.9, max(values)*1.05)
    
#     for bar in bars:
#         height = bar.get_height()
#         plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.2f}', ha='center', va='bottom')
        
#     plt.tight_layout()
#     plt.savefig(CONFIG["plot_filename"], dpi=300)
#     print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {CONFIG['plot_filename']}")
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib
import platform
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.stats import spearmanr, wilcoxon
import warnings
warnings.filterwarnings('ignore')

# ==============================================================================
# 0. ç»˜å›¾é…ç½®ï¼ˆã€å¼ºåŒ–ã€‘è‡ªåŠ¨é€‚é…ä¸­è‹±æ–‡ï¼Œä¿è¯ä¸­æ–‡æ˜¾ç¤ºï¼‰
# ==============================================================================
system_name = platform.system()
if system_name == 'Windows':
    font_list = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
elif system_name == 'Darwin':
    font_list = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS']
else:
    font_list = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']

# è®¾ç½®å…¨å±€å­—ä½“ï¼Œè´Ÿå·æ­£å¸¸æ˜¾ç¤º
matplotlib.rcParams['font.sans-serif'] = font_list
matplotlib.rcParams['axes.unicode_minus'] = False
plt.style.use('seaborn-v0_8-whitegrid')

# æ£€æŸ¥å­—ä½“æ˜¯å¦å¯ç”¨ï¼Œè‹¥é…ç½®çš„å­—ä½“å‡ä¸å­˜åœ¨åˆ™å›é€€åˆ° DejaVu Sansï¼ˆè‹±æ–‡ï¼‰
import matplotlib.font_manager as fm
available_fonts = [f.name for f in fm.fontManager.ttflist]
for font in font_list:
    if font in available_fonts:
        matplotlib.rcParams['font.sans-serif'] = [font]
        break
else:
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans']
    print("âš ï¸ æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ˜¾ç¤º")

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)

# ==============================================================================
# 1. è¶…å‚æ•°é…ç½®ï¼ˆã€æœ€å¼ºç‰ˆã€‘å›å½’å·²éªŒè¯å‚æ•°ï¼‰
# ==============================================================================
CONFIG = {
    "data_path": "E:\\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\\CTS_system\\ml_training\\modeling\\cts_data.xlsx",
    "feature_path": "E:\\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\\CTS_system\\ml_training\\image_features_database.csv",
    "epochs": 60,
    "patience": 15,
    "batch_size": 128,
    "lr": 0.0005,
    "reg_coeff": 1.0,
    "embed_dim": 32,
    "n_runs": 5,
    "random_seeds": [42, 123, 456, 789, 2024],
    "plot_ablation": "figure_3_6_ablation.png",
    "plot_calibration": "figure_3_7_calibration_ablation.png"
}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==============================================================================
# 2. æŸå¤±å‡½æ•°ï¼ˆä¸å˜ï¼‰
# ==============================================================================
def nig_nll_loss(y, gamma, v, alpha, beta):
    two_blambda = 2 * beta * (1 + v)
    nll = 0.5 * torch.log(np.pi / v) \
          - alpha * torch.log(two_blambda) \
          + (alpha + 0.5) * torch.log(v * (y - gamma)**2 + two_blambda) \
          + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
    return nll.mean()

def strong_eub_reg_loss(y, gamma, v, alpha, beta):
    error = torch.abs(y - gamma)
    var = beta / (v * (alpha - 1) + 1e-6)
    std = torch.sqrt(var + 1e-6)
    ratio = torch.clamp(error / (std + 1e-6), max=10.0)
    penalty = (ratio - 1.0) ** 2
    evidence = torch.clamp(2 * v + alpha, max=50.0)
    return (penalty * torch.log1p(evidence)).mean()

def vanilla_kl_reg_loss(y, gamma, v, alpha, beta):
    error = torch.abs(y - gamma)
    evidence = torch.clamp(2 * v + alpha, max=50.0)
    return (error * evidence).mean()

# ==============================================================================
# 3. æ¨¡å‹ç»„ä»¶ä¸å˜ä½“ï¼ˆMSEæ¨¡å‹ä¿ç•™å®šä¹‰ï¼Œä½†ä¸å‚ä¸æ¶ˆèä¸»å®éªŒï¼‰
# ==============================================================================
class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
        self.norm = nn.LayerNorm(embed_dim)
    def forward(self, x):
        return self.norm(x.unsqueeze(-1) * self.weights + self.biases)

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=4, dim_feedforward=embed_dim*4,
            batch_first=True, dropout=0.1, activation='gelu'
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
    def forward(self, x):
        tokens = self.tokenizer(x)
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        out = self.transformer(torch.cat((cls_tokens, tokens), dim=1))
        return out[:, 0, :]

class MLPTower(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(num_features, embed_dim * 2),
            nn.BatchNorm1d(embed_dim * 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.ReLU()
        )
    def forward(self, x):
        return self.net(x)

# ----- æ¨¡å‹å˜ä½“ -----
class OursModel(nn.Module):
    """å®Œæ•´æ¨¡å‹ï¼šåŒå¡”Transformer + å¹³å‡èåˆ + è¯æ®å›å½’å¤´"""
    def __init__(self, c_dim, i_dim, n_algos, embed_dim=32, output_dim=4):
        super().__init__()
        self.client_tower = TransformerTower(c_dim, embed_dim)
        self.image_tower = TransformerTower(i_dim, embed_dim)
        self.algo_embed = nn.Embedding(n_algos, embed_dim)
        self.hidden = nn.Sequential(
            nn.Linear(embed_dim * 3, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU()
        )
        self.head = nn.Linear(32, output_dim)
    def forward(self, cx, ix, ax):
        c = self.client_tower(cx)
        i = self.image_tower(ix)
        fused = (c + i) / 2.0          # ã€æ­£ç¡®ã€‘å¹³å‡èåˆï¼Œæ— é—¨æ§
        a = self.algo_embed(ax)
        out = self.head(self.hidden(torch.cat([fused, i, a], dim=1)))
        if out.shape[1] == 4:
            return torch.stack([
                out[:, 0],
                F.softplus(out[:, 1]) + 0.1,
                F.softplus(out[:, 2]) + 1.1,
                F.softplus(out[:, 3]) + 1e-6
            ], dim=1)
        return out

class MLPBackboneModel(nn.Module):
    """æ¶ˆèAï¼šTransformer â†’ MLP"""
    def __init__(self, c_dim, i_dim, n_algos, embed_dim=32):
        super().__init__()
        self.client_tower = MLPTower(c_dim, embed_dim)
        self.image_tower = MLPTower(i_dim, embed_dim)
        self.algo_embed = nn.Embedding(n_algos, embed_dim)
        self.hidden = nn.Sequential(
            nn.Linear(embed_dim * 3, 64), nn.LayerNorm(64), nn.GELU(),
            nn.Dropout(0.2), nn.Linear(64, 32), nn.GELU()
        )
        self.head = nn.Linear(32, 4)
    def forward(self, cx, ix, ax):
        c = self.client_tower(cx)
        i = self.image_tower(ix)
        fused = (c + i) / 2.0
        a = self.algo_embed(ax)
        out = self.head(self.hidden(torch.cat([fused, i, a], dim=1)))
        return torch.stack([
            out[:, 0], F.softplus(out[:, 1])+0.1,
            F.softplus(out[:, 2])+1.1, F.softplus(out[:, 3])+1e-6
        ], dim=1)

class SingleTowerModel(nn.Module):
    """æ¶ˆèBï¼šåŒå¡” â†’ å•å¡”ï¼ˆè¾“å…¥æ‹¼æ¥åè¿‡Transformerï¼‰"""
    def __init__(self, c_dim, i_dim, n_algos, embed_dim=32):
        super().__init__()
        self.tower = TransformerTower(c_dim + i_dim, embed_dim)
        self.algo_embed = nn.Embedding(n_algos, embed_dim)
        self.hidden = nn.Sequential(
            nn.Linear(embed_dim * 2, 64), nn.LayerNorm(64), nn.GELU(),
            nn.Dropout(0.2), nn.Linear(64, 32), nn.GELU()
        )
        self.head = nn.Linear(32, 4)
    def forward(self, cx, ix, ax):
        combined = torch.cat([cx, ix], dim=1)
        feat = self.tower(combined)
        a = self.algo_embed(ax)
        out = self.head(self.hidden(torch.cat([feat, a], dim=1)))
        return torch.stack([
            out[:, 0], F.softplus(out[:, 1])+0.1,
            F.softplus(out[:, 2])+1.1, F.softplus(out[:, 3])+1e-6
        ], dim=1)

# ----- MSEæ¨¡å‹ï¼ˆä¿ç•™å®šä¹‰ï¼Œä½†é»˜è®¤ä¸å‚ä¸æ¶ˆèå®éªŒï¼‰-----
class MSEModel(OursModel):
    """æ¶ˆèCï¼ˆè¡¥å……å¯¹ç…§ï¼‰ï¼šè¯æ®å›å½’ â†’ æ™®é€šMSEå›å½’ï¼ˆè¾“å‡ºç»´åº¦1ï¼‰"""
    def __init__(self, c_dim, i_dim, n_algos, embed_dim=32):
        super().__init__(c_dim, i_dim, n_algos, embed_dim, output_dim=1)

# ==============================================================================
# 4. æ•°æ®é›†
# ==============================================================================
class CTSDataset(Dataset):
    def __init__(self, cx, ix, ax, y):
        self.cx = torch.FloatTensor(cx)
        self.ix = torch.FloatTensor(ix)
        self.ax = torch.LongTensor(ax)
        self.y = torch.FloatTensor(y)
    def __len__(self):
        return len(self.y)
    def __getitem__(self, i):
        return self.cx[i], self.ix[i], self.ax[i], self.y[i]

# ==============================================================================
# 5. æ ¸å¿ƒè®­ç»ƒä¸è¯„ä¼°å‡½æ•°ï¼ˆå•æ¬¡è¿è¡Œï¼Œå¸¦æ—©åœï¼‰
# ==============================================================================
def compute_ece(uncertainty, abs_error, n_bins=15):
    """æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆå½’ä¸€åŒ–åï¼‰"""
    unc_norm = (uncertainty - uncertainty.min()) / (uncertainty.max() - uncertainty.min() + 1e-8)
    err_norm = (abs_error - abs_error.min()) / (abs_error.max() - abs_error.min() + 1e-8)
    bins = np.linspace(0, 1, n_bins+1)
    bin_indices = np.digitize(unc_norm, bins) - 1
    bin_indices = np.clip(bin_indices, 0, n_bins-1)
    ece = 0.0
    for i in range(n_bins):
        mask = bin_indices == i
        if np.sum(mask) > 0:
            avg_unc = unc_norm[mask].mean()
            avg_err = err_norm[mask].mean()
            ece += np.abs(avg_unc - avg_err) * (np.sum(mask) / len(uncertainty))
    return ece

def train_ablation_single(name, model_class, loss_type, c_dim, i_dim, n_algos,
                          Xc_train, Xi_train, Xa_train, y_train,
                          Xc_val, Xi_val, Xa_val, y_val, seed):
    """å•æ¬¡å®éªŒï¼šè®­ç»ƒå¹¶è¿”å›æœ€ä½³æŒ‡æ ‡åŠæ¨¡å‹ï¼ˆå¸¦æ—©åœï¼‰"""
    set_seed(seed)
    model = model_class(c_dim, i_dim, n_algos).to(device)
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG["lr"], weight_decay=1e-4)

    train_loader = DataLoader(CTSDataset(Xc_train, Xi_train, Xa_train, y_train),
                              batch_size=CONFIG["batch_size"], shuffle=True)
    val_loader = DataLoader(CTSDataset(Xc_val, Xi_val, Xa_val, y_val),
                            batch_size=CONFIG["batch_size"], shuffle=False)

    best_rmse = float('inf')
    best_metrics = {}
    best_model_state = None
    patience_counter = 0

    for epoch in range(CONFIG["epochs"]):
        model.train()
        for cx, ix, ax, y in train_loader:
            cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
            optimizer.zero_grad()
            preds = model(cx, ix, ax)

            if loss_type == 'mse':
                loss = F.mse_loss(preds.squeeze(), y)
            else:
                gamma, v, alpha, beta = preds[:,0], preds[:,1], preds[:,2], preds[:,3]
                loss_nll = nig_nll_loss(y, gamma, v, alpha, beta)
                if loss_type == 'strong_eub':
                    loss_reg = strong_eub_reg_loss(y, gamma, v, alpha, beta)
                elif loss_type == 'kl':
                    loss_reg = vanilla_kl_reg_loss(y, gamma, v, alpha, beta)
                else:
                    loss_reg = 0.0
                reg_w = 0.0 if epoch < 3 else CONFIG["reg_coeff"]
                loss = loss_nll + reg_w * loss_reg

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        # --- éªŒè¯ ---
        model.eval()
        pred_list, true_list, unc_list = [], [], []
        with torch.no_grad():
            for cx, ix, ax, y in val_loader:
                cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
                preds = model(cx, ix, ax)
                if loss_type == 'mse':
                    gamma = preds.squeeze()
                else:
                    gamma, v, alpha, beta = preds[:,0], preds[:,1], preds[:,2], preds[:,3]
                    unc = beta / (v * (alpha - 1))
                    unc_list.extend(unc.cpu().numpy())

                pred_list.extend(np.expm1(gamma.cpu().numpy()))
                true_list.extend(np.expm1(y.cpu().numpy()))

        pred_list = np.array(pred_list)
        true_list = np.array(true_list)
        curr_rmse = np.sqrt(mean_squared_error(true_list, pred_list))

        # --- é˜²åå¡Œæœºåˆ¶ï¼ˆä»…é’ˆå¯¹KLï¼‰---
        valid = True
        if loss_type == 'kl':
            pred_std = np.std(pred_list)
            if pred_std < 0.5:
                valid = False
            if len(unc_list) > 0:
                abs_err = np.abs(true_list - pred_list)
                corr, _ = spearmanr(unc_list, abs_err)
                if np.isnan(corr) or corr < 0.05:
                    valid = False

        # --- æ—©åœä¸æœ€ä½³æ¨¡å‹ä¿å­˜ ---
        if curr_rmse < best_rmse and valid:
            best_rmse = curr_rmse
            best_model_state = model.state_dict()
            best_metrics['rmse'] = curr_rmse
            best_metrics['mae'] = mean_absolute_error(true_list, pred_list)
            best_metrics['r2'] = r2_score(true_list, pred_list)
            if loss_type != 'mse' and len(unc_list) > 0:
                abs_err = np.abs(true_list - pred_list)
                spearman_corr, _ = spearmanr(unc_list, abs_err)
                best_metrics['spearman'] = spearman_corr if not np.isnan(spearman_corr) else 0.0
                best_metrics['ece'] = compute_ece(np.array(unc_list), abs_err)
            else:
                best_metrics['spearman'] = 0.0
                best_metrics['ece'] = 1.0
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= CONFIG["patience"]:
                break

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if best_model_state is not None:
        os.makedirs('ablation_models', exist_ok=True)
        torch.save(best_model_state, f'ablation_models/best_{name}_seed{seed}.pth')

    return best_metrics

# ==============================================================================
# 6. å¤šæ¬¡ç‹¬ç«‹å®éªŒä¸»æµç¨‹ï¼ˆå·²ç§»é™¤MSEï¼‰
# ==============================================================================
def run_ablation_experiments():
    print("="*60)
    print("ğŸ”¬ æ¶ˆèå®éªŒï¼ˆå¤šæ¬¡ç‹¬ç«‹è¿è¡Œï¼Œæ— MSEåŸºçº¿ï¼‰")
    print("="*60)

    df = pd.read_excel(CONFIG["data_path"])
    df_feat = pd.read_csv(CONFIG["feature_path"])
    rename_map = {"image": "image_name", "method": "algo_name",
                  "network_bw": "bandwidth_mbps", "network_delay": "network_rtt",
                  "mem_limit": "mem_limit_mb"}
    df = df.rename(columns=rename_map)
    if 'total_time' not in df.columns:
        df['total_time'] = df[[c for c in df.columns if 'total_tim' in c][0]]
    df = df[(df['status'] == 'SUCCESS') & (df['total_time'] > 0)]
    if 'mem_limit_mb' not in df.columns:
        df['mem_limit_mb'] = 1024.0
    df = pd.merge(df, df_feat, on="image_name", how="inner")

    cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
    cols_i = ['total_size_mb', 'avg_layer_entropy', 'entropy_std',
              'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
    cols_i = [c for c in cols_i if c in df.columns]

    # ä»…åŒ…å«å››ä¸ªæ¶ˆèå˜ä½“
    all_results = {
        'Ours (Full)': [],
        'w/o Transformer (MLP)': [],
        'w/o Dual-Tower (Single)': [],
        'w/o Robust EUB (KL)': []
    }

    for run_idx, seed in enumerate(CONFIG["random_seeds"][:CONFIG["n_runs"]]):
        print(f"\n--- å®éªŒ {run_idx+1}/{CONFIG['n_runs']} (seed={seed}) ---")
        set_seed(seed)

        idx = np.arange(len(df))
        idx_train, idx_val = train_test_split(idx, test_size=0.2, random_state=seed)

        scaler_c = StandardScaler()
        scaler_i = StandardScaler()
        enc = LabelEncoder()

        Xc_train_raw = df.iloc[idx_train][cols_c].values
        Xi_train_raw = df.iloc[idx_train][cols_i].values
        Xa_train_raw = df.iloc[idx_train]['algo_name'].values
        y_train_raw = np.log1p(df.iloc[idx_train]['total_time'].values)

        scaler_c.fit(Xc_train_raw)
        scaler_i.fit(Xi_train_raw)
        enc.fit(df['algo_name'].values)

        Xc_train = scaler_c.transform(Xc_train_raw)
        Xi_train = scaler_i.transform(Xi_train_raw)
        Xa_train = enc.transform(Xa_train_raw)

        Xc_val_raw = df.iloc[idx_val][cols_c].values
        Xi_val_raw = df.iloc[idx_val][cols_i].values
        Xa_val_raw = df.iloc[idx_val]['algo_name'].values
        y_val_raw = np.log1p(df.iloc[idx_val]['total_time'].values)

        Xc_val = scaler_c.transform(Xc_val_raw)
        Xi_val = scaler_i.transform(Xi_val_raw)
        Xa_val = enc.transform(Xa_val_raw)

        c_dim = Xc_train.shape[1]
        i_dim = Xi_train.shape[1]
        n_algos = len(enc.classes_)

        # 1. Ours (Full)
        res_ours = train_ablation_single(
            'Ours', OursModel, 'strong_eub',
            c_dim, i_dim, n_algos,
            Xc_train, Xi_train, Xa_train, y_train_raw,
            Xc_val, Xi_val, Xa_val, y_val_raw, seed
        )
        all_results['Ours (Full)'].append(res_ours)

        # 2. MLP Backbone
        res_mlp = train_ablation_single(
            'MLP', MLPBackboneModel, 'strong_eub',
            c_dim, i_dim, n_algos,
            Xc_train, Xi_train, Xa_train, y_train_raw,
            Xc_val, Xi_val, Xa_val, y_val_raw, seed
        )
        all_results['w/o Transformer (MLP)'].append(res_mlp)

        # 3. Single Tower
        res_single = train_ablation_single(
            'Single', SingleTowerModel, 'strong_eub',
            c_dim, i_dim, n_algos,
            Xc_train, Xi_train, Xa_train, y_train_raw,
            Xc_val, Xi_val, Xa_val, y_val_raw, seed
        )
        all_results['w/o Dual-Tower (Single)'].append(res_single)

        # 4. Vanilla KL
        res_kl = train_ablation_single(
            'KL', OursModel, 'kl',
            c_dim, i_dim, n_algos,
            Xc_train, Xi_train, Xa_train, y_train_raw,
            Xc_val, Xi_val, Xa_val, y_val_raw, seed
        )
        all_results['w/o Robust EUB (KL)'].append(res_kl)

        # ----- MSEåŸºçº¿ï¼ˆæ³¨é‡Šæ‰ï¼Œä¸å‚ä¸æ¶ˆèå®éªŒï¼‰-----
        # res_mse = train_ablation_single(
        #     'MSE', MSEModel, 'mse',
        #     c_dim, i_dim, n_algos,
        #     Xc_train, Xi_train, Xa_train, y_train_raw,
        #     Xc_val, Xi_val, Xa_val, y_val_raw, seed
        # )
        # all_results['MSE Baseline'].append(res_mse)

    # ----- æ±‡æ€»ç»Ÿè®¡ä¸æ˜¾è‘—æ€§æ£€éªŒ -----
    summary = {}
    for name, runs in all_results.items():
        df_runs = pd.DataFrame(runs)
        summary[name] = {
            'rmse_mean': df_runs['rmse'].mean(),
            'rmse_std': df_runs['rmse'].std(),
            'mae_mean': df_runs['mae'].mean(),
            'mae_std': df_runs['mae'].std(),
            'r2_mean': df_runs['r2'].mean(),
            'r2_std': df_runs['r2'].std(),
            'spearman_mean': df_runs['spearman'].mean() if 'spearman' in df_runs else 0,
            'spearman_std': df_runs['spearman'].std() if 'spearman' in df_runs else 0,
            'ece_mean': df_runs['ece'].mean() if 'ece' in df_runs else 1,
            'ece_std': df_runs['ece'].std() if 'ece' in df_runs else 0
        }

    # æ˜¾è‘—æ€§æ£€éªŒï¼šOurs vs æ¯ä¸ªæ¶ˆèå˜ä½“
    ours_rmse = [r['rmse'] for r in all_results['Ours (Full)']]
    for name in all_results.keys():
        if name == 'Ours (Full)': continue
        other_rmse = [r['rmse'] for r in all_results[name]]
        if len(ours_rmse) == len(other_rmse) and len(ours_rmse) > 1:
            try:
                stat, p = wilcoxon(ours_rmse, other_rmse, alternative='less')
                summary[name]['p_vs_ours'] = p
            except:
                summary[name]['p_vs_ours'] = 1.0
        else:
            summary[name]['p_vs_ours'] = 1.0

    return summary, all_results

# ==============================================================================
# 7. å¯è§†åŒ–ï¼ˆã€å¼ºåŒ–ã€‘å…¨ä¸­æ–‡æ˜¾ç¤ºï¼Œå­—ä½“å›é€€ï¼‰
# ==============================================================================
def plot_ablation_results(summary):
    """ç»˜åˆ¶æ¶ˆèå®éªŒå¯¹æ¯”å›¾ï¼ˆå…¨ä¸­æ–‡ï¼ŒOursç»¿è‰²å‹è½´ï¼‰"""
    # å¼ºåˆ¶ä¸­æ–‡å­—ä½“ï¼ˆäºŒæ¬¡ä¿é™©ï¼‰
    plt.rcParams['font.sans-serif'] = font_list
    plt.rcParams['axes.unicode_minus'] = False

    names = list(summary.keys())
    
    # å¼ºåˆ¶æ’åºï¼šéOursæŒ‰RMSEä»å¤§åˆ°å°ï¼ŒOurså›ºå®šæ”¾åœ¨æœ€å
    non_ours = [n for n in names if "Ours" not in n]
    rmse_means_non_ours = [summary[n]['rmse_mean'] for n in non_ours]
    sorted_idx = np.argsort(rmse_means_non_ours)[::-1]
    sorted_names = [non_ours[i] for i in sorted_idx]
    if 'Ours (Full)' in names:
        sorted_names.append('Ours (Full)')
    names = sorted_names

    # é…è‰²ï¼šOursç»¿è‰²ï¼Œå…¶ä»–ç°è‰²
    colors = ['#808080'] * (len(names)-1) + ['#2ca02c']

    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    metrics = ['rmse', 'mae', 'r2']
    titles = ['RMSE (ç§’) â†“', 'MAE (ç§’) â†“', 'RÂ² åˆ†æ•° â†‘']
    ylabels = ['RMSE', 'MAE', 'RÂ²']

    for i, metric in enumerate(metrics):
        means = [summary[n][f'{metric}_mean'] for n in names]
        stds = [summary[n][f'{metric}_std'] for n in names]

        bars = axes[i].bar(names, means, yerr=stds, capsize=5, color=colors,
                           error_kw={'elinewidth': 1.5, 'ecolor': 'black', 'alpha':0.7})
        axes[i].set_title(titles[i], fontsize=14, fontweight='bold')
        axes[i].set_ylabel(ylabels[i], fontsize=12)
        axes[i].tick_params(axis='x', rotation=20, labelsize=10)

        # æ•°å€¼æ ‡ç­¾ï¼ˆOursåŠ ç²—ï¼‰
        for bar, mean, std in zip(bars, means, stds):
            height = bar.get_height()
            offset = 0.02 if metric == 'r2' else height * 0.05
            fw = 'bold' if bar.get_facecolor() == (0.17254901960784313, 0.6274509803921569, 0.17254901960784313, 1.0) else 'normal'
            if metric == 'r2':
                text = f'{mean:.3f}Â±{std:.3f}'
            else:
                text = f'{mean:.2f}Â±{std:.2f}'
            axes[i].text(bar.get_x() + bar.get_width()/2., height + offset,
                         text, ha='center', va='bottom', fontsize=9, fontweight=fw)

    plt.tight_layout()
    plt.savefig(CONFIG["plot_ablation"], dpi=300)
    plt.close()
    print(f"âœ… æ¶ˆèå®éªŒæŸ±çŠ¶å›¾å·²ä¿å­˜: {CONFIG['plot_ablation']}")

def plot_calibration_ablation(summary):
    """ç»˜åˆ¶æ ¡å‡†æŒ‡æ ‡å¯¹æ¯”ï¼ˆå…¨ä¸­æ–‡ï¼ŒOursç»¿è‰²å‹è½´ï¼‰"""
    plt.rcParams['font.sans-serif'] = font_list
    plt.rcParams['axes.unicode_minus'] = False

    names = list(summary.keys())
    # Oursæ”¾æœ€å
    non_ours = [n for n in names if "Ours" not in n]
    rmse_means_non_ours = [summary[n]['rmse_mean'] for n in non_ours]
    sorted_idx = np.argsort(rmse_means_non_ours)[::-1]
    sorted_names = [non_ours[i] for i in sorted_idx]
    if 'Ours (Full)' in names:
        sorted_names.append('Ours (Full)')
    names = sorted_names

    colors = ['#808080'] * (len(names)-1) + ['#2ca02c']

    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Spearmanç›¸å…³æ€§
    sp_means = [summary[n]['spearman_mean'] for n in names]
    sp_stds = [summary[n]['spearman_std'] for n in names]
    bars1 = axes[0].bar(names, sp_means, yerr=sp_stds, capsize=5, color=colors,
                        error_kw={'elinewidth': 1.5, 'ecolor': 'black'})
    axes[0].set_title('ä¸ç¡®å®šæ€§æ ¡å‡† - Spearmanç›¸å…³æ€§ â†‘', fontsize=14, fontweight='bold')
    axes[0].set_ylabel('Spearman Ï', fontsize=12)
    axes[0].tick_params(axis='x', rotation=20, labelsize=10)
    for bar, m, s in zip(bars1, sp_means, sp_stds):
        fw = 'bold' if bar.get_facecolor() == (0.17254901960784313, 0.6274509803921569, 0.17254901960784313, 1.0) else 'normal'
        axes[0].text(bar.get_x()+bar.get_width()/2., m+s+0.02, f'{m:.3f}Â±{s:.3f}',
                     ha='center', va='bottom', fontsize=9, fontweight=fw)

    # æœŸæœ›æ ¡å‡†è¯¯å·®ï¼ˆECEï¼‰
    ece_means = [summary[n]['ece_mean'] for n in names]
    ece_stds = [summary[n]['ece_std'] for n in names]
    bars2 = axes[1].bar(names, ece_means, yerr=ece_stds, capsize=5, color=colors,
                        error_kw={'elinewidth': 1.5, 'ecolor': 'black'})
    axes[1].set_title('ä¸ç¡®å®šæ€§æ ¡å‡† - æœŸæœ›æ ¡å‡†è¯¯å·®(ECE) â†“', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('ECE', fontsize=12)
    axes[1].tick_params(axis='x', rotation=20, labelsize=10)
    for bar, m, s in zip(bars2, ece_means, ece_stds):
        fw = 'bold' if bar.get_facecolor() == (0.17254901960784313, 0.6274509803921569, 0.17254901960784313, 1.0) else 'normal'
        axes[1].text(bar.get_x()+bar.get_width()/2., m+s+0.01, f'{m:.3f}Â±{s:.3f}',
                     ha='center', va='bottom', fontsize=9, fontweight=fw)

    plt.tight_layout()
    plt.savefig(CONFIG["plot_calibration"], dpi=300)
    plt.close()
    print(f"âœ… æ ¡å‡†æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {CONFIG['plot_calibration']}")

# ==============================================================================
# 8. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    summary, all_results = run_ablation_experiments()

    print("\n" + "="*60)
    print("ğŸ“Š æ¶ˆèå®éªŒæœ€ç»ˆç»“æœï¼ˆå‡å€¼ Â± æ ‡å‡†å·®, n={})".format(CONFIG['n_runs']))
    print("="*60)
    header = f"{'å˜ä½“':<25} {'RMSE':<15} {'MAE':<15} {'RÂ²':<15} {'Spearman':<15} {'ECE':<15} {'p vs Ours'}"
    print(header)
    print("-"*100)
    for name in summary.keys():
        s = summary[name]
        rmse = f"{s['rmse_mean']:.2f}Â±{s['rmse_std']:.2f}"
        mae = f"{s['mae_mean']:.2f}Â±{s['mae_std']:.2f}"
        r2 = f"{s['r2_mean']:.3f}Â±{s['r2_std']:.3f}"
        sp = f"{s['spearman_mean']:.3f}Â±{s['spearman_std']:.3f}" if 'spearman_mean' in s else 'N/A'
        ece = f"{s['ece_mean']:.3f}Â±{s['ece_std']:.3f}" if 'ece_mean' in s else 'N/A'
        p = f"{s['p_vs_ours']:.4f}" if 'p_vs_ours' in s else '-'
        print(f"{name:<25} {rmse:<15} {mae:<15} {r2:<15} {sp:<15} {ece:<15} {p}")
    print("="*60)

    # ä¿å­˜æ±‡æ€»ç»“æœï¼ˆä¸åŒ…å«MSEï¼‰
    pd.DataFrame(summary).T.to_csv('ablation_results_summary.csv')
    print("âœ… æ±‡æ€»ç»“æœå·²ä¿å­˜è‡³ ablation_results_summary.csv")

    plot_ablation_results(summary)
    plot_calibration_ablation(summary)