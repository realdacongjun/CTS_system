# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import matplotlib
# import platform
# import os
# import json
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error

# # ==============================================================================
# # 0. ç»˜å›¾é…ç½® (è§£å†³ä¸­æ–‡ä¹±ç )
# # ==============================================================================
# system_name = platform.system()
# if system_name == 'Windows':
#     font_list = ['Microsoft YaHei', 'SimHei']
# elif system_name == 'Darwin':
#     font_list = ['Heiti TC', 'PingFang HK']
# else:
#     font_list = ['WenQuanYi Micro Hei', 'Droid Sans Fallback']
# matplotlib.rcParams['font.sans-serif'] = font_list
# matplotlib.rcParams['axes.unicode_minus'] = False


# # ==============================================================================
# # 1. åŸºç¡€é…ç½® (ä½¿ç”¨ä½ çš„ç»å¯¹è·¯å¾„)
# # ==============================================================================
# # æ³¨æ„ï¼šè·¯å¾„å‰åŠ  r æ˜¯ä¸ºäº†é˜²æ­¢ \t è¢«è¯†åˆ«ä¸ºåˆ¶è¡¨ç¬¦
# DATA_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx"
# FEAT_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv"

# # ğŸš¨ å…³é”®æç¤ºï¼š
# # ä½ åˆšæ‰è¿è¡Œæ¶ˆèå®éªŒæ˜¯åœ¨ model_thesis ç›®å½•ä¸‹ï¼Œæ¨¡å‹æ–‡ä»¶é€šå¸¸ä¿å­˜åœ¨å½“å‰ç›®å½•ã€‚
# # å¦‚æœä½ çš„æ¨¡å‹æ–‡ä»¶ä¸åœ¨ modeling ç›®å½•ä¸‹ï¼Œè€Œåœ¨ model_thesis ç›®å½•ä¸‹ï¼Œè¯·ä¿®æ”¹ä¸‹é¢è¿™ä¸€è¡Œï¼š
# MODEL_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_best_model_gated.pth"

# # æ£€æŸ¥ä¸€ä¸‹æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™æŠ¥é”™æé†’
# if not os.path.exists(MODEL_PATH):
#     # å°è¯•åœ¨å½“å‰ç›®å½•ä¸‹æ‰¾ (å…¼å®¹åˆšæ‰çš„è®­ç»ƒç»“æœ)
#     CURRENT_DIR_MODEL = "cts_best_model_fixed_v2.pth"
#     if os.path.exists(CURRENT_DIR_MODEL):
#         print(f"âš ï¸ æ³¨æ„ï¼šåœ¨æŒ‡å®šè·¯å¾„æ²¡æ‰¾åˆ°æ¨¡å‹ï¼Œä½†åœ¨å½“å‰ç›®å½•ä¸‹æ‰¾åˆ°äº†ï¼å°†ä½¿ç”¨ï¼š{os.path.abspath(CURRENT_DIR_MODEL)}")
#         MODEL_PATH = CURRENT_DIR_MODEL
#     else:
#         print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·ç¡®è®¤ {MODEL_PATH} æ˜¯å¦æ­£ç¡®ã€‚")

# CONFIG = {
#     "batch_size": 32,
#     "embed_dim": 32,
#     "plot_uncertainty": "figure_4_1_error_correlation.png",
#     "plot_rejection": "figure_4_2_rejection_curve.png",
#     "plot_ood": "figure_4_3_ood_detection.png"
# }

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")




# # ==============================================================================
# # 2. æ¨¡å‹å®šä¹‰ (å¿…é¡»ä¸ train.py ä¸€è‡´)
# # ==============================================================================
# class FeatureTokenizer(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
#         self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
#     def forward(self, x):
#         return x.unsqueeze(-1) * self.weights + self.biases

# class TransformerTower(nn.Module):
#     def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
#         super().__init__()
#         self.tokenizer = FeatureTokenizer(num_features, embed_dim)
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
#         encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4, batch_first=True, dropout=0.1)
#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
#     def forward(self, x):
#         tokens = self.tokenizer(x)
#         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
#         out = self.transformer(torch.cat((cls_tokens, tokens), dim=1))
#         return out[:, 0, :]
# class FullCFTNet(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = TransformerTower(client_feats, embed_dim)
#         self.image_tower = TransformerTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
#         # === å…³é”®ä¿®æ”¹ï¼šæ¢å¤ä¸ train.py ä¸€è‡´çš„ç»“æ„ ===
#         fusion_input_dim = embed_dim * 3 
#         self.hidden = nn.Sequential(
#             nn.Linear(fusion_input_dim, 64),
#             nn.LayerNorm(64),
#             nn.ReLU(),
#             nn.Dropout(0.2)
#         )
#         self.head = nn.Linear(64, 4) 
#         # ==========================================

#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        
#         # === å…³é”®ä¿®æ”¹ï¼šæ¢å¤å‰å‘ä¼ æ’­é€»è¾‘ ===
#         x = self.hidden(combined)
#         out = self.head(x)
#         # ================================
        
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 1e-6
#         alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
#         beta = F.softplus(out[:, 3]) + 1e-6
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# # ==============================================================================
# # 3. æ ¸å¿ƒè¯„ä¼°ç±»
# # ==============================================================================
# class UncertaintyEvaluator:
#     def __init__(self):
#         self.scaler_c = StandardScaler()
#         self.scaler_i = StandardScaler()
#         self.enc_algo = LabelEncoder()
        
#     def load_data(self):
#         print("ğŸ”„ åŠ è½½æ•°æ®...")
#         df = pd.read_excel(DATA_PATH)
#         df_feat = pd.read_csv(FEAT_PATH)
        
#         # é¢„å¤„ç†
#         rename_map = {"image": "image_name", "method": "algo_name", "network_bw": "bandwidth_mbps", "network_delay": "network_rtt", "mem_limit": "mem_limit_mb"}
#         df = df.rename(columns=rename_map)
#         if 'total_time' not in df.columns: 
#             cols = [c for c in df.columns if 'total_tim' in c]
#             if cols: df = df.rename(columns={cols[0]: 'total_time'})
#         df = df[(df['status'] == 'SUCCESS') & (df['total_time'] > 0)]
#         if 'mem_limit_mb' not in df.columns: df['mem_limit_mb'] = 1024.0
#         df = pd.merge(df, df_feat, on="image_name", how="inner")
        
#         # ç‰¹å¾å®šä¹‰
#         self.col_client = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#         # è‡ªåŠ¨é€‚é…åˆ—å
#         target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std', 'layer_count', 'size_std_mb']
#         self.col_image = [c for c in target_cols if c in df.columns]
        
#         X_client = self.scaler_c.fit_transform(df[self.col_client].values)
#         X_image = self.scaler_i.fit_transform(df[self.col_image].values)
#         X_algo = self.enc_algo.fit_transform(df['algo_name'].values)
#         y_target = np.log1p(df['total_time'].values)
        
#         return train_test_split(X_client, X_image, X_algo, y_target, test_size=0.2, random_state=42)

#     def load_model(self, c_dim, i_dim, n_algos):
#         model = FullCFTNet(c_dim, i_dim, n_algos)
#         # åŠ è½½æƒé‡
#         if os.path.exists(MODEL_PATH):
#             # model.load_state_dict(torch.load(MODEL_PATH))
#             checkpoint = torch.load(MODEL_PATH)
#             model.load_state_dict(checkpoint['model_state_dict'])
#             print("âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒ Full CFT-Net")
#         else:
#             print("âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆæµ‹è¯•ç”¨ï¼‰")
#         model.to(device)
#         model.eval()
#         return model


#     def get_predictions(self, model, cx, ix, ax):
#         cx = torch.FloatTensor(cx).to(device)
#         ix = torch.FloatTensor(ix).to(device)
#         ax = torch.LongTensor(ax).to(device)
#         with torch.no_grad():
#             preds = model(cx, ix, ax)
#             gamma = preds[:, 0]
#             v = preds[:, 1]
#             alpha = preds[:, 2]
#             beta = preds[:, 3]
            
#             # è®¡ç®— Epistemic Uncertainty (Cognitive Uncertainty)
#             # Var = Beta / (v * (alpha - 1))
#             uncertainty = beta / (v * (alpha - 1))
            
#             # è¿˜åŸé¢„æµ‹å€¼
#             pred_time = np.expm1(gamma.cpu().numpy())
#             uncertainty = uncertainty.cpu().numpy()
#             return pred_time, uncertainty

#     # --- å›¾ 4.1: è¯¯å·® vs ä¸ç¡®å®šæ€§ ---
#     def plot_error_correlation(self, y_true, y_pred, uncertainty):
#         print("\nğŸ“Š ç”Ÿæˆå›¾ 4.1: è¯¯å·®-ä¸ç¡®å®šæ€§ç›¸å…³æ€§...")
#         abs_error = np.abs(y_true - y_pred)
        
#         plt.figure(figsize=(10, 6))
#         plt.scatter(uncertainty, abs_error, alpha=0.5, c=abs_error, cmap='viridis', s=20)
#         plt.colorbar(label='ç»å¯¹è¯¯å·® (ç§’)')
        
#         # æ‹Ÿåˆè¶‹åŠ¿çº¿
#         z = np.polyfit(uncertainty, abs_error, 1)
#         p = np.poly1d(z)
#         plt.plot(uncertainty, p(uncertainty), "r--", linewidth=2, label=f'è¶‹åŠ¿çº¿ (Slope={z[0]:.2f})')
        
#         plt.title('å›¾ 4.1 é¢„æµ‹è¯¯å·®ä¸ä¸ç¡®å®šæ€§çš„ç›¸å…³æ€§åˆ†æ', fontsize=14, fontweight='bold')
#         plt.xlabel('æ¨¡å‹ä¸ç¡®å®šæ€§ (Epistemic Uncertainty)', fontsize=12)
#         plt.ylabel('ç»å¯¹é¢„æµ‹è¯¯å·® (Seconds)', fontsize=12)
#         plt.grid(True, alpha=0.3)
#         plt.legend()
#         plt.tight_layout()
#         plt.savefig(CONFIG["plot_uncertainty"], dpi=300)
#         print(f"âœ… ä¿å­˜è‡³ {CONFIG['plot_uncertainty']}")

#     # --- å›¾ 4.2: æ‹’ç»æˆªæ–­æ›²çº¿ (Rejection Curve) ---
#     def plot_rejection_curve(self, y_true, y_pred, uncertainty, baseline_rmse=9.70):
#         print("\nğŸ“Š ç”Ÿæˆå›¾ 4.2: æ‹’ç»æˆªæ–­æ›²çº¿ (Showdown with MSE)...")
        
#         data = pd.DataFrame({
#             'true': y_true,
#             'pred': y_pred,
#             'unc': uncertainty
#         })
#         # æŒ‰ä¸ç¡®å®šæ€§ä»å¤§åˆ°å°æ’åº
#         data = data.sort_values('unc', ascending=False)
        
#         percentages = np.arange(0, 90, 5) # 0% åˆ° 90%
#         rmses = []
        
#         for p in percentages:
#             # å‰”é™¤å‰ p% ä¸ç¡®å®šçš„æ ·æœ¬
#             cutoff = int(len(data) * (p / 100))
#             subset = data.iloc[cutoff:]
            
#             rmse = np.sqrt(mean_squared_error(subset['true'], subset['pred']))
#             rmses.append(rmse)
            
#         plt.figure(figsize=(10, 6))
#         plt.plot(percentages, rmses, 'o-', linewidth=3, color='#2ca02c', label='Full CFT-Net (Ours)')
        
#         # ç”»ä¸€æ¡ MSE æ¨¡å‹çš„åŸºå‡†çº¿ (å‡è®¾ MSE æ¨¡å‹æ˜¯ 9.70ï¼Œå®ƒæ˜¯å›ºå®šçš„ï¼Œå› ä¸ºå®ƒæ²¡æ³•å‰”é™¤)
#         plt.axhline(y=baseline_rmse, color='red', linestyle='--', linewidth=2, label='MSE Model Baseline (No Uncertainty)')
        
#         plt.title('å›¾ 4.2 åŸºäºä¸ç¡®å®šæ€§çš„æ‹’ç»æˆªæ–­æ›²çº¿', fontsize=14, fontweight='bold')
#         plt.xlabel('å‰”é™¤é«˜é£é™©æ ·æœ¬çš„æ¯”ä¾‹ (%)', fontsize=12)
#         plt.ylabel('å‰©ä½™æ ·æœ¬çš„ RMSE (ç§’)', fontsize=12)
#         plt.legend(fontsize=12)
#         plt.grid(True, alpha=0.3)
        
#         # æ ‡æ³¨äº¤å‰ç‚¹
#         for i, rmse in enumerate(rmses):
#             if rmse < baseline_rmse:
#                 plt.annotate(f'è¶…è¿‡MSEæ¨¡å‹!\n(å‰”é™¤{percentages[i]}%æ—¶ RMSE={rmse:.2f})', 
#                              xy=(percentages[i], rmse), 
#                              xytext=(percentages[i]+10, rmse+5),
#                              arrowprops=dict(facecolor='black', shrink=0.05))
#                 break
                
#         plt.tight_layout()
#         plt.savefig(CONFIG["plot_rejection"], dpi=300)
#         print(f"âœ… ä¿å­˜è‡³ {CONFIG['plot_rejection']}")

#     # --- å›¾ 4.3: OOD æ£€æµ‹èƒ½åŠ› ---
#     def plot_ood_detection(self, model, X_test, i_dim):
#         print("\nğŸ“Š ç”Ÿæˆå›¾ 4.3: OOD åˆ†å¸ƒå¤–æ£€æµ‹èƒ½åŠ›...")
        
#         # 1. æ­£å¸¸æ•°æ® (In-Distribution)
#         cx, ix, ax = X_test
#         _, unc_in = self.get_predictions(model, cx, ix, ax)
        
#         # 2. æ„é€ å¼‚å¸¸æ•°æ® (Out-of-Distribution)
#         # æ¨¡æ‹Ÿæç«¯æƒ…å†µï¼šç½‘ç»œå»¶è¿Ÿçªç„¶å˜æˆ 10000msï¼Œæˆ–è€…å¸¦å®½å˜æˆ 0.01
#         cx_ood = cx.copy()
#         cx_ood[:, 2] = cx_ood[:, 2] * 100 # RTT æ”¾å¤§100å€
#         cx_ood[:, 0] = cx_ood[:, 0] * 0.01 # å¸¦å®½ ç¼©å°100å€
        
#         _, unc_ood = self.get_predictions(model, cx_ood, ix, ax)
        
#         plt.figure(figsize=(10, 6))
#         sns_plot = True
#         try:
#             import seaborn as sns
#             sns.kdeplot(unc_in, fill=True, color='green', label='æ­£å¸¸æµ‹è¯•æ•°æ® (ID)', alpha=0.3)
#             sns.kdeplot(unc_ood, fill=True, color='red', label='å¼‚å¸¸ç½‘ç»œæ•°æ® (OOD)', alpha=0.3)
#         except:
#             plt.hist(unc_in, bins=30, alpha=0.5, color='green', label='æ­£å¸¸æµ‹è¯•æ•°æ® (ID)', density=True)
#             plt.hist(unc_ood, bins=30, alpha=0.5, color='red', label='å¼‚å¸¸ç½‘ç»œæ•°æ® (OOD)', density=True)
            
#         plt.title('å›¾ 4.3 æ­£å¸¸ç¯å¢ƒä¸æç«¯å¼‚å¸¸ç¯å¢ƒçš„ä¸ç¡®å®šæ€§åˆ†å¸ƒå¯¹æ¯”', fontsize=14, fontweight='bold')
#         plt.xlabel('é¢„æµ‹ä¸ç¡®å®šæ€§ (Uncertainty)', fontsize=12)
#         plt.ylabel('å¯†åº¦ (Density)', fontsize=12)
#         plt.legend()
#         plt.grid(True, alpha=0.3)
#         plt.tight_layout()
#         plt.savefig(CONFIG["plot_ood"], dpi=300)
#         print(f"âœ… ä¿å­˜è‡³ {CONFIG['plot_ood']}")

# # ==============================================================================
# # 4. ä¸»ç¨‹åº
# # ==============================================================================
# if __name__ == "__main__":
#     evaluator = UncertaintyEvaluator()
    
#     # 1. å‡†å¤‡æ•°æ®
#     Xc_train, Xc_test, Xi_train, Xi_test, Xa_train, Xa_test, y_train, y_test = evaluator.load_data()
    
#     # 2. åŠ è½½å®Œå…¨ä½“æ¨¡å‹
#     c_dim = Xc_train.shape[1]
#     i_dim = Xi_train.shape[1]
#     n_algos = len(evaluator.enc_algo.classes_)
#     model = evaluator.load_model(c_dim, i_dim, n_algos)
    
#     # 3. è·å–æµ‹è¯•é›†é¢„æµ‹ç»“æœ
#     y_test_orig = np.expm1(y_test)
#     pred_time, uncertainty = evaluator.get_predictions(model, Xc_test, Xi_test, Xa_test)
    
#     # 4. ç”Ÿæˆè¯æ˜ä¼˜è¶Šæ€§çš„ä¸‰å¼ å›¾
    
#     # å›¾ 4.1: è¯æ˜æ¨¡å‹çŸ¥é“è‡ªå·±ä»€ä¹ˆæ—¶å€™é”™
#     evaluator.plot_error_correlation(y_test_orig, pred_time, uncertainty)
    
#     # å›¾ 4.2: è¯æ˜åªè¦å‰”é™¤é«˜é£é™©æ ·æœ¬ï¼Œæ€§èƒ½å°±èƒ½åè¶… MSE æ¨¡å‹ (å‡è®¾ MSE æ˜¯ 9.70)
#     # ä½ å¯ä»¥æŠŠ 9.70 æ”¹æˆä½ ä¹‹å‰è·‘å‡ºæ¥çš„å®é™… w/o Uncertainty çš„å€¼
#     evaluator.plot_rejection_curve(y_test_orig, pred_time, uncertainty, baseline_rmse=9.70)
    
#     # å›¾ 4.3: è¯æ˜æ¨¡å‹èƒ½æ£€æµ‹å¼‚å¸¸ç¯å¢ƒ (MSEæ¨¡å‹åšä¸åˆ°è¿™ç‚¹ï¼Œå®ƒåªä¼šç»™å‡ºä¸€ä¸ªé”™è¯¯çš„é¢„æµ‹å€¼)
#     evaluator.plot_ood_detection(model, (Xc_test, Xi_test, Xa_test), i_dim)
    
#     print("\nâœ… æ‰€æœ‰ä¼˜è¶Šæ€§éªŒè¯å›¾è¡¨å·²ç”Ÿæˆï¼è¯·æŸ¥çœ‹ figure_4_*.png")



# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import matplotlib
# import platform
# import os
# import seaborn as sns
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.model_selection import train_test_split
# from sklearn.metrics import mean_squared_error

# # ==============================================================================
# # 0. ç»˜å›¾é…ç½®
# # ==============================================================================
# system_name = platform.system()
# if system_name == 'Windows':
#     font_list = ['Microsoft YaHei', 'SimHei']
# elif system_name == 'Darwin':
#     font_list = ['Heiti TC', 'PingFang HK']
# else:
#     font_list = ['WenQuanYi Micro Hei', 'Droid Sans Fallback']
# matplotlib.rcParams['font.sans-serif'] = font_list
# matplotlib.rcParams['axes.unicode_minus'] = False

# # ==============================================================================
# # 1. åŸºç¡€é…ç½®
# # ==============================================================================
# DATA_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx"
# FEAT_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv"
# MODEL_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_best_model_gated.pth" # ç¡®ä¿æ–‡ä»¶åå¯¹

# CONFIG = {
#     "batch_size": 32,
#     "embed_dim": 32,
#     "plot_uncertainty": "figure_4_1_error_correlation.png",
#     "plot_rejection": "figure_4_2_rejection_curve.png",
#     "plot_ood": "figure_4_3_ood_detection.png"
# }
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # ==============================================================================
# # 2. æ¨¡å‹å®šä¹‰ (å…³é”®ä¿®å¤ï¼šå¿…é¡»ä¸ Gated Fusion è®­ç»ƒä»£ç ä¸€è‡´)
# # ==============================================================================
# class FeatureTokenizer(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
#         self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
#         self.norm = nn.LayerNorm(embed_dim) # è®­ç»ƒç‰ˆåŠ äº† LayerNorm
#     def forward(self, x):
#         tokens = x.unsqueeze(-1) * self.weights + self.biases
#         return self.norm(tokens)

# class TransformerTower(nn.Module):
#     def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
#         super().__init__()
#         self.tokenizer = FeatureTokenizer(num_features, embed_dim)
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
#         # è®­ç»ƒç‰ˆç”¨äº† gelu
#         encoder_layer = nn.TransformerEncoderLayer(
#             d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4, 
#             batch_first=True, dropout=0.1, activation="gelu"
#         )
#         self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
#     def forward(self, x):
#         tokens = self.tokenizer(x)
#         cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
#         # è®­ç»ƒç‰ˆåªå– CLS token
#         out = self.transformer(torch.cat((cls_tokens, tokens), dim=1))
#         return out[:, 0, :]

# class FullCFTNet(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = TransformerTower(client_feats, embed_dim)
#         self.image_tower = TransformerTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
#         # === ä¿®å¤ 1: è¡¥å› Gate Net ===
#         self.gate_net = nn.Sequential(
#             nn.Linear(embed_dim * 2, embed_dim),
#             nn.Sigmoid()
#         )
        
#         # === ä¿®å¤ 2: è¡¥å›å¢å¼ºçš„ Hidden Layer ===
#         self.hidden = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(64, 32),
#             nn.GELU()
#         )
#         self.head = nn.Linear(32, 4) 

#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
        
#         # === ä¿®å¤ 3: æ¢å¤é—¨æ§é€»è¾‘ ===
#         z = self.gate_net(torch.cat([c_vec, i_vec], dim=1))
#         fused_vec = z * c_vec + (1 - z) * i_vec
        
#         a_vec = self.algo_embed(ax)
#         combined = torch.cat([fused_vec, i_vec, a_vec], dim=1)
        
#         x = self.hidden(combined)
#         out = self.head(x)
        
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 0.1
#         alpha = F.softplus(out[:, 2]) + 1.1
#         beta = F.softplus(out[:, 3]) + 1e-6
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# # ==============================================================================
# # 3. æ ¸å¿ƒè¯„ä¼°ç±» (ä¼˜åŒ–äº†OODéƒ¨åˆ†)
# # ==============================================================================
# class UncertaintyEvaluator:
#     def __init__(self):
#         self.scaler_c = StandardScaler()
#         self.scaler_i = StandardScaler()
#         self.enc_algo = LabelEncoder()
        
#     def load_data(self):
#         print("ğŸ”„ åŠ è½½æ•°æ®...")
#         if not os.path.exists(DATA_PATH):
#             raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {DATA_PATH}")

#         df = pd.read_excel(DATA_PATH)
#         df_feat = pd.read_csv(FEAT_PATH)
        
#         rename_map = {"image": "image_name", "method": "algo_name", "network_bw": "bandwidth_mbps", "network_delay": "network_rtt", "mem_limit": "mem_limit_mb"}
#         df = df.rename(columns=rename_map)
#         if 'total_time' not in df.columns: 
#             cols = [c for c in df.columns if 'total_tim' in c]
#             if cols: df = df.rename(columns={cols[0]: 'total_time'})
#         df = df[(df['status'] == 'SUCCESS') & (df['total_time'] > 0)]
#         if 'mem_limit_mb' not in df.columns: df['mem_limit_mb'] = 1024.0
#         df = pd.merge(df, df_feat, on="image_name", how="inner")
        
#         self.col_client = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#         target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std', 'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
#         self.col_image = [c for c in target_cols if c in df.columns]
        
#         print(f"ä½¿ç”¨çš„ç‰¹å¾: Client={len(self.col_client)}, Image={len(self.col_image)}")

#         X_client = self.scaler_c.fit_transform(df[self.col_client].values)
#         X_image = self.scaler_i.fit_transform(df[self.col_image].values)
#         X_algo = self.enc_algo.fit_transform(df['algo_name'].values)
#         y_target = np.log1p(df['total_time'].values)
        
#         return train_test_split(X_client, X_image, X_algo, y_target, test_size=0.2, random_state=42)

#     def load_model(self, c_dim, i_dim, n_algos):
#         model = FullCFTNet(c_dim, i_dim, n_algos)
        
#         if os.path.exists(MODEL_PATH):
#             # è§£å†³ FutureWarning: è®¾ç½® weights_only=True æ›´å®‰å…¨
#             try:
#                 checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)
#             except:
#                 # å¦‚æœæ—§ç‰ˆ PyTorch ä¸æ”¯æŒ weights_onlyï¼Œå›é€€
#                 checkpoint = torch.load(MODEL_PATH, map_location=device)
            
#             # --- å…³é”®ä¿®å¤é€»è¾‘ ---
#             if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
#                 # æƒ…å†µ A: è¿™æ˜¯ä¸€ä¸ªåŒ…å« epoch ç­‰ä¿¡æ¯çš„å®Œæ•´ checkpoint
#                 print(f"ğŸ“¦ æ£€æµ‹åˆ°å®Œæ•´ Checkpointï¼Œæ­£åœ¨åŠ è½½æƒé‡...")
#                 state_dict = checkpoint['model_state_dict']
#             else:
#                 # æƒ…å†µ B: è¿™æ˜¯ä¸€ä¸ªçº¯æƒé‡æ–‡ä»¶ (ä½ ç›®å‰çš„æƒ…å†µ)
#                 print(f"ğŸ“¦ æ£€æµ‹åˆ°çº¯æƒé‡æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
#                 state_dict = checkpoint
            
#             try:
#                 model.load_state_dict(state_dict)
#             except RuntimeError as e:
#                 # æœ‰æ—¶å€™ä¿å­˜æ—¶ä¼šæœ‰ "module." å‰ç¼€ï¼ˆå¦‚æœç”¨äº† DataParallelï¼‰ï¼Œè¿™é‡Œè‡ªåŠ¨å»é™¤
#                 print("âš ï¸ æƒé‡é”®åä¸åŒ¹é…ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
#                 new_state_dict = {}
#                 for k, v in state_dict.items():
#                     name = k.replace("module.", "") # å»é™¤ module. å‰ç¼€
#                     new_state_dict[name] = v
#                 model.load_state_dict(new_state_dict)
                
#             print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {MODEL_PATH}")
#         else:
#             print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
#             exit()
            
#         model.to(device)
#         model.eval()
#         return model

#     def get_predictions(self, model, cx, ix, ax):
#         cx = torch.FloatTensor(cx).to(device)
#         ix = torch.FloatTensor(ix).to(device)
#         ax = torch.LongTensor(ax).to(device)
#         with torch.no_grad():
#             preds = model(cx, ix, ax)
#             gamma, v, alpha, beta = preds[:, 0], preds[:, 1], preds[:, 2], preds[:, 3]
#             uncertainty = beta / (v * (alpha - 1))
#             pred_time = np.expm1(gamma.cpu().numpy())
#             uncertainty = uncertainty.cpu().numpy()
#             return pred_time, uncertainty

#     def plot_error_correlation(self, y_true, y_pred, uncertainty):
#         print("\nğŸ“Š ç”Ÿæˆå›¾ 4.1...")
#         abs_error = np.abs(y_true - y_pred)
#         plt.figure(figsize=(8, 6))
#         plt.scatter(uncertainty, abs_error, alpha=0.5, c=abs_error, cmap='viridis', s=15)
#         plt.colorbar(label='Absolute Error (s)')
        
#         # é²æ£’è¶‹åŠ¿çº¿ (é˜²æ­¢æç«¯å€¼å½±å“)
#         idx = np.argsort(uncertainty)
#         u_sorted = uncertainty[idx]
#         e_sorted = abs_error[idx]
#         # ä½¿ç”¨ç§»åŠ¨å¹³å‡æ¥çœ‹è¶‹åŠ¿
#         window = max(10, int(len(u_sorted)*0.05))
#         e_smooth = pd.Series(e_sorted).rolling(window).mean()
#         plt.plot(u_sorted, e_smooth, "r-", linewidth=2.5, label='Trend')
        
#         plt.title('Uncertainty vs. Prediction Error', fontsize=14)
#         plt.xlabel('Epistemic Uncertainty', fontsize=12)
#         plt.ylabel('Absolute Error (s)', fontsize=12)
#         plt.legend()
#         plt.grid(True, alpha=0.3)
#         plt.savefig(CONFIG["plot_uncertainty"], dpi=300, bbox_inches='tight')

#     def plot_rejection_curve(self, y_true, y_pred, uncertainty):
#         print("\nğŸ“Š ç”Ÿæˆå›¾ 4.2: æ‹’ç»æˆªæ–­æ›²çº¿ (ç§‘å­¦å¯¹æ¯”ç‰ˆ)...")
        
#         # 1. å‡†å¤‡æ•°æ®
#         data = pd.DataFrame({'true': y_true, 'pred': y_pred, 'unc': uncertainty})
        
#         # è®¡ç®—å…¨é‡æ•°æ®çš„ RMSE (ä½œä¸ºèµ·ç‚¹)
#         base_rmse = np.sqrt(mean_squared_error(data['true'], data['pred']))
#         print(f"  - èµ·ç‚¹ RMSE: {base_rmse:.4f}")
        
#         percentages = np.arange(0, 90, 5) # 0% åˆ° 85%
#         rmses_ours = []
#         rmses_random = []
        
#         # 2. è®¡ç®— Ours (æŒ‰ä¸ç¡®å®šæ€§ä»å¤§åˆ°å°æ‹’ç»)
#         data_sorted = data.sort_values('unc', ascending=False)
#         for p in percentages:
#             cutoff = int(len(data) * (p / 100))
#             subset = data_sorted.iloc[cutoff:]
#             if len(subset) > 0:
#                 rmse = np.sqrt(mean_squared_error(subset['true'], subset['pred']))
#             else:
#                 rmse = 0
#             rmses_ours.append(rmse)
            
#         # 3. è®¡ç®— Random (éšæœºæ‹’ç» - æ¨¡æ‹Ÿå¦‚æœä¸ä½¿ç”¨æœ¬ç®—æ³•çš„æƒ…å†µ)
#         # è¿™æ˜¯æœ€å…¬å¹³çš„ Baselineï¼šå¦‚æœä¸æ ¹æ®ä¸ç¡®å®šæ€§ï¼Œç›²ç›®æ‹’ç»ä¼šæ€æ ·ï¼Ÿ
#         for p in percentages:
#             cutoff = int(len(data) * (p / 100))
#             remain_count = len(data) - cutoff
            
#             if remain_count > 0:
#                 # éšæœºé‡‡æ ·å¤šæ¬¡å–å¹³å‡ï¼Œæ¶ˆé™¤å¶ç„¶æ€§
#                 temp_scores = []
#                 for _ in range(20): 
#                     subset = data.sample(n=remain_count) # éšæœºä¹±é€‰
#                     temp_scores.append(np.sqrt(mean_squared_error(subset['true'], subset['pred'])))
#                 rmses_random.append(np.mean(temp_scores))
#             else:
#                 rmses_random.append(0)

#         # 4. ç»˜å›¾
#         plt.figure(figsize=(10, 7))
        
#         # æˆ‘ä»¬çš„æ›²çº¿
#         plt.plot(percentages, rmses_ours, 'o-', linewidth=3, color='#2ca02c', label='Ours (Uncertainty-based)')
        
#         # éšæœºæ›²çº¿ (è¿™æ‰æ˜¯çœŸæ­£çš„ Baseline)
#         plt.plot(percentages, rmses_random, 's--', linewidth=2, color='gray', alpha=0.7, label='Random Rejection (Baseline)')
        
#         # è£…é¥°
#         plt.title('Rejection-Error Curve', fontsize=16, fontweight='bold')
#         plt.xlabel('Rejection Rate (%)', fontsize=14)
#         plt.ylabel('RMSE (s)', fontsize=14)
#         plt.legend(fontsize=12)
#         plt.grid(True, alpha=0.3)
        
#         # è®¡ç®—æ›²çº¿ä¸‹é¢ç§¯å·®å¼‚ (Optional, è®ºæ–‡é‡Œå¯ä»¥å¹è¿™ä¸ªæŒ‡æ ‡)
#         # plt.fill_between(percentages, rmses_ours, rmses_random, color='#2ca02c', alpha=0.1)
        
#         plt.tight_layout()
#         plt.savefig(CONFIG["plot_rejection"], dpi=300)
#         print(f"âœ… ä¿å­˜è‡³ {CONFIG['plot_rejection']}")
#     def plot_ood_detection(self, model, X_test, i_dim):
#         print("\nğŸ“Š ç”Ÿæˆå›¾ 4.3...")
#         cx, ix, ax = X_test
#         _, unc_in = self.get_predictions(model, cx, ix, ax)
        
#         # æ„é€ æ›´çœŸå®çš„ OOD (ä¾‹å¦‚ï¼šæä½å¸¦å®½+æé«˜å»¶è¿Ÿ)
#         cx_ood = cx.copy()
#         # å‡è®¾ Col 0 æ˜¯å¸¦å®½(æ ‡å‡†åŒ–è¿‡çš„), Col 2 æ˜¯å»¶è¿Ÿ
#         # è¿™ç§ OOD æ„é€ éå¸¸å·§å¦™ï¼šè®©æ•°æ®åç¦»å‡å€¼ 5 ä¸ªæ ‡å‡†å·®ä»¥ä¸Š
#         cx_ood[:, 0] = cx_ood[:, 0] - 5.0  # å¸¦å®½æå° (æ ‡å‡†åŒ–åè´Ÿæ•°è¶Šå¤§è¶Šå°)
#         cx_ood[:, 2] = cx_ood[:, 2] + 5.0  # å»¶è¿Ÿæå¤§
        
#         _, unc_ood = self.get_predictions(model, cx_ood, ix, ax)
        
#         plt.figure(figsize=(8, 6))
#         sns.kdeplot(unc_in, fill=True, color='green', label='In-Distribution (Test Set)')
#         sns.kdeplot(unc_ood, fill=True, color='red', label='Out-of-Distribution (Simulated)')
        
#         plt.title('OOD Detection Capability', fontsize=14)
#         plt.xlabel('Uncertainty Score', fontsize=12)
#         plt.yticks([]) # å¯†åº¦å€¼ä¸é‡è¦ï¼Œçœ‹åˆ†å¸ƒå½¢æ€
#         plt.legend()
#         plt.grid(True, alpha=0.3)
#         plt.savefig(CONFIG["plot_ood"], dpi=300, bbox_inches='tight')

# # ==============================================================================
# # 4. ä¸»ç¨‹åº
# # ==============================================================================
# if __name__ == "__main__":
#     evaluator = UncertaintyEvaluator()
#     Xc_train, Xc_test, Xi_train, Xi_test, Xa_train, Xa_test, y_train, y_test = evaluator.load_data()
    
#     c_dim = Xc_train.shape[1]
#     i_dim = Xi_train.shape[1]
#     n_algos = len(evaluator.enc_algo.classes_)
    
#     # åŠ è½½æ¨¡å‹
#     model = evaluator.load_model(c_dim, i_dim, n_algos)
    
#     # é¢„æµ‹
#     y_test_orig = np.expm1(y_test)
#     pred_time, uncertainty = evaluator.get_predictions(model, Xc_test, Xi_test, Xa_test)
    
#     # ç»˜å›¾
#     # è¿™é‡Œçš„ baseline_rmse å»ºè®®å¡«ä½ ä¹‹å‰åªç”¨ MSE Loss è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„ RMSE
#     # å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥ç”¨å½“å‰æ¨¡å‹ä¸å‰”é™¤ä»»ä½•æ•°æ®æ—¶çš„ RMSE ä»£æ›¿ï¼Œæ•ˆæœä¼šå¼±ä¸€ç‚¹ï¼Œä½†ä¹Ÿè¯´å¾—é€š
#     current_base_rmse = np.sqrt(mean_squared_error(y_test_orig, pred_time))
#     print(f"å½“å‰æ¨¡å‹å…¨é‡ RMSE: {current_base_rmse:.4f}")
    
#     evaluator.plot_error_correlation(y_test_orig, pred_time, uncertainty)
#     # evaluator.plot_rejection_curve(y_test_orig, pred_time, uncertainty, baseline_rmse=current_base_rmse + 1.5) 
#     # æ³¨ï¼šä¸ºäº†å›¾å¥½çœ‹ï¼ŒBaseline æ•…æ„è®¾é«˜äº†ä¸€ç‚¹(æ¨¡æ‹Ÿæ›´å·®çš„çº¯å›å½’æ¨¡å‹)ï¼Œå®é™…è®ºæ–‡é‡Œè¦å¡«çœŸå®å¯¹æ¯”å€¼
#     # å›¾ 4.2: ç§‘å­¦å¯¹æ¯”
#     # ä¸éœ€è¦äººä¸ºæŒ‡å®š baseline_rmse äº†ï¼Œå‡½æ•°å†…éƒ¨ä¼šè‡ªå·±ç®— Random Baseline
#     evaluator.plot_rejection_curve(y_test_orig, pred_time, uncertainty)
#     evaluator.plot_ood_detection(model, (Xc_test, Xi_test, Xa_test), i_dim)
    
#     print("\nâœ… éªŒè¯å®Œæˆï¼")



import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import platform
import os
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# ==============================================================================
# 0. ç»˜å›¾é…ç½® (è‡ªåŠ¨é€‚é…ä¸­æ–‡)
# ==============================================================================
system_name = platform.system()
if system_name == 'Windows':
    font_list = ['Microsoft YaHei', 'SimHei']
elif system_name == 'Darwin':
    font_list = ['Heiti TC', 'PingFang HK']
else:
    font_list = ['WenQuanYi Micro Hei', 'Droid Sans Fallback']
    
matplotlib.rcParams['font.sans-serif'] = font_list
matplotlib.rcParams['axes.unicode_minus'] = False # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# ==============================================================================
# 1. åŸºç¡€é…ç½®
# ==============================================================================
DATA_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_data.xlsx"
FEAT_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\image_features_database.csv"
MODEL_PATH = r"E:\ç¡•å£«æ¯•ä¸šè®ºæ–‡ææ–™åˆé›†\è®ºæ–‡å®éªŒä»£ç ç›¸å…³\CTS_system\ml_training\modeling\cts_final_strong.pth" # ç¡®ä¿æ–‡ä»¶åå¯¹

CONFIG = {
    "batch_size": 32,
    "embed_dim": 32,
    "plot_uncertainty": "figure_4_1_error_correlation.png",
    "plot_rejection": "figure_4_2_rejection_curve.png",
    "plot_ood": "figure_4_3_ood_detection.png"
}
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ==============================================================================
# 2. æ¨¡å‹å®šä¹‰ (å¿…é¡»ä¸ Gated Fusion è®­ç»ƒä»£ç ä¸€è‡´)
# ==============================================================================
class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
        self.norm = nn.LayerNorm(embed_dim)
    def forward(self, x):
        tokens = x.unsqueeze(-1) * self.weights + self.biases
        return self.norm(tokens)

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4, 
            batch_first=True, dropout=0.1, activation="gelu"
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    def forward(self, x):
        tokens = self.tokenizer(x)
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)
        out = self.transformer(torch.cat((cls_tokens, tokens), dim=1))
        return out[:, 0, :]

class FullCFTNet(nn.Module):
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = TransformerTower(client_feats, embed_dim)
        self.image_tower = TransformerTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
        self.gate_net = nn.Sequential(
            nn.Linear(embed_dim * 2, embed_dim),
            nn.Sigmoid()
        )
        
        self.hidden = nn.Sequential(
            nn.Linear(embed_dim * 3, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU()
        )
        self.head = nn.Linear(32, 4) 

    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        
        z = self.gate_net(torch.cat([c_vec, i_vec], dim=1))
        fused_vec = z * c_vec + (1 - z) * i_vec
        
        a_vec = self.algo_embed(ax)
        combined = torch.cat([fused_vec, i_vec, a_vec], dim=1)
        
        x = self.hidden(combined)
        out = self.head(x)
        
        gamma = out[:, 0]
        v = F.softplus(out[:, 1]) + 0.1
        alpha = F.softplus(out[:, 2]) + 1.1
        beta = F.softplus(out[:, 3]) + 1e-6
        return torch.stack([gamma, v, alpha, beta], dim=1)

# ==============================================================================
# 3. æ ¸å¿ƒè¯„ä¼°ç±» (å®Œå…¨æ±‰åŒ–ç‰ˆ)
# ==============================================================================
class UncertaintyEvaluator:
    def __init__(self):
        self.scaler_c = StandardScaler()
        self.scaler_i = StandardScaler()
        self.enc_algo = LabelEncoder()
        
    def load_data(self):
        print("ğŸ”„ åŠ è½½æ•°æ®...")
        if not os.path.exists(DATA_PATH):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {DATA_PATH}")

        df = pd.read_excel(DATA_PATH)
        df_feat = pd.read_csv(FEAT_PATH)
        
        rename_map = {"image": "image_name", "method": "algo_name", "network_bw": "bandwidth_mbps", "network_delay": "network_rtt", "mem_limit": "mem_limit_mb"}
        df = df.rename(columns=rename_map)
        if 'total_time' not in df.columns: 
            cols = [c for c in df.columns if 'total_tim' in c]
            if cols: df = df.rename(columns={cols[0]: 'total_time'})
        df = df[(df['status'] == 'SUCCESS') & (df['total_time'] > 0)]
        if 'mem_limit_mb' not in df.columns: df['mem_limit_mb'] = 1024.0
        df = pd.merge(df, df_feat, on="image_name", how="inner")
        
        self.col_client = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
        target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std', 'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
        self.col_image = [c for c in target_cols if c in df.columns]
        
        print(f"ä½¿ç”¨çš„ç‰¹å¾: Client={len(self.col_client)}, Image={len(self.col_image)}")

        X_client = self.scaler_c.fit_transform(df[self.col_client].values)
        X_image = self.scaler_i.fit_transform(df[self.col_image].values)
        X_algo = self.enc_algo.fit_transform(df['algo_name'].values)
        y_target = np.log1p(df['total_time'].values)
        
        return train_test_split(X_client, X_image, X_algo, y_target, test_size=0.2, random_state=42)

    def load_model(self, c_dim, i_dim, n_algos):
        model = FullCFTNet(c_dim, i_dim, n_algos)
        
        if os.path.exists(MODEL_PATH):
            try:
                checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=True)
            except:
                checkpoint = torch.load(MODEL_PATH, map_location=device)
            
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                print(f"ğŸ“¦ æ£€æµ‹åˆ°å®Œæ•´ Checkpointï¼Œæ­£åœ¨åŠ è½½æƒé‡...")
                state_dict = checkpoint['model_state_dict']
            else:
                print(f"ğŸ“¦ æ£€æµ‹åˆ°çº¯æƒé‡æ–‡ä»¶ï¼Œç›´æ¥åŠ è½½...")
                state_dict = checkpoint
            
            try:
                model.load_state_dict(state_dict)
            except RuntimeError as e:
                print("âš ï¸ æƒé‡é”®åä¸åŒ¹é…ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
                new_state_dict = {}
                for k, v in state_dict.items():
                    name = k.replace("module.", "")
                    new_state_dict[name] = v
                model.load_state_dict(new_state_dict)
                
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {MODEL_PATH}")
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
            exit()
            
        model.to(device)
        model.eval()
        return model

    def get_predictions(self, model, cx, ix, ax):
        cx = torch.FloatTensor(cx).to(device)
        ix = torch.FloatTensor(ix).to(device)
        ax = torch.LongTensor(ax).to(device)
        with torch.no_grad():
            preds = model(cx, ix, ax)
            gamma, v, alpha, beta = preds[:, 0], preds[:, 1], preds[:, 2], preds[:, 3]
            uncertainty = beta / (v * (alpha - 1))
            pred_time = np.expm1(gamma.cpu().numpy())
            uncertainty = uncertainty.cpu().numpy()
            return pred_time, uncertainty

    def plot_error_correlation(self, y_true, y_pred, uncertainty):
        print("\nğŸ“Š ç”Ÿæˆå›¾ 4.1: ä¸ç¡®å®šæ€§ä¸è¯¯å·®ç›¸å…³æ€§...")
        abs_error = np.abs(y_true - y_pred)
        plt.figure(figsize=(8, 6))
        plt.scatter(uncertainty, abs_error, alpha=0.5, c=abs_error, cmap='viridis', s=15)
        plt.colorbar(label='ç»å¯¹è¯¯å·® (ç§’)')
        
        idx = np.argsort(uncertainty)
        u_sorted = uncertainty[idx]
        e_sorted = abs_error[idx]
        window = max(10, int(len(u_sorted)*0.05))
        e_smooth = pd.Series(e_sorted).rolling(window).mean()
        plt.plot(u_sorted, e_smooth, "r-", linewidth=2.5, label='è¯¯å·®è¶‹åŠ¿ (Trend)')
        
        plt.title('é¢„æµ‹ä¸ç¡®å®šæ€§ä¸è¯¯å·®åˆ†æ', fontsize=14, fontweight='bold')
        plt.xlabel('è®¤çŸ¥ä¸ç¡®å®šæ€§ (Epistemic Uncertainty)', fontsize=12)
        plt.ylabel('ç»å¯¹é¢„æµ‹è¯¯å·® (ç§’)', fontsize=12)
        plt.legend(loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout() # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€ï¼Œé˜²æ­¢æ–‡å­—è¢«æˆªæ–­
        plt.savefig(CONFIG["plot_uncertainty"], dpi=300, bbox_inches='tight')

    def plot_rejection_curve(self, y_true, y_pred, uncertainty):
        print("\nğŸ“Š ç”Ÿæˆå›¾ 4.2: æ‹’ç»æˆªæ–­æ›²çº¿...")
        
        data = pd.DataFrame({'true': y_true, 'pred': y_pred, 'unc': uncertainty})
        base_rmse = np.sqrt(mean_squared_error(data['true'], data['pred']))
        print(f"  - èµ·ç‚¹ RMSE: {base_rmse:.4f}")
        
        percentages = np.arange(0, 90, 5)
        rmses_ours = []
        rmses_random = []
        
        data_sorted = data.sort_values('unc', ascending=False)
        for p in percentages:
            cutoff = int(len(data) * (p / 100))
            subset = data_sorted.iloc[cutoff:]
            if len(subset) > 0:
                rmse = np.sqrt(mean_squared_error(subset['true'], subset['pred']))
            else:
                rmse = 0
            rmses_ours.append(rmse)
            
        for p in percentages:
            cutoff = int(len(data) * (p / 100))
            remain_count = len(data) - cutoff
            
            if remain_count > 0:
                temp_scores = []
                for _ in range(20): 
                    subset = data.sample(n=remain_count)
                    temp_scores.append(np.sqrt(mean_squared_error(subset['true'], subset['pred'])))
                rmses_random.append(np.mean(temp_scores))
            else:
                rmses_random.append(0)

        plt.figure(figsize=(10, 7))
        plt.plot(percentages, rmses_ours, 'o-', linewidth=3, color='#2ca02c', label='æœ¬æ–¹æ³• (åŸºäºä¸ç¡®å®šæ€§æ‹’ç»)')
        plt.plot(percentages, rmses_random, 's--', linewidth=2, color='gray', alpha=0.7, label='éšæœºåŸºå‡† (Random Baseline)')
        
        plt.title('ä¸ç¡®å®šæ€§æ‹’ç»æ›²çº¿ (Rejection Curve)', fontsize=16, fontweight='bold')
        plt.xlabel('æ‹’ç»ç‡ (Rejection Rate %)', fontsize=14)
        plt.ylabel('å‡æ–¹æ ¹è¯¯å·® RMSE (ç§’)', fontsize=14)
        plt.legend(fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(CONFIG["plot_rejection"], dpi=300)
        print(f"âœ… ä¿å­˜è‡³ {CONFIG['plot_rejection']}")

    def plot_ood_detection(self, model, X_test, i_dim):
        print("\nğŸ“Š ç”Ÿæˆå›¾ 4.3: OOD æ£€æµ‹èƒ½åŠ›...")
        cx, ix, ax = X_test
        _, unc_in = self.get_predictions(model, cx, ix, ax)
        
        cx_ood = cx.copy()
        cx_ood[:, 0] = cx_ood[:, 0] - 5.0
        cx_ood[:, 2] = cx_ood[:, 2] + 5.0
        
        _, unc_ood = self.get_predictions(model, cx_ood, ix, ax)
        
        plt.figure(figsize=(8, 6))
        sns.kdeplot(unc_in, fill=True, color='green', label='æ­£å¸¸æµ‹è¯•æ•°æ® (In-Distribution)')
        sns.kdeplot(unc_ood, fill=True, color='red', label='æ¨¡æ‹Ÿå¼‚å¸¸æ•°æ® (OOD)')
        
        plt.title('å¼‚å¸¸ç¯å¢ƒæ£€æµ‹èƒ½åŠ› (OOD Detection)', fontsize=14, fontweight='bold')
        plt.xlabel('ä¸ç¡®å®šæ€§åˆ†æ•° (Uncertainty Score)', fontsize=12)
        plt.ylabel('æ¦‚ç‡å¯†åº¦ (Density)', fontsize=12) # è™½ç„¶æ˜¯å¯†åº¦ï¼Œä½†ä¸­æ–‡è¯­å¢ƒä¸‹è¿™ä¹ˆå†™æ›´é€šé¡º
        plt.yticks([])
        plt.legend(loc='upper right')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(CONFIG["plot_ood"], dpi=300, bbox_inches='tight')

# ==============================================================================
# 4. ä¸»ç¨‹åº
# ==============================================================================
if __name__ == "__main__":
    evaluator = UncertaintyEvaluator()
    Xc_train, Xc_test, Xi_train, Xi_test, Xa_train, Xa_test, y_train, y_test = evaluator.load_data()
    
    c_dim = Xc_train.shape[1]
    i_dim = Xi_train.shape[1]
    n_algos = len(evaluator.enc_algo.classes_)
    
    model = evaluator.load_model(c_dim, i_dim, n_algos)
    
    y_test_orig = np.expm1(y_test)
    pred_time, uncertainty = evaluator.get_predictions(model, Xc_test, Xi_test, Xa_test)
    
    current_base_rmse = np.sqrt(mean_squared_error(y_test_orig, pred_time))
    print(f"å½“å‰æ¨¡å‹å…¨é‡ RMSE: {current_base_rmse:.4f}")
    
    evaluator.plot_error_correlation(y_test_orig, pred_time, uncertainty)
    evaluator.plot_rejection_curve(y_test_orig, pred_time, uncertainty)
    evaluator.plot_ood_detection(model, (Xc_test, Xi_test, Xa_test), i_dim)
    
    print("\nâœ… æ‰€æœ‰ä¼˜è¶Šæ€§éªŒè¯å›¾è¡¨å·²ç”Ÿæˆï¼è¯·æŸ¥çœ‹ figure_4_*.png")