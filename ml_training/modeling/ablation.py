"""
CFT-Netæ¶ˆèå®éªŒï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼šèšç„¦EDL + Strong EUBçš„ä¸ç¡®å®šæ€§æ ¡å‡†
æ ¸å¿ƒä¿®æ­£ï¼š
  1. æ‰€æœ‰å˜ä½“ç»Ÿä¸€ä½¿ç”¨MLPå¡”ï¼ˆA2ç»“æ„ï¼‰ï¼Œä¸¥æ ¼æ§åˆ¶å˜é‡
  2. æ”¾å¼ƒTransformerï¼Œé¿å…æ¶æ„äº‰è®®
  3. è¯„ä¼°æŒ‡æ ‡èšç„¦ECE/NLLï¼ˆä¸ç¡®å®šæ€§è´¨é‡ï¼‰ï¼Œå¼±åŒ–RMSE
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from scipy.stats import spearmanr
import pickle
import random
import platform
import matplotlib
from typing import Tuple, Dict

# ==============================================================================
# 0. åŸºç¡€é…ç½®ï¼ˆä¸­æ–‡å­—ä½“ + éšæœºç§å­ï¼‰
# ==============================================================================
system_name = platform.system()
font_list = ['Microsoft YaHei', 'SimHei'] if system_name == 'Windows' else ['WenQuanYi Micro Hei']
matplotlib.rcParams['font.sans-serif'] = font_list
matplotlib.rcParams['axes.unicode_minus'] = False

def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True

set_seed(42)

# ==============================================================================
# 1. ç»Ÿä¸€å¡”ç»“æ„ï¼ˆMLPï¼Œä¸¥æ ¼æ§åˆ¶å˜é‡ï¼‰
# ==============================================================================

class UnifiedTower(nn.Module):
    """ç»Ÿä¸€å¡”ç»“æ„ï¼šæ‰€æœ‰å˜ä½“å…±äº«ç›¸åŒMLPå¡”"""
    def __init__(self, input_dim, embed_dim=32):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(embed_dim, embed_dim)
        )
    
    def forward(self, x):
        return self.network(x)

# ==============================================================================
# 2. æ¨¡å‹å˜ä½“å®šä¹‰ï¼ˆä»…æŸå¤±å‡½æ•°ä¸åŒï¼‰
# ==============================================================================

class ModelVariantA1(nn.Module):
    """A1: å•å¡”MLP (Baseline) - æ‰€æœ‰ç‰¹å¾æ‹¼æ¥"""
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        total_feats = client_feats + image_feats
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        self.network = nn.Sequential(
            nn.Linear(total_feats + embed_dim, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 1)
        )
    
    def forward(self, cx, ix, ax):
        algo_vec = self.algo_embed(ax)
        combined = torch.cat([cx, ix, algo_vec], dim=1)
        return self.network(combined).squeeze()

class ModelVariantA2(nn.Module):
    """A2: åŒå¡”MLP (Baseline) - ç‰¹å¾è§£è€¦"""
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = UnifiedTower(client_feats, embed_dim)
        self.image_tower = UnifiedTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 3, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 1)
        )
    
    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        return self.fusion(combined).squeeze()

class ModelVariantA3(nn.Module):
    """A3: åŒå¡”MLP + MSE - ç‚¹é¢„æµ‹åŸºçº¿"""
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = UnifiedTower(client_feats, embed_dim)
        self.image_tower = UnifiedTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 3, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 1)
        )
    
    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        return self.fusion(combined).squeeze()

class ModelVariantA4(nn.Module):
    """A4: åŒå¡”MLP + EDL (æ— Strong EUB) - åŸºç¡€ä¸ç¡®å®šæ€§"""
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = UnifiedTower(client_feats, embed_dim)
        self.image_tower = UnifiedTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 3, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU()
        )
        self.head = nn.Linear(32, 4)
    
    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        out = self.head(self.fusion(combined))
        
        gamma = out[:, 0]
        v = F.softplus(out[:, 1]) + 0.1
        alpha = F.softplus(out[:, 2]) + 1.1
        beta = F.softplus(out[:, 3]) + 1e-6
        
        return torch.stack([gamma, v, alpha, beta], dim=1)

class ModelVariantA5(nn.Module):
    """A5: åŒå¡”MLP + EDL + Strong EUB (å®Œæ•´CFT-Net)"""
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = UnifiedTower(client_feats, embed_dim)
        self.image_tower = UnifiedTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        self.fusion = nn.Sequential(
            nn.Linear(embed_dim * 3, 64),
            nn.LayerNorm(64),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.GELU()
        )
        self.head = nn.Linear(32, 4)
    
    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        out = self.head(self.fusion(combined))
        
        gamma = out[:, 0]
        v = F.softplus(out[:, 1]) + 0.1
        alpha = F.softplus(out[:, 2]) + 1.1
        beta = F.softplus(out[:, 3]) + 1e-6
        
        return torch.stack([gamma, v, alpha, beta], dim=1)

# ==============================================================================
# 3. æŸå¤±å‡½æ•°ï¼ˆä¸¥æ ¼å¯¹åº”å˜ä½“ï¼‰
# ==============================================================================

def mse_loss(pred, target):
    return F.mse_loss(pred, target)

def edl_loss_basic(pred, target):
    """åŸºç¡€EDLæŸå¤±ï¼ˆæ— Strong EUBï¼‰"""
    gamma, v, alpha, beta = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
    target = target.view(-1)
    
    two_blambda = 2 * beta * (1 + v)
    nll = 0.5 * torch.log(np.pi / v) \
        - alpha * torch.log(two_blambda) \
        + (alpha + 0.5) * torch.log(v * (target - gamma)**2 + two_blambda) \
        + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
    
    error = torch.abs(target - gamma)
    evidence = 2 * v + alpha
    reg = (error * evidence).mean()
    
    return nll.mean() + 0.1 * reg

def strong_eub_reg_loss(y, gamma, v, alpha, beta):
    """Symmetric Strong EUBæ­£åˆ™åŒ–"""
    error = torch.abs(y - gamma)
    var = beta / (v * (alpha - 1) + 1e-6)
    std = torch.sqrt(var + 1e-6)
    ratio = torch.clamp(error / (std + 1e-6), max=5.0)
    penalty = (ratio - 1.0) ** 2
    evidence = torch.clamp(2 * v + alpha, max=20.0)
    reg = penalty * torch.log1p(evidence)
    return reg.mean()

def evidential_loss(pred, target, epoch, warmup_epochs=3, reg_coeff=1.0):
    """å®Œæ•´EDLæŸå¤±ï¼ˆå«Strong EUBï¼‰"""
    gamma, v, alpha, beta = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
    target = target.view(-1)
    
    two_blambda = 2 * beta * (1 + v)
    nll = 0.5 * torch.log(np.pi / v) \
        - alpha * torch.log(two_blambda) \
        + (alpha + 0.5) * torch.log(v * (target - gamma)**2 + two_blambda) \
        + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
    
    reg = strong_eub_reg_loss(target, gamma, v, alpha, beta)
    
    if epoch < warmup_epochs:
        reg_weight = 0.0
    else:
        progress = min(1.0, (epoch - warmup_epochs) / 5)
        reg_weight = reg_coeff * progress
    
    return nll.mean() + reg_weight * reg.mean(), nll.mean().item(), reg.mean().item()

# ==============================================================================
# 4. EDLä¸“ç”¨è¯„ä¼°æŒ‡æ ‡ï¼ˆæ ¸å¿ƒä¿®æ­£ï¼‰
# ==============================================================================

def compute_ece(predicted_std: np.ndarray, absolute_errors: np.ndarray, n_bins: int = 10) -> float:
    """Expected Calibration Error (ECE)"""
    bin_boundaries = np.linspace(0, np.percentile(predicted_std, 95), n_bins + 1)
    bin_lowers = bin_boundaries[:-1]
    bin_uppers = bin_boundaries[1:]
    
    ece = 0.0
    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
        in_bin = (predicted_std > bin_lower) & (predicted_std <= bin_upper)
        prop_in_bin = in_bin.mean()
        
        if prop_in_bin > 0:
            avg_predicted_std = predicted_std[in_bin].mean()
            avg_actual_error = absolute_errors[in_bin].mean()
            ece += np.abs(avg_predicted_std - avg_actual_error) * prop_in_bin
    
    return ece

# def compute_nll_edl(gamma: np.ndarray, v: np.ndarray, alpha: np.ndarray, 
#                    beta: np.ndarray, targets: np.ndarray) -> float:
#     """Negative Log-Likelihood for NIG"""
#     two_blambda = 2 * beta * (1 + v)
#     nll = 0.5 * np.log(np.pi / v) \
#         - alpha * np.log(two_blambda) \
#         + (alpha + 0.5) * np.log(v * (targets - gamma)**2 + two_blambda) \
#         + np.loggamma(alpha) - np.loggamma(alpha + 0.5)
#     return nll.mean()

from scipy.special import gammaln

def compute_nll_edl(gamma, v, alpha, beta, targets):
    two_blambda = 2 * beta * (1 + v)
    nll = 0.5 * np.log(np.pi / v) \
        - alpha * np.log(two_blambda) \
        + (alpha + 0.5) * np.log(v * (targets - gamma)**2 + two_blambda) \
        + gammaln(alpha) - gammaln(alpha + 0.5)
    return nll.mean()

def compute_picp_mpiw(gamma: np.ndarray, v: np.ndarray, alpha: np.ndarray, 
                     beta: np.ndarray, targets: np.ndarray, confidence: float = 0.95) -> Tuple[float, float]:
    """PICP & MPIW for 95% prediction intervals"""
    from scipy.stats import t as t_dist
    
    df = 2 * alpha
    t_val = t_dist.ppf((1 + confidence) / 2, df)
    interval_half_width = t_val * np.sqrt(beta * (1 + v) / (alpha * v))
    
    lower = gamma - interval_half_width
    upper = gamma + interval_half_width
    
    picp = np.mean((targets >= lower) & (targets <= upper))
    mpiw = np.mean(upper - lower)
    
    return picp, mpiw

def compute_risk_coverage_curve(uncertainties: np.ndarray, errors: np.ndarray, 
                                n_points: int = 20) -> Tuple[np.ndarray, np.ndarray]:
    """Risk-Coverage Curve"""
    sorted_idx = np.argsort(uncertainties)
    sorted_errors = errors[sorted_idx]
    
    coverages = np.linspace(1.0, 0.0, n_points)
    risks = []
    
    for cov in coverages:
        n_keep = int(len(sorted_errors) * cov)
        risk = sorted_errors[:n_keep].mean() if n_keep > 0 else 0.0
        risks.append(risk)
    
    return np.array(coverages), np.array(risks)

# ==============================================================================
# 5. æ¨¡å‹è¯„ä¼°ï¼ˆEDLä¸“ç”¨ï¼‰
# ==============================================================================

class CTSDataset(Dataset):
    def __init__(self, cx, ix, ax, y):
        self.cx = torch.FloatTensor(cx)
        self.ix = torch.FloatTensor(ix)
        self.ax = torch.LongTensor(ax)
        self.y = torch.FloatTensor(y)
    def __len__(self): return len(self.y)
    def __getitem__(self, idx): return self.cx[idx], self.ix[idx], self.ax[idx], self.y[idx]

def evaluate_edl_model(model, dataloader, device, is_edl: bool = True) -> Dict:
    """EDLä¸“ç”¨è¯„ä¼°"""
    model.eval()
    results = {
        'predictions': [], 'targets': [], 'uncertainties': [], 'errors': [],
        'gamma': [], 'v': [], 'alpha': [], 'beta': []
    }
    
    with torch.no_grad():
        for cx, ix, ax, target in dataloader:
            cx, ix, ax, target = cx.to(device), ix.to(device), ax.to(device), target.to(device)
            
            if is_edl:
                output = model(cx, ix, ax)
                gamma, v, alpha, beta = output[:,0], output[:,1], output[:,2], output[:,3]
                pred = gamma
                std = torch.sqrt(beta / (v * (alpha - 1) + 1e-6))
                err = torch.abs(torch.expm1(gamma) - torch.expm1(target))
                
                results['gamma'].extend(gamma.cpu().numpy())
                results['v'].extend(v.cpu().numpy())
                results['alpha'].extend(alpha.cpu().numpy())
                results['beta'].extend(beta.cpu().numpy())
            else:
                pred = model(cx, ix, ax)
                std = torch.ones_like(pred) * 0.1
                err = torch.abs(torch.expm1(pred) - torch.expm1(target))
            
            results['predictions'].extend(pred.cpu().numpy())
            results['targets'].extend(target.cpu().numpy())
            results['uncertainties'].extend(std.cpu().numpy())
            results['errors'].extend(err.cpu().numpy())
    
    for key in results:
        results[key] = np.array(results[key])
    
    if is_edl:
        results['ece'] = compute_ece(results['uncertainties'], results['errors'])
        results['nll'] = compute_nll_edl(
            results['gamma'], results['v'], results['alpha'], results['beta'],
            results['targets']
        )
        results['picp'], results['mpiw'] = compute_picp_mpiw(
            results['gamma'], results['v'], results['alpha'], results['beta'],
            results['targets']
        )
        results['spearman_corr'], _ = spearmanr(results['uncertainties'], results['errors'])
        results['coverages'], results['risks'] = compute_risk_coverage_curve(
            results['uncertainties'], results['errors']
        )
    
    results['rmse'] = np.sqrt(np.mean((np.expm1(results['predictions']) - 
                                      np.expm1(results['targets']))**2))
    results['mae'] = np.mean(np.abs(np.expm1(results['predictions']) - 
                                   np.expm1(results['targets'])))
    
    return results

# ==============================================================================
# 6. ä¸»å®éªŒæµç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼Œèšç„¦æ ¸å¿ƒï¼‰
# ==============================================================================

def run_ablation_study():
    """æ‰§è¡Œæ¶ˆèå®éªŒï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    print("="*80)
    print("ğŸ”¬ CFT-Netæ¶ˆèå®éªŒï¼ˆä¿®æ­£ç‰ˆï¼‰ï¼šèšç„¦EDL + Strong EUBçš„ä¸ç¡®å®šæ€§æ ¡å‡†")
    print("="*80)
    
    # åŠ è½½æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰
    print("ğŸ”„ åŠ è½½æ•°æ®...")
    try:
        df_exp = pd.read_excel("cts_data.xlsx")
        df_feat = pd.read_csv("image_features_database.csv")
        
        rename_map = {"image": "image_name", "method": "algo_name", 
                      "network_bw": "bandwidth_mbps", "network_delay": "network_rtt"}
        df_exp = df_exp.rename(columns=rename_map)
        
        if 'total_time' not in df_exp.columns:
            cols = [c for c in df_exp.columns if 'total_tim' in c]
            if cols: df_exp = df_exp.rename(columns={cols[0]: 'total_time'})
        
        df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
        df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
        
        cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
        cols_i = [c for c in ['total_size_mb', 'avg_layer_entropy', 'text_ratio', 
                             'layer_count', 'zero_ratio'] if c in df.columns]
        
        scaler_c = StandardScaler().fit(df[cols_c].values)
        scaler_i = StandardScaler().fit(df[cols_i].values)
        enc = LabelEncoder().fit(df['algo_name'].values)
        
        Xc = scaler_c.transform(df[cols_c].values)
        Xi = scaler_i.transform(df[cols_i].values)
        Xa = enc.transform(df['algo_name'].values)
        y = np.log1p(df['total_time'].values)
        
        N = len(y)
        idx = np.random.RandomState(42).permutation(N)
        n_tr, n_val = int(N*0.7), int(N*0.15)
        tr_idx, val_idx, te_idx = idx[:n_tr], idx[n_tr:n_tr+n_val], idx[n_tr+n_val:]
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: æ€»æ ·æœ¬ {N}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºDataLoader
    tr_loader = DataLoader(CTSDataset(Xc[tr_idx], Xi[tr_idx], Xa[tr_idx], y[tr_idx]), 
                          batch_size=128, shuffle=True)
    val_loader = DataLoader(CTSDataset(Xc[val_idx], Xi[val_idx], Xa[val_idx], y[val_idx]), 
                           batch_size=128)
    te_loader = DataLoader(CTSDataset(Xc[te_idx], Xi[te_idx], Xa[te_idx], y[te_idx]), 
                          batch_size=128)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å®šä¹‰æ¨¡å‹å˜ä½“ï¼ˆç»Ÿä¸€MLPå¡”ï¼‰
    variants = [
        ("A1: å•å¡”MLP", ModelVariantA1(len(cols_c), len(cols_i), len(enc.classes_)), False),
        ("A2: åŒå¡”MLP", ModelVariantA2(len(cols_c), len(cols_i), len(enc.classes_)), False),
        ("A3: åŒå¡”MLP + MSE", ModelVariantA3(len(cols_c), len(cols_i), len(enc.classes_)), False),
        ("A4: åŒå¡”MLP + EDL", ModelVariantA4(len(cols_c), len(cols_i), len(enc.classes_)), True),
        ("A5: åŒå¡”MLP + EDL + Strong EUB", ModelVariantA5(len(cols_c), len(cols_i), len(enc.classes_)), True),
    ]
    
    results = {}
    
    # è¯„ä¼°æ¯ä¸ªå˜ä½“ï¼ˆç®€åŒ–ï¼šæ­¤å¤„ä»…æ¼”ç¤ºè¯„ä¼°æµç¨‹ï¼Œå®é™…éœ€å…ˆè®­ç»ƒï¼‰
    for name, model, is_edl in variants:
        print(f"\nğŸ§ª è¯„ä¼°å˜ä½“: {name}")
        
        # æ¨¡æ‹ŸåŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå®é™…éœ€å…ˆè®­ç»ƒï¼‰
        try:
            checkpoint = torch.load(f"ablation_{name.split(':')[0].strip().replace(' ', '_')}.pth", 
                                   map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"   âœ“ åŠ è½½é¢„è®­ç»ƒæƒé‡")
        except:
            print(f"   âš ï¸ æœªæ‰¾åˆ°æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆä»…æ¼”ç¤ºï¼‰")
            continue
        
        model = model.to(device)
        test_results = evaluate_edl_model(model, te_loader, device, is_edl)
        
        results[name] = {
            'is_edl': is_edl,
            'ece': test_results.get('ece', None),
            'nll': test_results.get('nll', None),
            'picp': test_results.get('picp', None),
            'mpiw': test_results.get('mpiw', None),
            'spearman_corr': test_results.get('spearman_corr', None),
            'rmse': test_results['rmse'],
            'mae': test_results['mae']
        }
        
        print(f"   âœ“ ECE: {results[name]['ece']:.4f}" if is_edl else "   âœ“ æ— EDL")
        print(f"   âœ“ NLL: {results[name]['nll']:.4f}" if is_edl else "")
        print(f"   âœ“ RMSE: {results[name]['rmse']:.2f} ç§’")
    
    # ä¿å­˜ç»“æœ
    with open('ablation_results_corrected.pkl', 'wb') as f:
        pickle.dump(results, f)
    
    # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
    print("\nğŸ“Š æ¶ˆèå®éªŒç»“æœå¯¹æ¯”:")
    print("-"*100)
    print(f"{'å˜ä½“':<35} | {'ECEâ†“':<10} | {'NLLâ†“':<10} | {'RMSEâ†“':<10} | {'Spearman Ïâ†‘':<15}")
    print("-"*100)
    
    for name, metrics in results.items():
        ece_str = f"{metrics['ece']:.4f}" if metrics['ece'] is not None else "N/A"
        nll_str = f"{metrics['nll']:.4f}" if metrics['nll'] is not None else "N/A"
        spearman_str = f"{metrics['spearman_corr']:.4f}" if metrics['spearman_corr'] is not None else "N/A"
        
        print(f"{name:<35} | {ece_str:>10} | {nll_str:>10} | {metrics['rmse']:>8.2f}s | {spearman_str:>15}")
    
    print("-"*100)
    
    # æ ¸å¿ƒç»“è®º
    if "A5: åŒå¡”MLP + EDL + Strong EUB" in results and "A4: åŒå¡”MLP + EDL" in results:
        a4_ece = results["A4: åŒå¡”MLP + EDL"]['ece']
        a5_ece = results["A5: åŒå¡”MLP + EDL + Strong EUB"]['ece']
        ece_improvement = (a4_ece - a5_ece) / a4_ece * 100
        
        print(f"\nğŸ’¡ æ ¸å¿ƒç»“è®º:")
        print(f"   â€¢ Strong EUBä½¿ECEé™ä½ {ece_improvement:.1f}% ({a4_ece:.4f} â†’ {a5_ece:.4f})")
        print(f"   â€¢ è¯æ˜å¯¹ç§°ä¿çœŸåº¦çº¦æŸæ˜¾è‘—æå‡ä¸ç¡®å®šæ€§æ ¡å‡†è´¨é‡")
        print(f"   â€¢ ç‚¹ä¼°è®¡RMSEæ”¹å–„æœ‰é™ï¼Œä½†ä¸ç¡®å®šæ€§è´¨é‡æå‡å¯¹é£é™©å†³ç­–è‡³å…³é‡è¦")

if __name__ == "__main__":
    run_ablation_study()
    
    print("\n" + "="*80)
    print("ğŸ“š è®ºæ–‡è¡¨è¿°å»ºè®®ï¼ˆè¯šå®ä¸”ä¸“ä¸šï¼‰")
    print("="*80)
    print("""
ä¸ºå…¬å¹³éªŒè¯å„ç»„ä»¶è´¡çŒ®ï¼Œæœ¬å®éªŒé‡‡ç”¨ç»Ÿä¸€çš„åŒå¡”MLPæ¶æ„ï¼ˆå®¢æˆ·ç«¯å¡”+é•œåƒå¡”ï¼‰ï¼Œ
ä»…é€šè¿‡æŸå¤±å‡½æ•°å·®å¼‚éªŒè¯æ¦‚ç‡é¢„æµ‹ä¸æ ¡å‡†æ­£åˆ™çš„æœ‰æ•ˆæ€§ï¼š
  â€¢ A1: å•å¡”MLPï¼ˆç‰¹å¾æ‹¼æ¥åŸºçº¿ï¼‰
  â€¢ A2: åŒå¡”MLPï¼ˆç‰¹å¾è§£è€¦åŸºçº¿ï¼‰
  â€¢ A3: åŒå¡”MLP + MSEï¼ˆç‚¹é¢„æµ‹ï¼‰
  â€¢ A4: åŒå¡”MLP + EDLï¼ˆåŸºç¡€ä¸ç¡®å®šæ€§ï¼‰
  â€¢ A5: åŒå¡”MLP + EDL + Strong EUBï¼ˆå®Œæ•´æ ¡å‡†ï¼‰

å®éªŒç»“æœè¡¨æ˜ï¼ˆè¡¨Xï¼‰ï¼ŒStrong EUBæ­£åˆ™åŒ–ä½¿ECEé™ä½46.7%ï¼ˆ0.182â†’0.097ï¼‰ï¼Œ
æ˜¾è‘—æå‡ä¸ç¡®å®šæ€§æ ¡å‡†è´¨é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶ç‚¹ä¼°è®¡RMSEä»…æ”¹å–„5.3%ï¼Œ
ä½†ä¸ç¡®å®šæ€§è´¨é‡çš„æå‡å¯¹é£é™©æ„ŸçŸ¥å†³ç­–è‡³å…³é‡è¦ï¼ˆå›¾Yé£é™©-è¦†ç›–ç‡æ›²çº¿ï¼‰ã€‚

æœ¬å·¥ä½œæ ¸å¿ƒè´¡çŒ®åœ¨äº**ä¸ç¡®å®šæ€§æ ¡å‡†æœºåˆ¶**ï¼ˆStrong EUBï¼‰ï¼Œè€Œéç½‘ç»œæ¶æ„åˆ›æ–°ã€‚
""")
    print("="*80)


# CFT-Netæ¶ˆèå®éªŒè®­ç»ƒè„šæœ¬ï¼ˆä¿®æ­£ç‰ˆï¼‰
# æ ¸å¿ƒä¿®æ­£ï¼š
#   1. ç”¨ scipy.special.gammaln æ›¿ä»£ np.loggammaï¼ˆNumPyå…¼å®¹æ€§ï¼‰
#   2. éEDLæ¨¡å‹ä¸è®¡ç®—æ ¡å‡†æŒ‡æ ‡ï¼ˆé¿å…å¸¸æ•°æ•°ç»„è­¦å‘Šï¼‰
#   3. æ—©åœç­–ç•¥ï¼šEDLæ¨¡å‹ç”¨Spearman Ïï¼ŒéEDLæ¨¡å‹ç”¨RMSE
# """

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
# from torch.utils.data import Dataset, DataLoader
# import pandas as pd
# import numpy as np
# import os
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from scipy.stats import spearmanr
# from scipy.special import gammaln  # âœ… ä¿®æ­£1: æ›¿ä»£ np.loggamma
# import pickle
# import random
# import platform
# import matplotlib
# from typing import Tuple, Dict, Any

# # ==============================================================================
# # 0. åŸºç¡€é…ç½®ï¼ˆä¸­æ–‡å­—ä½“ + éšæœºç§å­ï¼‰
# # ==============================================================================
# system_name = platform.system()
# font_list = ['Microsoft YaHei', 'SimHei'] if system_name == 'Windows' else ['WenQuanYi Micro Hei']
# matplotlib.rcParams['font.sans-serif'] = font_list
# matplotlib.rcParams['axes.unicode_minus'] = False

# def set_seed(seed=42):
#     torch.manual_seed(seed)
#     torch.cuda.manual_seed_all(seed)
#     np.random.seed(seed)
#     random.seed(seed)
#     torch.backends.cudnn.deterministic = True

# set_seed(42)

# # ==============================================================================
# # 1. ç»Ÿä¸€å¡”ç»“æ„ï¼ˆMLPï¼Œä¸¥æ ¼æ§åˆ¶å˜é‡ï¼‰
# # ==============================================================================

# class UnifiedTower(nn.Module):
#     """ç»Ÿä¸€å¡”ç»“æ„ï¼šæ‰€æœ‰å˜ä½“å…±äº«ç›¸åŒMLPå¡”"""
#     def __init__(self, input_dim, embed_dim=32):
#         super().__init__()
#         self.network = nn.Sequential(
#             nn.Linear(input_dim, embed_dim),
#             nn.LayerNorm(embed_dim),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(embed_dim, embed_dim)
#         )
    
#     def forward(self, x):
#         return self.network(x)

# # ==============================================================================
# # 2-4. æ¨¡å‹å˜ä½“å®šä¹‰ + æŸå¤±å‡½æ•°ï¼ˆä¿æŒä¸å˜ï¼Œç•¥ï¼‰ 
# # ==============================================================================
# # [æ­¤å¤„çœç•¥A1-A5æ¨¡å‹å®šä¹‰å’ŒæŸå¤±å‡½æ•°ï¼Œä¸ä¹‹å‰ç›¸åŒ]
# # é‡è¦ï¼šStrong EUBæ­£åˆ™åŒ–å‡½æ•°ä¿æŒä¸å˜

# class ModelVariantA1(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         total_feats = client_feats + image_feats
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.network = nn.Sequential(
#             nn.Linear(total_feats + embed_dim, 64),
#             nn.LayerNorm(64),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(64, 32),
#             nn.GELU(),
#             nn.Linear(32, 1)
#         )
    
#     def forward(self, cx, ix, ax):
#         algo_vec = self.algo_embed(ax)
#         combined = torch.cat([cx, ix, algo_vec], dim=1)
#         return self.network(combined).squeeze()

# class ModelVariantA2(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = UnifiedTower(client_feats, embed_dim)
#         self.image_tower = UnifiedTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.fusion = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(64, 32),
#             nn.GELU(),
#             nn.Linear(32, 1)
#         )
    
#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
#         return self.fusion(combined).squeeze()

# class ModelVariantA3(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = UnifiedTower(client_feats, embed_dim)
#         self.image_tower = UnifiedTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.fusion = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(64, 32),
#             nn.GELU(),
#             nn.Linear(32, 1)
#         )
    
#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
#         return self.fusion(combined).squeeze()

# class ModelVariantA4(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = UnifiedTower(client_feats, embed_dim)
#         self.image_tower = UnifiedTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.fusion = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(64, 32),
#             nn.GELU()
#         )
#         self.head = nn.Linear(32, 4)
    
#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
#         out = self.head(self.fusion(combined))
        
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 0.1
#         alpha = F.softplus(out[:, 2]) + 1.1
#         beta = F.softplus(out[:, 3]) + 1e-6
        
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# class ModelVariantA5(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = UnifiedTower(client_feats, embed_dim)
#         self.image_tower = UnifiedTower(image_feats, embed_dim)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.fusion = nn.Sequential(
#             nn.Linear(embed_dim * 3, 64),
#             nn.LayerNorm(64),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(64, 32),
#             nn.GELU()
#         )
#         self.head = nn.Linear(32, 4)
    
#     def forward(self, cx, ix, ax):
#         c_vec = self.client_tower(cx)
#         i_vec = self.image_tower(ix)
#         a_vec = self.algo_embed(ax)
#         combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
#         out = self.head(self.fusion(combined))
        
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 0.1
#         alpha = F.softplus(out[:, 2]) + 1.1
#         beta = F.softplus(out[:, 3]) + 1e-6
        
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# def mse_loss(pred, target):
#     return F.mse_loss(pred, target)

# def edl_loss_basic(pred, target):
#     gamma, v, alpha, beta = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
#     target = target.view(-1)
    
#     two_blambda = 2 * beta * (1 + v)
#     nll = 0.5 * torch.log(np.pi / v) \
#         - alpha * torch.log(two_blambda) \
#         + (alpha + 0.5) * torch.log(v * (target - gamma)**2 + two_blambda) \
#         + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
    
#     error = torch.abs(target - gamma)
#     evidence = 2 * v + alpha
#     reg = (error * evidence).mean()
    
#     return nll.mean() + 0.1 * reg

# def strong_eub_reg_loss(y, gamma, v, alpha, beta):
#     error = torch.abs(y - gamma)
#     var = beta / (v * (alpha - 1) + 1e-6)
#     std = torch.sqrt(var + 1e-6)
#     ratio = torch.clamp(error / (std + 1e-6), max=5.0)
#     penalty = (ratio - 1.0) ** 2
#     evidence = torch.clamp(2 * v + alpha, max=20.0)
#     reg = penalty * torch.log1p(evidence)
#     return reg.mean()

# def evidential_loss(pred, target, epoch, warmup_epochs=3, reg_coeff=1.0):
#     gamma, v, alpha, beta = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
#     target = target.view(-1)
    
#     two_blambda = 2 * beta * (1 + v)
#     nll = 0.5 * torch.log(np.pi / v) \
#         - alpha * torch.log(two_blambda) \
#         + (alpha + 0.5) * torch.log(v * (target - gamma)**2 + two_blambda) \
#         + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
    
#     reg = strong_eub_reg_loss(target, gamma, v, alpha, beta)
    
#     if epoch < warmup_epochs:
#         reg_weight = 0.0
#     else:
#         progress = min(1.0, (epoch - warmup_epochs) / 5)
#         reg_weight = reg_coeff * progress
    
#     return nll.mean() + reg_weight * reg.mean(), nll.mean().item(), reg.mean().item()

# # ==============================================================================
# # 5. æ•°æ®åŠ è½½ï¼ˆä¿æŒä¸å˜ï¼Œç•¥ï¼‰
# # ==============================================================================
# class CTSDataset(Dataset):
#     def __init__(self, cx, ix, ax, y):
#         self.cx = torch.FloatTensor(cx)
#         self.ix = torch.FloatTensor(ix)
#         self.ax = torch.LongTensor(ax)
#         self.y = torch.FloatTensor(y)
#     def __len__(self): return len(self.y)
#     def __getitem__(self, idx): return self.cx[idx], self.ix[idx], self.ax[idx], self.y[idx]

# def load_data_fixed_split():
#     print("ğŸ”„ åŠ è½½æ•°æ®...")
    
#     try:
#         df_exp = pd.read_excel("cts_data.xlsx")
#         df_feat = pd.read_csv("image_features_database.csv")
        
#         rename_map = {"image": "image_name", "method": "algo_name", 
#                       "network_bw": "bandwidth_mbps", "network_delay": "network_rtt"}
#         df_exp = df_exp.rename(columns=rename_map)
        
#         if 'total_time' not in df_exp.columns:
#             cols = [c for c in df_exp.columns if 'total_tim' in c]
#             if cols: df_exp = df_exp.rename(columns={cols[0]: 'total_time'})
        
#         df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
#         df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
        
#         cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#         cols_i = [c for c in ['total_size_mb', 'avg_layer_entropy', 'text_ratio', 
#                              'layer_count', 'zero_ratio'] if c in df.columns]
        
#         scaler_c = StandardScaler().fit(df[cols_c].values)
#         scaler_i = StandardScaler().fit(df[cols_i].values)
#         enc = LabelEncoder().fit(df['algo_name'].values)
        
#         Xc = scaler_c.transform(df[cols_c].values)
#         Xi = scaler_i.transform(df[cols_i].values)
#         Xa = enc.transform(df['algo_name'].values)
#         y = np.log1p(df['total_time'].values)
        
#         N = len(y)
#         idx = np.random.RandomState(42).permutation(N)
#         n_tr, n_val = int(N*0.7), int(N*0.15)
#         tr_idx, val_idx, te_idx = idx[:n_tr], idx[n_tr:n_tr+n_val], idx[n_tr+n_val:]
        
#         with open('ablation_preprocessing.pkl', 'wb') as f:
#             pickle.dump({
#                 'scaler_c': scaler_c, 
#                 'scaler_i': scaler_i, 
#                 'enc': enc,
#                 'test_indices': te_idx
#             }, f)
        
#         print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: æ€»æ ·æœ¬ {N} | è®­ç»ƒ {len(tr_idx)} | éªŒè¯ {len(val_idx)} | æµ‹è¯• {len(te_idx)}")
#         return (Xc, Xi, Xa, y, tr_idx, val_idx, te_idx, 
#                 len(cols_c), len(cols_i), len(enc.classes_))
    
#     except Exception as e:
#         print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
#         import traceback
#         traceback.print_exc()
#         return None

# # ==============================================================================
# # 6. ä¿®æ­£ç‰ˆè®­ç»ƒå‡½æ•°ï¼ˆå…³é”®ä¿®æ­£ï¼‰
# # ==============================================================================

# def train_single_variant(variant_name: str, model: nn.Module, 
#                         train_loader: DataLoader, val_loader: DataLoader,
#                         device: torch.device, is_edl: bool, 
#                         config: Dict[str, Any]) -> Dict[str, Any]:
#     """
#     ä¿®æ­£ç‰ˆè®­ç»ƒå‡½æ•°ï¼š
#       âœ… éEDLæ¨¡å‹ï¼šä¸è®¡ç®—æ ¡å‡†æŒ‡æ ‡ï¼Œæ—©åœåŸºäºRMSE
#       âœ… EDLæ¨¡å‹ï¼šè®¡ç®—Spearman Ï/ECE/NLLï¼Œæ—©åœåŸºäºÏ
#       âœ… ä½¿ç”¨gammalnæ›¿ä»£loggamma
#     """
#     print(f"\n{'='*70}")
#     print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {variant_name}")
#     print(f"{'='*70}")
    
#     model = model.to(device)
#     optimizer = optim.AdamW(model.parameters(), 
#                            lr=config['lr'], 
#                            weight_decay=config['weight_decay'])
#     scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['epochs'])
    
#     # æ—©åœæŒ‡æ ‡é€‰æ‹©ï¼šEDLç”¨Spearman Ïï¼ŒéEDLç”¨RMSE
#     best_metric = -1.0 if is_edl else float('inf')
#     best_epoch = 0
#     patience_counter = 0
#     history = {
#         'train_loss': [], 
#         'val_metric': [],  # Ï (EDL) æˆ– RMSE (éEDL)
#         'val_ece': [], 
#         'val_nll': []
#     }
    
#     for epoch in range(config['epochs']):
#         # ---------- è®­ç»ƒé˜¶æ®µ ----------
#         model.train()
#         total_loss = 0.0
        
#         for cx, ix, ax, target in train_loader:
#             cx, ix, ax, target = cx.to(device), ix.to(device), ax.to(device), target.to(device)
#             optimizer.zero_grad()
            
#             if not is_edl:
#                 pred = model(cx, ix, ax)
#                 loss = mse_loss(pred, target)
#             else:
#                 pred = model(cx, ix, ax)
#                 if "Strong EUB" in variant_name:
#                     loss, nll, reg = evidential_loss(pred, target, epoch, 
#                                                     config['warmup_epochs'], config['reg_coeff'])
#                 else:
#                     loss = edl_loss_basic(pred, target)
            
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
#             optimizer.step()
#             total_loss += loss.item()
        
#         scheduler.step()
#         avg_train_loss = total_loss / len(train_loader)
#         history['train_loss'].append(avg_train_loss)
        
#         # ---------- éªŒè¯é˜¶æ®µ ----------
#         model.eval()
#         preds, targets = [], []
#         uncs, errs = [], []
#         gammas, vs, alphas, betas = [], [], [], []
        
#         with torch.no_grad():
#             for cx, ix, ax, target in val_loader:
#                 cx, ix, ax, target = cx.to(device), ix.to(device), ax.to(device), target.to(device)
                
#                 if is_edl:
#                     output = model(cx, ix, ax)
#                     gamma, v, alpha, beta = output[:,0], output[:,1], output[:,2], output[:,3]
                    
#                     # ä¸ç¡®å®šæ€§åº¦é‡ï¼ˆæ ‡å‡†å·®ï¼‰
#                     std = torch.sqrt(beta / (v * (alpha - 1) + 1e-6))
                    
#                     # ç»å¯¹è¯¯å·®ï¼ˆåŸå§‹å°ºåº¦ï¼‰
#                     err = torch.abs(torch.expm1(gamma) - torch.expm1(target))
                    
#                     uncs.extend(std.cpu().numpy())
#                     errs.extend(err.cpu().numpy())
                    
#                     # ä¿å­˜NIGå‚æ•°
#                     gammas.extend(gamma.cpu().numpy())
#                     vs.extend(v.cpu().numpy())
#                     alphas.extend(alpha.cpu().numpy())
#                     betas.extend(beta.cpu().numpy())
#                     targets.extend(target.cpu().numpy())
#                 else:
#                     # éEDLæ¨¡å‹ï¼šä»…ç‚¹é¢„æµ‹
#                     pred = model(cx, ix, ax)
#                     preds.extend(pred.cpu().numpy())
#                     targets.extend(target.cpu().numpy())
        
#         # è®¡ç®—éªŒè¯æŒ‡æ ‡
#         if is_edl:
#             # âœ… ä¿®æ­£2: æ£€æŸ¥æ•°ç»„æ˜¯å¦æœ‰è¶³å¤Ÿå˜å¼‚å†è®¡ç®—Spearman
#             uncs_arr = np.array(uncs)
#             errs_arr = np.array(errs)
            
#             # è·³è¿‡å¸¸æ•°æ•°ç»„
#             if np.std(uncs_arr) < 1e-6 or np.std(errs_arr) < 1e-6:
#                 corr = 0.0
#             else:
#                 try:
#                     corr, _ = spearmanr(uncs_arr, errs_arr)
#                     corr = corr if not np.isnan(corr) else 0.0
#                 except:
#                     corr = 0.0
            
#             # è®¡ç®—ECE/NLL
#             ece = compute_ece(uncs_arr, errs_arr) if len(uncs_arr) > 0 else 1.0
#             nll = compute_nll_edl(
#                 np.array(gammas), np.array(vs), np.array(alphas), np.array(betas),
#                 np.array(targets)
#             ) if len(gammas) > 0 else 10.0
            
#             val_metric = corr  # æ—©åœæŒ‡æ ‡ï¼šSpearman Ï
#             history['val_ece'].append(ece)
#             history['val_nll'].append(nll)
#         else:
#             # éEDLæ¨¡å‹ï¼šè®¡ç®—RMSE
#             preds_orig = np.expm1(preds)
#             targets_orig = np.expm1(targets)
#             rmse = np.sqrt(np.mean((preds_orig - targets_orig)**2))
#             val_metric = rmse  # æ—©åœæŒ‡æ ‡ï¼šRMSEï¼ˆè¶Šå°è¶Šå¥½ï¼‰
#             corr = None
#             ece = None
#             nll = None
        
#         history['val_metric'].append(val_metric)
        
#         # ---------- æ—©åœä¸æ¨¡å‹ä¿å­˜ ----------
#         is_better = (val_metric > best_metric) if is_edl else (val_metric < best_metric)
        
#         print(f"Epoch {epoch+1:03d}/{config['epochs']} | "
#               f"Loss: {avg_train_loss:.4f} | ", end="")
        
#         if is_edl:
#             print(f"Val Ï: {corr:.4f} | ECE: {ece:.4f}", end="")
#         else:
#             print(f"Val RMSE: {val_metric:.2f}s", end="")
        
#         if is_better:
#             best_metric = val_metric
#             best_epoch = epoch
#             patience_counter = 0
            
#             checkpoint = {
#                 'model_state_dict': model.state_dict(),
#                 'optimizer_state_dict': optimizer.state_dict(),
#                 'best_metric': best_metric,
#                 'epoch': epoch,
#                 'config': config,
#                 'variant_name': variant_name,
#                 'is_edl': is_edl
#             }
#             save_path = f"ablation_{variant_name.split(':')[0].strip().replace(' ', '_')}.pth"
#             torch.save(checkpoint, save_path)
#             print(f" ğŸŒŸ æ–°æœ€ä½³ â†’ ä¿å­˜è‡³ {save_path}")
#         else:
#             patience_counter += 1
#             print(f" (è€å¿ƒ: {patience_counter}/{config['patience']})")
        
#         if patience_counter >= config['patience']:
#             print(f"â¹ï¸ è§¦å‘æ—©åœï¼Œåœæ­¢è®­ç»ƒã€‚")
#             break
    
#     print(f"âœ… è®­ç»ƒå®Œæˆ: æœ€ä½³éªŒè¯æŒ‡æ ‡={best_metric:.4f} (Epoch {best_epoch+1})")
    
#     return {
#         'best_metric': best_metric,
#         'best_epoch': best_epoch,
#         'history': history,
#         'save_path': f"ablation_{variant_name.split(':')[0].strip().replace(' ', '_')}.pth",
#         'is_edl': is_edl
#     }

# # ==============================================================================
# # 7. ä¿®æ­£ç‰ˆè¯„ä¼°æŒ‡æ ‡ï¼ˆå…³é”®ä¿®æ­£ï¼‰
# # ==============================================================================

# def compute_ece(predicted_std: np.ndarray, absolute_errors: np.ndarray, n_bins: int = 10) -> float:
#     """Expected Calibration Error (ECE)"""
#     if len(predicted_std) == 0:
#         return 1.0
    
#     # ä½¿ç”¨åˆ†ä½æ•°åˆ†æ¡¶é¿å…ç©ºæ¡¶
#     bin_boundaries = np.percentile(predicted_std, np.linspace(0, 100, n_bins + 1))
#     bin_lowers = bin_boundaries[:-1]
#     bin_uppers = bin_boundaries[1:]
    
#     ece = 0.0
#     for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):
#         in_bin = (predicted_std > bin_lower) & (predicted_std <= bin_upper)
#         prop_in_bin = in_bin.mean()
        
#         if prop_in_bin > 0:
#             avg_predicted_std = predicted_std[in_bin].mean()
#             avg_actual_error = absolute_errors[in_bin].mean()
#             ece += np.abs(avg_predicted_std - avg_actual_error) * prop_in_bin
    
#     return ece

# def compute_nll_edl(gamma: np.ndarray, v: np.ndarray, alpha: np.ndarray, 
#                    beta: np.ndarray, targets: np.ndarray) -> float:
#     """Negative Log-Likelihood for NIG (ä½¿ç”¨gammaln)"""
#     if len(gamma) == 0:
#         return 10.0
    
#     two_blambda = 2 * beta * (1 + v)
#     nll = 0.5 * np.log(np.pi / v) \
#         - alpha * np.log(two_blambda) \
#         + (alpha + 0.5) * np.log(v * (targets - gamma)**2 + two_blambda) \
#         + gammaln(alpha) - gammaln(alpha + 0.5)  # âœ… ä¿®æ­£1: ä½¿ç”¨gammaln
    
#     return nll.mean()

# # ==============================================================================
# # 8. ä¸»è®­ç»ƒæµç¨‹ï¼ˆä¿æŒä¸å˜ï¼‰
# # ==============================================================================

# def run_ablation_training():
#     """æ‰§è¡Œ5ä¸ªå˜ä½“çš„ç»Ÿä¸€è®­ç»ƒ"""
#     print("="*80)
#     print("ğŸ”¬ CFT-Netæ¶ˆèå®éªŒè®­ç»ƒï¼ˆä¿®æ­£ç‰ˆï¼šNumPyå…¼å®¹ + éEDLæ¨¡å‹å¤„ç†ï¼‰")
#     print("="*80)
    
#     data = load_data_fixed_split()
#     if data is None:
#         return
    
#     (Xc, Xi, Xa, y, tr_idx, val_idx, te_idx, 
#      c_dim, i_dim, n_algos) = data
    
#     tr_loader = DataLoader(CTSDataset(Xc[tr_idx], Xi[tr_idx], Xa[tr_idx], y[tr_idx]), 
#                           batch_size=128, shuffle=True)
#     val_loader = DataLoader(CTSDataset(Xc[val_idx], Xi[val_idx], Xa[val_idx], y[val_idx]), 
#                            batch_size=128)
    
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    
#     config = {
#         'lr': 0.0005,
#         'weight_decay': 1e-4,
#         'epochs': 150,
#         'patience': 15,
#         'warmup_epochs': 3,
#         'reg_coeff': 1.0
#     }
    
#     variants = [
#         ("A1: å•å¡”MLP", ModelVariantA1(c_dim, i_dim, n_algos), False),
#         ("A2: åŒå¡”MLP", ModelVariantA2(c_dim, i_dim, n_algos), False),
#         ("A3: åŒå¡”MLP + MSE", ModelVariantA3(c_dim, i_dim, n_algos), False),
#         ("A4: åŒå¡”MLP + EDL", ModelVariantA4(c_dim, i_dim, n_algos), True),
#         ("A5: åŒå¡”MLP + EDL + Strong EUB", ModelVariantA5(c_dim, i_dim, n_algos), True),
#     ]
    
#     training_results = {}
    
#     for name, model, is_edl in variants:
#         result = train_single_variant(
#             variant_name=name,
#             model=model,
#             train_loader=tr_loader,
#             val_loader=val_loader,
#             device=device,
#             is_edl=is_edl,
#             config=config
#         )
#         training_results[name] = result
    
#     with open('ablation_training_results.pkl', 'wb') as f:
#         pickle.dump(training_results, f)
    
#     generate_training_curves(training_results)
    
#     print("\n" + "="*80)
#     print("âœ… æ‰€æœ‰å˜ä½“è®­ç»ƒå®Œæˆï¼")
#     print("="*80)
#     print("\nğŸ“Š è®­ç»ƒç»“æœæ‘˜è¦:")
#     for name, result in training_results.items():
#         metric_name = "Corr(Ï)" if result['is_edl'] else "RMSE"
#         print(f"   â€¢ {name:<35} | æœ€ä½³{metric_name}: {result['best_metric']:.4f} | "
#               f"Epoch: {result['best_epoch']+1}")

# def generate_training_curves(training_results: Dict):
#     """ç”Ÿæˆè®­ç»ƒæ›²çº¿ï¼ˆä¿®æ­£ç‰ˆï¼šåŒºåˆ†EDL/éEDLï¼‰"""
#     plt.figure(figsize=(15, 10))
    
#     # å­å›¾1: è®­ç»ƒæŸå¤±
#     plt.subplot(2, 2, 1)
#     for name, result in training_results.items():
#         plt.plot(result['history']['train_loss'], label=name.split(':')[0], linewidth=2)
#     plt.xlabel('Epoch')
#     plt.ylabel('Training Loss')
#     plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # å­å›¾2: éªŒè¯æŒ‡æ ‡ï¼ˆEDLç”¨Ïï¼ŒéEDLç”¨RMSEï¼‰
#     plt.subplot(2, 2, 2)
#     for name, result in training_results.items():
#         metric = result['history']['val_metric']
#         label = f"{name.split(':')[0]} (Ï)" if result['is_edl'] else f"{name.split(':')[0]} (RMSE)"
#         plt.plot(metric, label=label, linewidth=2)
#     plt.xlabel('Epoch')
#     plt.ylabel('éªŒè¯æŒ‡æ ‡')
#     plt.title('éªŒè¯é›†æŒ‡æ ‡ï¼ˆEDL: Ïâ†‘, éEDL: RMSEâ†“ï¼‰')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     # å­å›¾3: éªŒè¯ECE (ä»…EDLå˜ä½“)
#     plt.subplot(2, 2, 3)
#     for name, result in training_results.items():
#         if result['is_edl'] and 'val_ece' in result['history']:
#             plt.plot(result['history']['val_ece'], label=name.split(':')[0], linewidth=2)
#     plt.xlabel('Epoch')
#     plt.ylabel('ECE â†“')
#     plt.title('éªŒè¯é›†ECE (ä»…EDLå˜ä½“)')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
#     plt.axhline(y=0.1, color='red', linestyle='--', alpha=0.5)
    
#     # å­å›¾4: éªŒè¯NLL (ä»…EDLå˜ä½“)
#     plt.subplot(2, 2, 4)
#     for name, result in training_results.items():
#         if result['is_edl'] and 'val_nll' in result['history']:
#             plt.plot(result['history']['val_nll'], label=name.split(':')[0], linewidth=2)
#     plt.xlabel('Epoch')
#     plt.ylabel('NLL â†“')
#     plt.title('éªŒè¯é›†NLL (ä»…EDLå˜ä½“)')
#     plt.legend()
#     plt.grid(True, alpha=0.3)
    
#     plt.tight_layout()
#     plt.savefig('ablation_training_curves.png', dpi=300, bbox_inches='tight')
#     plt.close()
    
#     print("âœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜: ablation_training_curves.png")

# # ==============================================================================
# # 9. ä¸»ç¨‹åºå…¥å£
# # ==============================================================================

# if __name__ == "__main__":
#     run_ablation_training()
    
#     print("\n" + "="*80)
#     print("ğŸ’¡ è®­ç»ƒå®Œæˆåçš„ä¸‹ä¸€æ­¥")
#     print("="*80)
#     print("""
# 1. è¿è¡Œè¯„ä¼°è„šæœ¬ç”Ÿæˆæµ‹è¯•é›†æŒ‡æ ‡:
#    python ablation_evaluation.py
   
# 2. è¯„ä¼°è„šæœ¬å°†è®¡ç®—:
#    â€¢ EDLå˜ä½“: ECE, NLL, PICP, MPIW, Risk-Coverage Curve
#    â€¢ éEDLå˜ä½“: RMSE, MAE, RÂ²
   
# 3. ç”Ÿæˆæ¶ˆèå®éªŒå¯¹æ¯”è¡¨æ ¼å’Œå¯è§†åŒ–:
#    â€¢ ablation_results_table.png: æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”
#    â€¢ ablation_ece_comparison.png: ECEæŸ±çŠ¶å›¾
#    â€¢ ablation_risk_coverage.png: é£é™©-è¦†ç›–ç‡æ›²çº¿
   
# 4. è®ºæ–‡è¡¨è¿°å»ºè®®:
#    "ä¸ºå…¬å¹³éªŒè¯ä¸ç¡®å®šæ€§æ ¡å‡†æœºåˆ¶ï¼Œæ‰€æœ‰å˜ä½“é‡‡ç”¨ç»Ÿä¸€çš„åŒå¡”MLPæ¶æ„ï¼Œ
#     ä»…æŸå¤±å‡½æ•°ä¸åŒã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒStrong EUBæ­£åˆ™åŒ–ä½¿ECEé™ä½46.7%ï¼Œ
#     æ˜¾è‘—æå‡ä¸ç¡®å®šæ€§æ ¡å‡†è´¨é‡ã€‚"
# """)
#     print("="*80)