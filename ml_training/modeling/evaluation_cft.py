
# """
# CFT-Net å®Œæ•´å¯¹æ¯”è¯„æµ‹è„šæœ¬ï¼ˆä¿®å¤ç‰ˆ v2ï¼‰
# ç”Ÿæˆç”¨äºè®ºæ–‡çš„å¯¹æ¯”è¡¨æ ¼å’Œé›·è¾¾å›¾ï¼ˆç²¾åº¦ã€é£é™©æ„ŸçŸ¥ã€å¯é æ€§ã€è½»é‡åŒ–ï¼‰

# ä¿®å¤å†…å®¹ï¼š
# 1. ç®—æ³•ç‰¹å¾ä½¿ç”¨One-Hotç¼–ç ï¼Œé¿å…æ•°å€¼é¡ºåºè¯¯å¯¼
# 2. ç»Ÿä¸€æ¨ç†æ—¶é—´æµ‹é‡æ ‡å‡†ï¼ˆå…¨éƒ¨åœ¨CPUä¸Šæµ‹é‡ï¼‰
# 3. å¢åŠ ç‰©ç†ç¡®å®šæ€§çš„è®¨è®ºå’Œè¯´æ˜
# 4. å¢åŠ åˆ†å±‚æ ¡å‡†å’Œæ›´å®Œæ•´çš„è¯„ä¼°æŒ‡æ ‡
# 5. ã€æ–°å¢ã€‘æ·»åŠ  Pred vs Actual æ•£ç‚¹å›¾
# 6. ã€ä¿®å¤ã€‘prediction_intervals ä½¿ç”¨å…¨é‡æµ‹è¯•é›†ï¼ŒPICPä¸è¡¨æ ¼ä¸€è‡´
# """

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import numpy as np
# import pandas as pd
# import os
# import time
# import pickle
# import json
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# from sklearn.ensemble import RandomForestRegressor
# import xgboost as xgb
# import lightgbm as lgb
# from scipy.stats import spearmanr, norm, wilcoxon
# from scipy.optimize import brentq
# from collections import Counter
# import warnings
# import platform

# warnings.filterwarnings('ignore')

# # ==============================================================================
# # 0. åŸºç¡€é…ç½®
# # ==============================================================================
# system = platform.system()
# if system == 'Windows':
#     plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
# elif system == 'Darwin':
#     plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS']
# else:
#     plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']

# plt.rcParams['axes.unicode_minus'] = False

# SEED = 42
# np.random.seed(SEED)
# torch.manual_seed(SEED)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# os.makedirs("evaluation_results", exist_ok=True)

# # ==============================================================================
# # 1. æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
# # ==============================================================================
# class LightweightFeatureTokenizer(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.embeddings = nn.Parameter(torch.randn(num_features, embed_dim) * 0.02)
#         self.bias = nn.Parameter(torch.zeros(num_features, embed_dim))
#         self.norm = nn.LayerNorm(embed_dim)
#     def forward(self, x):
#         x = x.unsqueeze(-1)
#         out = x * self.embeddings + self.bias
#         return self.norm(out)

# class LightweightTransformerTower(nn.Module):
#     def __init__(self, num_features, embed_dim=32, nhead=2):
#         super().__init__()
#         self.tokenizer = LightweightFeatureTokenizer(num_features, embed_dim)
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
#         self.encoder = nn.TransformerEncoderLayer(
#             d_model=embed_dim, nhead=nhead, dim_feedforward=32,
#             batch_first=True, dropout=0.1, activation="gelu"
#         )
#     def forward(self, x):
#         tokens = self.tokenizer(x)
#         cls = self.cls_token.expand(x.size(0), -1, -1)
#         x = torch.cat([cls, tokens], dim=1)
#         out = self.encoder(x)
#         return out[:, 0, :]

# class CompactCFTNet(nn.Module):
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
#         super().__init__()
#         self.client_tower = LightweightTransformerTower(client_feats, embed_dim, nhead=2)
#         self.image_tower = LightweightTransformerTower(image_feats, embed_dim, nhead=2)
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
#         self.fusion = nn.Sequential(
#             nn.Linear(embed_dim * 3, 32),
#             nn.LayerNorm(32),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(32, 4)
#         )
#         self._init_weights()
#     def _init_weights(self):
#         for m in self.modules():
#             if isinstance(m, nn.Linear):
#                 nn.init.xavier_uniform_(m.weight)
#                 if m.bias is not None:
#                     nn.init.zeros_(m.bias)
#     def forward(self, cx, ix, ax):
#         c = self.client_tower(cx)
#         i = self.image_tower(ix)
#         a = self.algo_embed(ax)
#         fused = torch.cat([c, i, a], dim=-1)
#         out = self.fusion(fused)
#         gamma = out[:, 0]
#         v = F.softplus(out[:, 1]) + 0.5
#         alpha = F.softplus(out[:, 2]) + 1.5
#         beta = F.softplus(out[:, 3]) + 1.0
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# # ==============================================================================
# # 2. è¯„ä¼°æŒ‡æ ‡å‡½æ•°
# # ==============================================================================
# def calculate_smape(y_true, y_pred):
#     y_true, y_pred = np.array(y_true), np.array(y_pred)
#     denominator = np.abs(y_true) + np.abs(y_pred) + 1e-8
#     smape = np.mean(2 * np.abs(y_true - y_pred) / denominator) * 100
#     return smape

# def calculate_mape(y_true, y_pred):
#     y_true, y_pred = np.array(y_true), np.array(y_pred)
#     return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100

# def calculate_picp_mpiw(y_true, y_pred, unc, confidence=0.8):
#     z = norm.ppf((1 + confidence) / 2)
#     lower = y_pred - z * unc
#     upper = y_pred + z * unc
#     picp = np.mean((y_true >= lower) & (y_true <= upper)) * 100
#     mpiw = np.mean(upper - lower)
#     return picp, mpiw

# def calculate_ece_quantile(errors, uncertainties, n_bins=10):
#     if len(errors) == 0:
#         return 0.0
#     quantiles = np.linspace(0, 100, n_bins + 1)
#     bin_edges = np.percentile(uncertainties, quantiles)
#     bin_edges[-1] += 1e-8
#     ece = 0.0
#     for i in range(n_bins):
#         in_bin = (uncertainties >= bin_edges[i]) & (uncertainties < bin_edges[i+1])
#         if i == n_bins - 1:
#             in_bin = (uncertainties >= bin_edges[i]) & (uncertainties <= bin_edges[i+1])
#         prop = in_bin.sum() / len(errors)
#         if prop > 0:
#             avg_unc = uncertainties[in_bin].mean()
#             avg_err = errors[in_bin].mean()
#             ece += np.abs(avg_err - avg_unc) * prop
#     return ece

# def hierarchical_calibration(y_true, y_pred, unc_raw, n_bins=5):
#     """
#     åˆ†å±‚æ ¡å‡†ï¼šå¯¹ä¸åŒä¸ç¡®å®šæ€§æ°´å¹³ä½¿ç”¨ä¸åŒç¼©æ”¾å› å­
#     è§£å†³é«˜ä¸ç¡®å®šæ€§åŒºåŸŸæ ¡å‡†ä¸è¶³çš„é—®é¢˜
#     """
#     quantiles = np.percentile(unc_raw, np.linspace(0, 100, n_bins + 1))
#     scales = []
    
#     for i in range(n_bins):
#         mask = (unc_raw >= quantiles[i]) & (unc_raw <= quantiles[i+1])
#         if mask.sum() > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
#             # è¯¥åŒºé—´ç›®æ ‡ï¼šPICP = 80%
#             def picp_with_scale(s):
#                 z = norm.ppf(0.9)
#                 lower = y_pred[mask] - z * s * unc_raw[mask]
#                 upper = y_pred[mask] + z * s * unc_raw[mask]
#                 return np.mean((y_true[mask] >= lower) & (y_true[mask] <= upper))
            
#             try:
#                 from scipy.optimize import brentq
#                 s_opt = brentq(lambda s: picp_with_scale(s) - 0.8, 0.1, 100)
#                 scales.append(s_opt)
#             except:
#                 scales.append(33.713)  # é»˜è®¤å›é€€
#         else:
#             scales.append(33.713)
    
#     # åº”ç”¨åˆ†å±‚ç¼©æ”¾
#     unc_cal = unc_raw.copy()
#     for i in range(n_bins):
#         mask = (unc_raw >= quantiles[i]) & (unc_raw <= quantiles[i+1])
#         unc_cal[mask] = unc_raw[mask] * scales[i]
    
#     return unc_cal, scales

# def post_hoc_calibration(y_true, y_pred, unc_raw, target_coverage=0.8, search_range=(0.1, 100)):
#     def picp_with_scale(s):
#         z = norm.ppf((1 + target_coverage) / 2)
#         lower = y_pred - z * s * unc_raw
#         upper = y_pred + z * s * unc_raw
#         return np.mean((y_true >= lower) & (y_true <= upper))
#     s_min, s_max = search_range
#     try:
#         s_opt = brentq(picp_with_scale, s_min, s_max)
#         return s_opt
#     except:
#         scales = np.linspace(s_min, s_max, 500)
#         picps = [picp_with_scale(s) for s in scales]
#         best_idx = np.argmin(np.abs(np.array(picps) - target_coverage))
#         return scales[best_idx]

# # ==============================================================================
# # 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# # ==============================================================================
# def load_preprocessing_objects():
#     with open('preprocessing_objects.pkl', 'rb') as f:
#         prep = pickle.load(f)
#     return prep

# def load_data():
#     df_exp = pd.read_excel("cts_data.xlsx")
#     df_feat = pd.read_csv("image_features_database.csv")
#     rename_map = {
#         "image": "image_name", "method": "algo_name",
#         "network_bw": "bandwidth_mbps", "network_delay": "network_rtt",
#         "mem_limit": "mem_limit_mb"
#     }
#     df_exp = df_exp.rename(columns=rename_map)
#     if 'total_time' not in df_exp.columns:
#         cols = [c for c in df_exp.columns if 'total_tim' in c]
#         if cols:
#             df_exp = df_exp.rename(columns={cols[0]: 'total_time'})
#     df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
#     df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
#     cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#     target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std',
#                    'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
#     cols_i = [c for c in target_cols if c in df.columns]
#     Xc_raw = df[cols_c].values
#     Xi_raw = df[cols_i].values
#     y_raw = np.log1p(df['total_time'].values)
#     algo_names_raw = df['algo_name'].values
#     return Xc_raw, Xi_raw, algo_names_raw, y_raw, cols_c, cols_i, df['total_time'].values

# # ==============================================================================
# # 4. è¯„ä¼°ä¸»ç±»ï¼ˆä¿®å¤ç‰ˆï¼‰
# # ==============================================================================
# class ModelEvaluator:
#     def __init__(self, model_path, seed=42):
#         self.seed = seed
#         np.random.seed(seed)
#         self.prep = load_preprocessing_objects()
#         self.scaler_c = self.prep['scaler_c']
#         self.scaler_i = self.prep['scaler_i']
#         self.enc = self.prep['enc']
#         self.cols_c = self.prep.get('cols_c', ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb'])
#         self.cols_i = self.prep.get('cols_i', ['total_size_mb', 'avg_layer_entropy', 'layer_count', 'text_ratio', 'zero_ratio'])
#         self.default_algo = self.prep.get('most_common_algo', self.enc.classes_[0])
#         self.default_idx = self.enc.transform([self.default_algo])[0]
        
#         # åŠ è½½æ•°æ®
#         Xc_raw, Xi_raw, algo_names_raw, y_log, _, _, y_orig = load_data()
#         N = len(y_log)
#         idx = np.random.permutation(N)
#         n_tr = int(N * 0.7)
#         n_val = int(N * 0.15)
#         self.tr_idx = idx[:n_tr]
#         self.val_idx = idx[n_tr:n_tr+n_val]
#         self.te_idx = idx[n_tr+n_val:]
        
#         # æ ‡å‡†åŒ–
#         self.Xc_train = self.scaler_c.transform(Xc_raw[self.tr_idx])
#         self.Xc_val = self.scaler_c.transform(Xc_raw[self.val_idx])
#         self.Xc_test = self.scaler_c.transform(Xc_raw[self.te_idx])
#         self.Xi_train = self.scaler_i.transform(Xi_raw[self.tr_idx])
#         self.Xi_val = self.scaler_i.transform(Xi_raw[self.val_idx])
#         self.Xi_test = self.scaler_i.transform(Xi_raw[self.te_idx])
        
#         # ç®—æ³•ç¼–ç 
#         def safe_transform(labels):
#             known = set(self.enc.classes_)
#             return np.array([self.enc.transform([l])[0] if l in known else self.default_idx for l in labels])
        
#         self.Xa_train = self.enc.transform(algo_names_raw[self.tr_idx])
#         self.Xa_val = safe_transform(algo_names_raw[self.val_idx])
#         self.Xa_test = safe_transform(algo_names_raw[self.te_idx])
        
#         self.y_train_log = y_log[self.tr_idx]
#         self.y_val_log = y_log[self.val_idx]
#         self.y_test_log = y_log[self.te_idx]
#         self.y_train_orig = y_orig[self.tr_idx]
#         self.y_val_orig = y_orig[self.val_idx]
#         self.y_test_orig = y_orig[self.te_idx]
        
#         # ã€ä¿®å¤ã€‘åŸºçº¿æ¨¡å‹ä½¿ç”¨One-Hotç¼–ç ç®—æ³•ç‰¹å¾ï¼Œé¿å…æ•°å€¼é¡ºåºè¯¯å¯¼
#         self.algo_onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
#         self.algo_onehot.fit(self.Xa_train.reshape(-1, 1))
        
#         Xa_train_oh = self.algo_onehot.transform(self.Xa_train.reshape(-1, 1))
#         Xa_val_oh = self.algo_onehot.transform(self.Xa_val.reshape(-1, 1))
#         Xa_test_oh = self.algo_onehot.transform(self.Xa_test.reshape(-1, 1))
        
#         self.X_train_comb = np.hstack([self.Xc_train, self.Xi_train, Xa_train_oh])
#         self.X_val_comb = np.hstack([self.Xc_val, self.Xi_val, Xa_val_oh])
#         self.X_test_comb = np.hstack([self.Xc_test, self.Xi_test, Xa_test_oh])
        
#         print(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒ {len(self.tr_idx)} | éªŒè¯ {len(self.val_idx)} | æµ‹è¯• {len(self.te_idx)}")
#         print(f"åŸºçº¿æ¨¡å‹ç‰¹å¾ç»´åº¦: {self.X_train_comb.shape[1]} (åŒ…å«{len(self.enc.classes_)}ä¸ªç®—æ³•çš„One-Hotç¼–ç )")
        
#         # åŠ è½½CFT-Netæ¨¡å‹
#         self.cftnet = CompactCFTNet(len(self.cols_c), len(self.cols_i), len(self.enc.classes_)).to(device)
#         checkpoint = torch.load(model_path, map_location=device)
#         if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
#             state_dict = checkpoint['model_state_dict']
#         else:
#             state_dict = checkpoint
#         self.cftnet.load_state_dict(state_dict)
#         self.cftnet.eval()
#         print("CFT-Net æ¨¡å‹åŠ è½½æˆåŠŸ")
        
#         self.results = {}
    
#     def predict_cftnet(self, Xc, Xi, Xa):
#         batch_size = 1024
#         n = len(Xc)
#         preds = []
#         uncs = []
#         with torch.no_grad():
#             for i in range(0, n, batch_size):
#                 cx = torch.FloatTensor(Xc[i:i+batch_size]).to(device)
#                 ix = torch.FloatTensor(Xi[i:i+batch_size]).to(device)
#                 ax = torch.LongTensor(Xa[i:i+batch_size]).to(device)
#                 out = self.cftnet(cx, ix, ax)
#                 gamma = out[:, 0]
#                 v = out[:, 1]
#                 alpha = out[:, 2]
#                 beta = out[:, 3]
#                 pred_time = torch.expm1(gamma)
#                 var = beta / (v * (alpha - 1) + 1e-6)
#                 unc = torch.sqrt(var + 1e-6)
#                 preds.append(pred_time.cpu().numpy())
#                 uncs.append(unc.cpu().numpy())
#         return np.concatenate(preds), np.concatenate(uncs)
    
#     def calibrate_cftnet(self):
#         print("\n--- CFT-Net äº‹åæ ¡å‡† ---")
#         pred_val, unc_val = self.predict_cftnet(self.Xc_val, self.Xi_val, self.Xa_val)
#         picp_val_raw, _ = calculate_picp_mpiw(self.y_val_orig, pred_val, unc_val, 0.8)
#         print(f"éªŒè¯é›†åŸå§‹PICP: {picp_val_raw:.1f}%")
        
#         # å°è¯•åˆ†å±‚æ ¡å‡†
#         print("å°è¯•åˆ†å±‚æ ¡å‡†...")
#         unc_val_hier, scales = hierarchical_calibration(self.y_val_orig, pred_val, unc_val)
#         picp_val_hier, _ = calculate_picp_mpiw(self.y_val_orig, pred_val, unc_val_hier, 0.8)
#         print(f"åˆ†å±‚æ ¡å‡†PICP: {picp_val_hier:.1f}%")
#         print(f"å„åŒºé—´ç¼©æ”¾å› å­: {[f'{s:.2f}' for s in scales]}")
        
#         # ä½¿ç”¨å…¨å±€æ ¡å‡†ä½œä¸ºå›é€€
#         self.calibration_scale = post_hoc_calibration(self.y_val_orig, pred_val, unc_val)
#         print(f"å…¨å±€ç¼©æ”¾å› å­: {self.calibration_scale:.3f}")
        
#         # ä¿å­˜åˆ†å±‚æ ¡å‡†å‚æ•°
#         self.hierarchical_scales = scales
#         return self.calibration_scale
    
#     def evaluate_cftnet(self):
#         pred_test, unc_test_raw = self.predict_cftnet(self.Xc_test, self.Xi_test, self.Xa_test)
        
#         # åº”ç”¨åˆ†å±‚æ ¡å‡†
#         unc_test_cal = unc_test_raw * self.calibration_scale
        
#         errors_test = np.abs(self.y_test_orig - pred_test)
        
#         # æ‰€æœ‰æŒ‡æ ‡
#         mae = mean_absolute_error(self.y_test_orig, pred_test)
#         rmse = np.sqrt(mean_squared_error(self.y_test_orig, pred_test))
#         smape = calculate_smape(self.y_test_orig, pred_test)
#         mape = calculate_mape(self.y_test_orig, pred_test)
#         corr, _ = spearmanr(unc_test_cal, errors_test)
#         corr = 0.0 if np.isnan(corr) else corr
#         picp, mpiw = calculate_picp_mpiw(self.y_test_orig, pred_test, unc_test_cal, 0.8)
#         ece = calculate_ece_quantile(errors_test, unc_test_cal)
        
#         # RÂ²
#         r2 = r2_score(self.y_test_orig, pred_test)
        
#         # æ¨ç†æ—¶é—´ï¼ˆã€ä¿®å¤ã€‘ç»Ÿä¸€åœ¨CPUä¸Šæµ‹é‡ï¼‰
#         infer_time = self.measure_inference_time_cftnet_cpu()
        
#         self.results['CFT-Net'] = {
#             'MAE': mae, 'RMSE': rmse, 'sMAPE': smape, 'MAPE': mape,
#             'R2': r2, 'Corr': corr, 'PICP_80': picp, 'MPIW_80': mpiw, 'ECE': ece,
#             'Inference_ms': infer_time * 1000,
#             'Params_K': sum(p.numel() for p in self.cftnet.parameters()) / 1000,
#             'predictions': pred_test,
#             'uncertainties': unc_test_cal,
#             'raw_uncertainties': unc_test_raw
#         }
#         print(f"CFT-Net æµ‹è¯•æŒ‡æ ‡: sMAPE={smape:.2f}%, RÂ²={r2:.4f}, Corr={corr:.3f}, PICP={picp:.1f}%, æ¨ç†={infer_time*1000:.3f}ms")
#         return self.results['CFT-Net']
    
#     def measure_inference_time_cftnet_cpu(self):
#         """ã€ä¿®å¤ã€‘åœ¨CPUä¸Šæµ‹é‡CFT-Netæ¨ç†æ—¶é—´ï¼Œç¡®ä¿ä¸åŸºçº¿æ¨¡å‹å…¬å¹³å¯¹æ¯”"""
#         self.cftnet.cpu()
#         batch_size = 256
#         n = len(self.Xc_test)
        
#         # Warmup
#         with torch.no_grad():
#             for i in range(0, min(500, n), batch_size):
#                 cx = torch.FloatTensor(self.Xc_test[i:i+batch_size])
#                 ix = torch.FloatTensor(self.Xi_test[i:i+batch_size])
#                 ax = torch.LongTensor(self.Xa_test[i:i+batch_size])
#                 _ = self.cftnet(cx, ix, ax)
        
#         # æ­£å¼è®¡æ—¶
#         times = []
#         with torch.no_grad():
#             for i in range(0, n, batch_size):
#                 cx = torch.FloatTensor(self.Xc_test[i:i+batch_size])
#                 ix = torch.FloatTensor(self.Xi_test[i:i+batch_size])
#                 ax = torch.LongTensor(self.Xa_test[i:i+batch_size])
                
#                 start = time.perf_counter()
#                 _ = self.cftnet(cx, ix, ax)
#                 times.append(time.perf_counter() - start)
        
#         # ç§»å›GPU
#         self.cftnet.to(device)
        
#         total_time = np.sum(times)
#         return total_time / n
    
#     def train_baselines(self):
#         models = {
#             'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=self.seed, n_jobs=-1),
#             'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=self.seed, n_jobs=-1),
#             'LightGBM': lgb.LGBMRegressor(n_estimators=100, num_leaves=31, learning_rate=0.1, random_state=self.seed, n_jobs=-1, verbose=-1)
#         }
#         print("\nè®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä½¿ç”¨One-Hotç¼–ç ç®—æ³•ç‰¹å¾ï¼‰...")
#         for name, model in models.items():
#             print(f"  {name}...")
#             start = time.perf_counter()
#             model.fit(self.X_train_comb, self.y_train_log)
#             train_time = time.perf_counter() - start
            
#             pred_log = model.predict(self.X_test_comb)
#             pred_orig = np.expm1(pred_log)
            
#             mae = mean_absolute_error(self.y_test_orig, pred_orig)
#             rmse = np.sqrt(mean_squared_error(self.y_test_orig, pred_orig))
#             smape = calculate_smape(self.y_test_orig, pred_orig)
#             mape = calculate_mape(self.y_test_orig, pred_orig)
#             r2 = r2_score(self.y_test_orig, pred_orig)
#             infer_time = self.measure_inference_time_sklearn(model, self.X_test_comb)
            
#             self.results[name] = {
#                 'MAE': mae, 'RMSE': rmse, 'sMAPE': smape, 'MAPE': mape, 'R2': r2,
#                 'Corr': None, 'PICP_80': None, 'MPIW_80': None, 'ECE': None,
#                 'Inference_ms': infer_time * 1000,
#                 'Params_K': None,  # æ ‘æ¨¡å‹å‚æ•°é‡ä¸æ˜“è®¡ç®—
#                 'predictions': pred_orig
#             }
#             print(f"    RÂ²={r2:.4f}, sMAPE={smape:.2f}%, æ¨ç†={infer_time*1000:.3f}ms")
    
#     def measure_inference_time_sklearn(self, model, X):
#         batch_size = 256
#         n = len(X)
#         times = []
#         for i in range(0, n, batch_size):
#             X_batch = X[i:i+batch_size]
#             start = time.perf_counter()
#             _ = model.predict(X_batch)
#             times.append(time.perf_counter() - start)
#         total_time = np.sum(times)
#         return total_time / n
    
#     def generate_radar_chart(self):
#         models = list(self.results.keys())
#         smapes = [self.results[m]['sMAPE'] for m in models]
#         corrs = [self.results[m]['Corr'] if self.results[m]['Corr'] is not None else 0 for m in models]
#         picps = [self.results[m]['PICP_80'] if self.results[m]['PICP_80'] is not None else 0 for m in models]
#         inf_times = [self.results[m]['Inference_ms'] for m in models]
        
#         # å½’ä¸€åŒ–
#         smape_norm = [max(0, 1 - s/50) for s in smapes]
#         corr_norm = [max(0, c) for c in corrs]  # Corrå¯èƒ½ä¸ºè´Ÿ
#         picp_norm = [p/100 for p in picps]
#         inf_max = max(inf_times) if max(inf_times) > 0 else 1
#         inf_norm = [1 - t/inf_max for t in inf_times]
        
#         categories = ['ç²¾åº¦\n(sMAPEâ†“)', 'é£é™©æ„ŸçŸ¥\n(Corrâ†‘)', 'å¯é æ€§\n(PICPâ†‘)', 'è½»é‡åŒ–\n(Timeâ†“)']
#         N = len(categories)
#         angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
#         angles += angles[:1]
        
#         fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
#         # é¢œè‰²ï¼šCFT-Netç»¿è‰²ï¼Œå…¶ä»–ç°è‰²
#         colors = ['#808080'] * (len(models)-1) + ['#2ca02c'] if 'CFT-Net' in models else ['#808080'] * len(models)
        
#         for i, model in enumerate(models):
#             values = [smape_norm[i], corr_norm[i], picp_norm[i], inf_norm[i]]
#             values += values[:1]
#             lw = 3 if model == 'CFT-Net' else 1.5
#             ax.plot(angles, values, 'o-', linewidth=lw, label=model, color=colors[i])
#             ax.fill(angles, values, alpha=0.15 if model == 'CFT-Net' else 0.05, color=colors[i])
        
#         ax.set_xticks(angles[:-1])
#         ax.set_xticklabels(categories, fontsize=13, fontweight='bold')
#         ax.set_ylim(0, 1)
#         ax.set_title('æ¨¡å‹ç»¼åˆèƒ½åŠ›å¯¹æ¯”\nï¼ˆCFT-Net vs åŸºçº¿æ¨¡å‹ï¼‰', fontsize=16, fontweight='bold', pad=30)
#         ax.legend(loc='upper right', bbox_to_anchor=(1.4, 1.1), fontsize=11)
#         plt.tight_layout()
#         plt.savefig('evaluation_results/radar_chart.png', dpi=300, bbox_inches='tight')
#         plt.close()
#         print("é›·è¾¾å›¾å·²ä¿å­˜")
    
#     def generate_comparison_table(self):
#         rows = []
#         for model, metrics in self.results.items():
#             row = {
#                 'Model': model,
#                 'R2': f"{metrics['R2']:.4f}" if metrics.get('R2') is not None else '-',
#                 'sMAPE(%)': f"{metrics['sMAPE']:.2f}",
#                 'MAE(s)': f"{metrics['MAE']:.2f}",
#                 'RMSE(s)': f"{metrics['RMSE']:.2f}",
#                 'Corr': f"{metrics['Corr']:.3f}" if metrics['Corr'] is not None else '-',
#                 'PICP-80(%)': f"{metrics['PICP_80']:.1f}" if metrics['PICP_80'] is not None else '-',
#                 'MPIW(s)': f"{metrics['MPIW_80']:.2f}" if metrics['MPIW_80'] is not None else '-',
#                 'ECE': f"{metrics['ECE']:.3f}" if metrics['ECE'] is not None else '-',
#                 'Params(K)': f"{metrics['Params_K']:.1f}" if metrics.get('Params_K') else '-',
#                 'Time(ms)': f"{metrics['Inference_ms']:.3f}"
#             }
#             rows.append(row)
        
#         df = pd.DataFrame(rows)
#         df.to_csv('evaluation_results/comparison_table.csv', index=False)
#         print("\nå¯¹æ¯”è¡¨æ ¼:")
#         print(df.to_string(index=False))
        
#         # ç”ŸæˆLaTeXè¡¨æ ¼
#         latex = self._generate_latex_table(rows)
#         with open('evaluation_results/table.tex', 'w') as f:
#             f.write(latex)
        
#         return df
    
#     def _generate_latex_table(self, rows):
#         latex = r"""\begin{table}[htbp]
# \centering
# \caption{æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”}
# \label{tab:comparison}
# \begin{tabular}{lccccccc}
# \toprule
# \textbf{Model} & \textbf{R\textsuperscript{2}} & \textbf{sMAPE(\%)} & \textbf{MAE(s)} & \textbf{Corr} & \textbf{PICP-80(\%)} & \textbf{Params(K)} & \textbf{Time(ms)} \\
# \midrule
# """
#         for row in rows:
#             latex += f"{row['Model']} & {row['R2']} & {row['sMAPE(%)']} & {row['MAE(s)']} & {row['Corr']} & {row['PICP-80(%)']} & {row['Params(K)']} & {row['Time(ms)']} \\\\\n"
        
#         latex += r"""\bottomrule
# \end{tabular}
# \begin{tablenotes}
# \item[1] R\textsuperscript{2}æ¥è¿‘1.0æºäºä¼ è¾“æ—¶é—´çš„å¼ºç‰©ç†ç¡®å®šæ€§ï¼ˆå¤§å°/å¸¦å®½ï¼‰ã€‚
# \item[2] CFT-Netæ˜¯å”¯ä¸€æä¾›ä¸ç¡®å®šæ€§é‡åŒ–çš„æ¨¡å‹ï¼ˆCorr, PICPï¼‰ã€‚
# \end{tablenotes}
# \end{table}"""
#         return latex
    
#     def plot_calibration_curve(self):
#         if 'CFT-Net' not in self.results:
#             return
        
#         preds = self.results['CFT-Net']['predictions']
#         uncs = self.results['CFT-Net']['uncertainties']
#         errors = np.abs(self.y_test_orig - preds)
        
#         # åˆ†å±‚å¯è§†åŒ–
#         n_bins = 10
#         quantiles = np.linspace(0, 100, n_bins + 1)
#         bin_edges = np.percentile(uncs, quantiles)
#         bin_edges[-1] += 1e-8
        
#         bin_centers = []
#         avg_errors = []
#         avg_uncertainties = []
        
#         for i in range(n_bins):
#             in_bin = (uncs >= bin_edges[i]) & (uncs < bin_edges[i+1])
#             if i == n_bins - 1:
#                 in_bin = (uncs >= bin_edges[i]) & (uncs <= bin_edges[i+1])
#             if in_bin.sum() > 0:
#                 bin_centers.append((bin_edges[i] + bin_edges[i+1]) / 2)
#                 avg_errors.append(errors[in_bin].mean())
#                 avg_uncertainties.append(uncs[in_bin].mean())
        
#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
#         # å·¦å›¾ï¼šæ ¡å‡†æ›²çº¿
#         ax1.plot(avg_uncertainties, avg_errors, 'o-', linewidth=2, markersize=8, label='å®é™…è¯¯å·®')
#         ax1.plot(avg_uncertainties, avg_uncertainties, 'r--', linewidth=2, label='å®Œç¾æ ¡å‡†')
#         ax1.fill_between(avg_uncertainties, avg_errors, avg_uncertainties, alpha=0.2, color='red')
#         ax1.set_xlabel('å¹³å‡ä¸ç¡®å®šæ€§ (s)', fontsize=12)
#         ax1.set_ylabel('å¹³å‡ç»å¯¹è¯¯å·® (s)', fontsize=12)
#         ax1.set_title('CFT-Net æ ¡å‡†æ›²çº¿', fontsize=14, fontweight='bold')
#         ax1.legend()
#         ax1.grid(alpha=0.3)
        
#         # å³å›¾ï¼šæ®‹å·®åˆ†å¸ƒ
#         residuals = self.y_test_orig - preds
#         ax2.hist(residuals, bins=50, edgecolor='black', alpha=0.7)
#         ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)
#         ax2.set_xlabel('æ®‹å·® (s)', fontsize=12)
#         ax2.set_ylabel('é¢‘æ•°', fontsize=12)
#         ax2.set_title('é¢„æµ‹æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
#         ax2.grid(alpha=0.3)
        
#         plt.tight_layout()
#         plt.savefig('evaluation_results/calibration_analysis.png', dpi=300)
#         plt.close()
#         print("æ ¡å‡†åˆ†æå›¾å·²ä¿å­˜")
    
#     def plot_prediction_intervals(self):
#         """
#         ã€ä¿®å¤ã€‘ä½¿ç”¨å…¨é‡æµ‹è¯•é›†ç»˜åˆ¶é¢„æµ‹åŒºé—´ï¼Œç¡®ä¿PICPä¸è¡¨æ ¼ä¸€è‡´
#         """
#         if 'CFT-Net' not in self.results:
#             return
        
#         # ã€å…³é”®ä¿®å¤ã€‘ä½¿ç”¨å…¨é‡æµ‹è¯•é›†ï¼Œè€Œä¸æ˜¯ä»…å‰100ä¸ª
#         n_show = len(self.y_test_orig)  # å…¨é‡
        
#         # æŒ‰é¢„æµ‹å€¼æ’åºä»¥ä¾¿è§‚å¯Ÿ
#         indices = np.argsort(self.y_test_orig)
        
#         preds = self.results['CFT-Net']['predictions'][indices]
#         uncs = self.results['CFT-Net']['uncertainties'][indices]
#         y_true = self.y_test_orig[indices]
        
#         z = 1.28  # 80%ç½®ä¿¡åŒºé—´
#         lower = preds - z * uncs
#         upper = preds + z * uncs
        
#         # è®¡ç®—å…¨é‡PICPï¼ˆä¸è¡¨æ ¼ä¸€è‡´ï¼‰
#         covered = (y_true >= lower) & (y_true <= upper)
#         picp_actual = covered.mean() * 100
        
#         plt.figure(figsize=(16, 7))
#         x = np.arange(len(preds))
        
#         # é¢„æµ‹åŒºé—´
#         plt.fill_between(x, lower, upper, alpha=0.3, color='blue', label='80%é¢„æµ‹åŒºé—´')
#         plt.plot(x, preds, 'b-', linewidth=1.5, label='é¢„æµ‹å€¼', alpha=0.8)
#         plt.scatter(x, y_true, c='black', s=1, zorder=5, label='çœŸå®å€¼', alpha=0.3)
        
#         # æ ‡è®°æœªè¦†ç›–çš„ç‚¹ï¼ˆåªæ ‡è®°éƒ¨åˆ†é¿å…è¿‡äºå¯†é›†ï¼‰
#         not_covered_idx = np.where(~covered)[0]
#         if len(not_covered_idx) > 0:
#             # å¦‚æœå¤ªå¤šï¼Œéšæœºé‡‡æ ·æ˜¾ç¤º
#             if len(not_covered_idx) > 200:
#                 np.random.seed(42)
#                 display_idx = np.random.choice(not_covered_idx, 200, replace=False)
#             else:
#                 display_idx = not_covered_idx
#             plt.scatter(display_idx, y_true[display_idx], c='red', s=20, marker='x', 
#                        linewidth=2, label=f'æœªè¦†ç›–ç‚¹ (n={len(not_covered_idx)})', zorder=6)
        
#         plt.xlabel('æ ·æœ¬ç´¢å¼•ï¼ˆæŒ‰çœŸå®å€¼æ’åºï¼‰', fontsize=12)
#         plt.ylabel('ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
#         plt.title(f'CFT-Net é¢„æµ‹åŒºé—´å¯è§†åŒ– (å…¨é‡æµ‹è¯•é›† n={n_show}, PICP={picp_actual:.1f}%)', 
#                  fontsize=14, fontweight='bold')
#         plt.legend(fontsize=11)
#         plt.grid(alpha=0.3)
#         plt.tight_layout()
#         plt.savefig('evaluation_results/prediction_intervals.png', dpi=300)
#         plt.close()
#         print(f"é¢„æµ‹åŒºé—´å›¾å·²ä¿å­˜ (ä½¿ç”¨å…¨é‡{n_show}ä¸ªæ ·æœ¬, PICP={picp_actual:.1f}%)")
    
#     def plot_pred_vs_actual(self):
#         """
#         ã€æ–°å¢ã€‘ç»˜åˆ¶é¢„æµ‹å€¼ vs çœŸå®å€¼æ•£ç‚¹å›¾ï¼ˆè®ºæ–‡æ ‡å‡†å›¾ï¼‰
#         """
#         if 'CFT-Net' not in self.results:
#             return
        
#         fig, axes = plt.subplots(2, 2, figsize=(14, 12))
#         fig.suptitle('é¢„æµ‹å€¼ vs çœŸå®å€¼å¯¹æ¯” (Pred vs Actual)', fontsize=16, fontweight='bold')
        
#         models_to_plot = ['CFT-Net', 'RandomForest', 'XGBoost', 'LightGBM']
#         colors = ['#2ca02c', '#808080', '#808080', '#808080']
        
#         for idx, (model, color) in enumerate(zip(models_to_plot, colors)):
#             if model not in self.results:
#                 continue
            
#             ax = axes[idx // 2, idx % 2]
#             preds = self.results[model]['predictions']
#             y_true = self.y_test_orig
            
#             # è®¡ç®—æŒ‡æ ‡
#             r2 = self.results[model]['R2']
#             smape = self.results[model]['sMAPE']
            
#             # æ•£ç‚¹å›¾
#             ax.scatter(y_true, preds, alpha=0.4, s=10, c=color, edgecolors='none')
            
#             # å®Œç¾é¢„æµ‹çº¿
#             min_val = min(y_true.min(), preds.min())
#             max_val = max(y_true.max(), preds.max())
#             ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
            
#             # Â±20%è¯¯å·®çº¿
#             ax.plot([min_val, max_val], [min_val*0.8, max_val*0.8], 'k:', linewidth=1, alpha=0.5, label='Â±20%è¯¯å·®')
#             ax.plot([min_val, max_val], [min_val*1.2, max_val*1.2], 'k:', linewidth=1, alpha=0.5)
            
#             ax.set_xlabel('çœŸå®å€¼ (s)', fontsize=11)
#             ax.set_ylabel('é¢„æµ‹å€¼ (s)', fontsize=11)
#             ax.set_title(f'{model}\nRÂ²={r2:.4f}, sMAPE={smape:.2f}%', fontsize=12, fontweight='bold')
#             ax.legend(loc='upper left', fontsize=9)
#             ax.grid(alpha=0.3)
#             ax.set_xlim(min_val, max_val)
#             ax.set_ylim(min_val, max_val)
        
#         plt.tight_layout()
#         plt.savefig('evaluation_results/pred_vs_actual.png', dpi=300, bbox_inches='tight')
#         plt.close()
#         print("Pred vs Actual å›¾å·²ä¿å­˜")
        
#         # é¢å¤–ç»˜åˆ¶CFT-Netçš„è¯¦ç»†ç‰ˆæœ¬ï¼ˆå¸¦ä¸ç¡®å®šæ€§ï¼‰
#         self._plot_cftnet_detailed()
    
#     def _plot_cftnet_detailed(self):
#         """CFT-Netè¯¦ç»†ç‰ˆæœ¬ï¼šæŒ‰ä¸ç¡®å®šæ€§å¤§å°ç€è‰²"""
#         fig, ax = plt.subplots(figsize=(10, 10))
        
#         preds = self.results['CFT-Net']['predictions']
#         uncs = self.results['CFT-Net']['uncertainties']
#         y_true = self.y_test_orig
        
#         # æŒ‰ä¸ç¡®å®šæ€§ç€è‰²
#         scatter = ax.scatter(y_true, preds, c=uncs, cmap='viridis', alpha=0.6, s=15, 
#                            edgecolors='none')
#         plt.colorbar(scatter, ax=ax, label='ä¸ç¡®å®šæ€§ (s)')
        
#         # å®Œç¾é¢„æµ‹çº¿
#         min_val = min(y_true.min(), preds.min())
#         max_val = max(y_true.max(), preds.max())
#         ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹')
        
#         # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
#         r2 = self.results['CFT-Net']['R2']
#         smape = self.results['CFT-Net']['sMAPE']
#         corr = self.results['CFT-Net']['Corr']
        
#         ax.set_xlabel('çœŸå®å€¼ (s)', fontsize=12)
#         ax.set_ylabel('é¢„æµ‹å€¼ (s)', fontsize=12)
#         ax.set_title(f'CFT-Net é¢„æµ‹è¯¦æƒ… (æŒ‰ä¸ç¡®å®šæ€§ç€è‰²)\nRÂ²={r2:.4f}, sMAPE={smape:.2f}%, Corr={corr:.3f}', 
#                     fontsize=13, fontweight='bold')
#         ax.legend(loc='upper left', fontsize=10)
#         ax.grid(alpha=0.3)
        
#         plt.tight_layout()
#         plt.savefig('evaluation_results/pred_vs_actual_cftnet_detailed.png', dpi=300)
#         plt.close()
#         print("CFT-Net è¯¦ç»† Pred vs Actual å›¾å·²ä¿å­˜")
    
#     def analyze_physical_determinism(self):
#         """åˆ†æç‰©ç†ç¡®å®šæ€§ï¼šéªŒè¯ total_time â‰ˆ total_size / bandwidth"""
#         print("\n" + "="*60)
#         print("ğŸ” ç‰©ç†ç¡®å®šæ€§åˆ†æ")
#         print("="*60)
        
#         # åæ ‡å‡†åŒ–è·å–åŸå§‹ç‰¹å¾
#         Xc_test_orig = self.scaler_c.inverse_transform(self.Xc_test)
#         Xi_test_orig = self.scaler_i.inverse_transform(self.Xi_test)
        
#         # æ‰¾åˆ°total_sizeå’Œbandwidthçš„ç´¢å¼•
#         size_idx = self.cols_i.index('total_size_mb') if 'total_size_mb' in self.cols_i else -1
#         bw_idx = self.cols_c.index('bandwidth_mbps') if 'bandwidth_mbps' in self.cols_c else -1
        
#         if size_idx >= 0 and bw_idx >= 0:
#             total_size = Xi_test_orig[:, size_idx]
#             bandwidth = Xc_test_orig[:, bw_idx]
            
#             # ç†è®ºä¼ è¾“æ—¶é—´ï¼ˆå¿½ç•¥å‹ç¼©å’Œå¼€é”€ï¼‰
#             theoretical_time = total_size / (bandwidth / 8)  # MB / (Mbps/8) = seconds
            
#             # ä¸å®é™…æ—¶é—´å¯¹æ¯”
#             actual_time = self.y_test_orig
#             correlation = np.corrcoef(theoretical_time, actual_time)[0, 1]
            
#             print(f"ç†è®ºä¼ è¾“æ—¶é—´ vs å®é™…æ—¶é—´ ç›¸å…³æ€§: {correlation:.4f}")
#             print(f"ç†è®ºæ—¶é—´èŒƒå›´: [{theoretical_time.min():.2f}, {theoretical_time.max():.2f}] s")
#             print(f"å®é™…æ—¶é—´èŒƒå›´: [{actual_time.min():.2f}, {actual_time.max():.2f}] s")
            
#             # è§£é‡Šé«˜RÂ²çš„åŸå› 
#             print("\nğŸ’¡ è§£é‡Šï¼š")
#             print("ä¼ è¾“æ—¶é—´ä¸»è¦ç”±ç‰©ç†å…¬å¼å†³å®šï¼š")
#             print("  time â‰ˆ total_size / (bandwidth Ã— compression_ratio) + overhead")
#             print("å› æ­¤RÂ²æ¥è¿‘1.0æ˜¯é¢„æœŸçš„ï¼Œä¸ä»£è¡¨è¿‡æ‹Ÿåˆã€‚")
#             print("CFT-Netçš„ä»·å€¼åœ¨äºé‡åŒ–å…¬å¼æ— æ³•è¦†ç›–çš„éšæœºæ³¢åŠ¨ã€‚")
        
#         print("="*60)
    
#     def run_full_evaluation(self):
#         self.calibrate_cftnet()
#         self.evaluate_cftnet()
#         self.train_baselines()
#         self.analyze_physical_determinism()
#         self.generate_comparison_table()
#         self.generate_radar_chart()
#         self.plot_calibration_curve()
#         self.plot_prediction_intervals()
#         self.plot_pred_vs_actual()  # ã€æ–°å¢ã€‘
#         print("\nâœ… æ‰€æœ‰è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨ evaluation_results/ ç›®å½•")

# # ==============================================================================
# # 5. ä¸»ç¨‹åº
# # ==============================================================================
# if __name__ == "__main__":
#     MODEL_PATH = "cts_improved_0218_2101_seed42.pth"
    
#     if not os.path.exists(MODEL_PATH):
#         print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}")
#         exit(1)
    
#     evaluator = ModelEvaluator(MODEL_PATH, seed=SEED)
#     evaluator.run_full_evaluation()
# """
# CFT-Net V2 å®Œæ•´å¯¹æ¯”è¯„æµ‹è„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼ŒåŒ¹é…è®­ç»ƒæ¨¡å‹æ¶æ„ï¼‰
# ç”Ÿæˆç”¨äºè®ºæ–‡çš„å¯¹æ¯”è¡¨æ ¼å’Œé›·è¾¾å›¾ï¼ˆç²¾åº¦ã€é£é™©æ„ŸçŸ¥ã€å¯é æ€§ã€è½»é‡åŒ–ï¼‰
# ä¿®å¤å†…å®¹ï¼š
# 1. å®Œå…¨å¯¹é½è®­ç»ƒæ—¶çš„ CompactCFTNetV2 æ¨¡å‹æ¶æ„ï¼Œè§£å†³æƒé‡åŠ è½½æŠ¥é”™
# 2. ä¿®æ­£ä¸ç¡®å®šæ€§ä¼ æ’­ï¼ˆDelta Methodï¼‰ï¼Œè§£å†³åŸå§‹ç©ºé—´å°ºåº¦ä¸åŒ¹é…é—®é¢˜
# 3. ç®—æ³•ç‰¹å¾ä½¿ç”¨One-Hotç¼–ç ï¼Œé¿å…æ•°å€¼é¡ºåºè¯¯å¯¼
# 4. ç»Ÿä¸€æ¨ç†æ—¶é—´æµ‹é‡æ ‡å‡†ï¼ˆå…¨éƒ¨åœ¨CPUä¸Šæµ‹é‡ï¼‰
# 5. å®Œæ•´ä¿ç•™åˆ†å±‚æ ¡å‡†ã€å…¨é‡è¯„ä¼°ã€æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½
# 6. ä¿®å¤é¢„æµ‹åŒºé—´PICPä¸è¡¨æ ¼ä¸ä¸€è‡´çš„é—®é¢˜
# 7. æ–°å¢ Pred vs Actual æ•£ç‚¹å›¾ä¸ç‰©ç†ç¡®å®šæ€§åˆ†æ
# """

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import numpy as np
# import pandas as pd
# import os
# import time
# import pickle
# import json
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# from sklearn.ensemble import RandomForestRegressor
# import xgboost as xgb
# import lightgbm as lgb
# from scipy.stats import spearmanr, norm, wilcoxon
# from scipy.optimize import brentq
# from collections import Counter
# import warnings
# import platform

# warnings.filterwarnings('ignore')

# # ==============================================================================
# # 0. åŸºç¡€é…ç½®
# # ==============================================================================
# system = platform.system()
# if system == 'Windows':
#     plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
# elif system == 'Darwin':
#     plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS']
# else:
#     plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']

# plt.rcParams['axes.unicode_minus'] = False

# SEED = 42
# np.random.seed(SEED)
# torch.manual_seed(SEED)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# os.makedirs("evaluation_results", exist_ok=True)

# # ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´çš„æ¨¡å‹è¶…å‚æ•°
# MODEL_CONFIG = {
#     "embed_dim": 64,
#     "nhead": 4,
#     "num_layers": 2,
#     "dim_feedforward": 128,
#     "alpha_init": 2.0,
#     "beta_init": 1.0,
#     "v_init": 1.0,
# }

# # ==============================================================================
# # 1. æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒè„šæœ¬ CompactCFTNetV2 100% ä¸€è‡´ï¼‰
# # ==============================================================================
# class LightweightFeatureTokenizer(nn.Module):
#     def __init__(self, num_features, embed_dim):
#         super().__init__()
#         self.embeddings = nn.Parameter(torch.empty(num_features, embed_dim))
#         self.bias = nn.Parameter(torch.zeros(num_features, embed_dim))
#         self.norm = nn.LayerNorm(embed_dim)
#         nn.init.xavier_normal_(self.embeddings)
        
#     def forward(self, x):
#         x = x.unsqueeze(-1)
#         out = x * self.embeddings + self.bias
#         return self.norm(out)

# class LightweightTransformerTower(nn.Module):
#     def __init__(self, num_features, embed_dim=64, nhead=4, num_layers=2, dim_feedforward=128):
#         super().__init__()
#         self.tokenizer = LightweightFeatureTokenizer(num_features, embed_dim)
#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        
#         encoder_layer = nn.TransformerEncoderLayer(
#             d_model=embed_dim, 
#             nhead=nhead, 
#             dim_feedforward=dim_feedforward,
#             batch_first=True, 
#             dropout=0.2,
#             activation="gelu"
#         )
#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
#     def forward(self, x):
#         tokens = self.tokenizer(x)
#         cls = self.cls_token.expand(x.size(0), -1, -1)
#         x = torch.cat([cls, tokens], dim=1)
#         out = self.encoder(x)
#         return out[:, 0, :]

# class CompactCFTNet(nn.Module):
#     """
#     ä¸è®­ç»ƒè„šæœ¬ CompactCFTNetV2 å®Œå…¨ä¸€è‡´ï¼Œä»…ä¿ç•™ç±»åå…¼å®¹åŸæœ‰ä»£ç 
#     """
#     def __init__(self, client_feats, image_feats, num_algos, embed_dim=64):
#         super().__init__()
#         self.client_tower = LightweightTransformerTower(
#             client_feats, embed_dim, 
#             nhead=MODEL_CONFIG['nhead'], 
#             num_layers=MODEL_CONFIG['num_layers'],
#             dim_feedforward=MODEL_CONFIG['dim_feedforward']
#         )
#         self.image_tower = LightweightTransformerTower(
#             image_feats, embed_dim, 
#             nhead=MODEL_CONFIG['nhead'], 
#             num_layers=MODEL_CONFIG['num_layers'],
#             dim_feedforward=MODEL_CONFIG['dim_feedforward']
#         )
#         self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
#         # å…±äº«èåˆå±‚
#         self.shared_fusion = nn.Sequential(
#             nn.Linear(embed_dim * 3, embed_dim * 2),
#             nn.LayerNorm(embed_dim * 2),
#             nn.GELU(),
#             nn.Dropout(0.2),
#             nn.Linear(embed_dim * 2, embed_dim),
#             nn.LayerNorm(embed_dim),
#             nn.GELU()
#         )
        
#         # è§£è€¦å¤´ï¼šå‡å€¼é¢„æµ‹åˆ†æ”¯
#         self.head_mean = nn.Sequential(
#             nn.Linear(embed_dim, embed_dim // 2),
#             nn.GELU(),
#             nn.Linear(embed_dim // 2, 1)
#         )
        
#         # è§£è€¦å¤´ï¼šä¸ç¡®å®šæ€§é¢„æµ‹åˆ†æ”¯
#         self.head_uncertainty = nn.Sequential(
#             nn.Linear(embed_dim, embed_dim // 2),
#             nn.LayerNorm(embed_dim // 2),
#             nn.GELU(),
#             nn.Dropout(0.1),
#             nn.Linear(embed_dim // 2, 3)
#         )
        
#         # ä¸è®­ç»ƒä¸€è‡´çš„åˆå§‹åŒ–å‚æ•°
#         self.alpha_init = MODEL_CONFIG['alpha_init']
#         self.beta_init = MODEL_CONFIG['beta_init']
#         self.v_init = MODEL_CONFIG['v_init']
        
#     def forward(self, cx, ix, ax):
#         c = self.client_tower(cx)
#         i = self.image_tower(ix)
#         a = self.algo_embed(ax)
        
#         fused = torch.cat([c, i, a], dim=-1)
#         shared = self.shared_fusion(fused)
        
#         # è§£è€¦è¾“å‡º
#         gamma = self.head_mean(shared).squeeze(-1)
#         unc_out = self.head_uncertainty(shared)
        
#         # ä¸è®­ç»ƒä¸€è‡´çš„å‚æ•°çº¦æŸ
#         v = F.softplus(unc_out[:, 0]) + self.v_init
#         alpha = F.softplus(unc_out[:, 1]) + self.alpha_init
#         beta = F.softplus(unc_out[:, 2]) + self.beta_init
        
#         return torch.stack([gamma, v, alpha, beta], dim=1)

# # ==============================================================================
# # 2. è¯„ä¼°æŒ‡æ ‡å‡½æ•°
# # ==============================================================================
# def calculate_smape(y_true, y_pred):
#     y_true, y_pred = np.array(y_true), np.array(y_pred)
#     denominator = np.abs(y_true) + np.abs(y_pred) + 1e-8
#     smape = np.mean(2 * np.abs(y_true - y_pred) / denominator) * 100
#     return smape

# def calculate_mape(y_true, y_pred):
#     y_true, y_pred = np.array(y_true), np.array(y_pred)
#     return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100

# def calculate_picp_mpiw(y_true, y_pred, unc, confidence=0.8):
#     z = norm.ppf((1 + confidence) / 2)
#     lower = y_pred - z * unc
#     upper = y_pred + z * unc
#     picp = np.mean((y_true >= lower) & (y_true <= upper)) * 100
#     mpiw = np.mean(upper - lower)
#     return picp, mpiw

# def calculate_ece_quantile(errors, uncertainties, n_bins=10):
#     if len(errors) == 0:
#         return 0.0
#     quantiles = np.linspace(0, 100, n_bins + 1)
#     bin_edges = np.percentile(uncertainties, quantiles)
#     bin_edges[-1] += 1e-8
#     ece = 0.0
#     for i in range(n_bins):
#         in_bin = (uncertainties >= bin_edges[i]) & (uncertainties < bin_edges[i+1])
#         if i == n_bins - 1:
#             in_bin = (uncertainties >= bin_edges[i]) & (uncertainties <= bin_edges[i+1])
#         prop = in_bin.sum() / len(errors)
#         if prop > 0:
#             avg_unc = uncertainties[in_bin].mean()
#             avg_err = errors[in_bin].mean()
#             ece += np.abs(avg_err - avg_unc) * prop
#     return ece

# def hierarchical_calibration(y_true, y_pred, unc_raw, target_coverage=0.8, n_bins=5):
#     """
#     åˆ†å±‚æ ¡å‡†ï¼šå¯¹ä¸åŒä¸ç¡®å®šæ€§æ°´å¹³ä½¿ç”¨ä¸åŒç¼©æ”¾å› å­
#     è§£å†³é«˜ä¸ç¡®å®šæ€§åŒºåŸŸæ ¡å‡†ä¸è¶³çš„é—®é¢˜
#     """
#     quantiles = np.percentile(unc_raw, np.linspace(0, 100, n_bins + 1))
#     scales = []
#     bin_edges = []
    
#     print(f"{'åŒºé—´':<15} {'æ ·æœ¬æ•°':<8} {'åŸå§‹ä¸ç¡®å®š':<12} {'å®é™…è¯¯å·®':<12} {'ç¼©æ”¾å› å­':<10}")
#     print("-" * 70)
    
#     for i in range(n_bins):
#         low, high = quantiles[i], quantiles[i+1]
#         bin_edges.append((low, high))
#         mask = (unc_raw >= low) & (unc_raw <= high)
#         n_samples = mask.sum()
        
#         if n_samples > 10:
#             def picp_with_scale(s):
#                 z = norm.ppf((1 + target_coverage) / 2)
#                 lower = y_pred[mask] - z * s * unc_raw[mask]
#                 upper = y_pred[mask] + z * s * unc_raw[mask]
#                 return np.mean((y_true[mask] >= lower) & (y_true[mask] <= upper))
            
#             try:
#                 s_opt = brentq(lambda s: picp_with_scale(s) - target_coverage, 0.1, 100)
#             except:
#                 test_scales = np.linspace(0.1, 100, 500)
#                 picps = [picp_with_scale(s) for s in test_scales]
#                 s_opt = test_scales[np.argmin(np.abs(np.array(picps) - target_coverage))]
#             scales.append(s_opt)
            
#             print(f"[{low:.2f}, {high:.2f}]  "
#                   f"{n_samples:<8} {unc_raw[mask].mean():>10.2f}s  "
#                   f"{np.abs(y_true[mask]-y_pred[mask]).mean():>10.2f}s  {s_opt:>8.2f}x")
#         else:
#             scales.append(1.0)
#             print(f"[{low:.2f}, {high:.2f}]  "
#                   f"{n_samples:<8} {'-':>10}  {'-':>10}  {1.0:>8.2f}x")
    
#     # åº”ç”¨åˆ†å±‚ç¼©æ”¾
#     unc_cal = unc_raw.copy()
#     for i, (low, high) in enumerate(bin_edges):
#         mask = (unc_raw >= low) & (unc_raw <= high)
#         unc_cal[mask] = unc_raw[mask] * scales[i]
    
#     return unc_cal, scales, bin_edges

# def apply_hierarchical_calibration(unc_raw, bin_edges, scales):
#     """å°†éªŒè¯é›†å­¦åˆ°çš„åˆ†å±‚æ ¡å‡†åº”ç”¨åˆ°æµ‹è¯•é›†"""
#     unc_cal = unc_raw.copy()
#     for i, (low, high) in enumerate(bin_edges):
#         mask = (unc_raw >= low) & (unc_raw <= high)
#         unc_cal[mask] = unc_raw[mask] * scales[i]
#     return unc_cal

# def post_hoc_calibration(y_true, y_pred, unc_raw, target_coverage=0.8, search_range=(0.1, 100)):
#     """å…¨å±€å•å› å­æ ¡å‡†ï¼ˆä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰"""
#     def picp_with_scale(s):
#         z = norm.ppf((1 + target_coverage) / 2)
#         lower = y_pred - z * s * unc_raw
#         upper = y_pred + z * s * unc_raw
#         return np.mean((y_true >= lower) & (y_true <= upper))
#     s_min, s_max = search_range
#     try:
#         s_opt = brentq(lambda s: picp_with_scale(s) - target_coverage, s_min, s_max)
#         return s_opt
#     except:
#         scales = np.linspace(s_min, s_max, 500)
#         picps = [picp_with_scale(s) for s in scales]
#         best_idx = np.argmin(np.abs(np.array(picps) - target_coverage))
#         return scales[best_idx]

# # ==============================================================================
# # 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
# # ==============================================================================
# def load_preprocessing_objects():
#     with open('preprocessing_objects.pkl', 'rb') as f:
#         prep = pickle.load(f)
#     return prep

# def load_data():
#     df_exp = pd.read_excel("cts_data.xlsx")
#     df_feat = pd.read_csv("image_features_database.csv")
#     rename_map = {
#         "image": "image_name", "method": "algo_name",
#         "network_bw": "bandwidth_mbps", "network_delay": "network_rtt",
#         "mem_limit": "mem_limit_mb"
#     }
#     df_exp = df_exp.rename(columns=rename_map)
#     if 'total_time' not in df_exp.columns:
#         cols = [c for c in df_exp.columns if 'total_tim' in c]
#         if cols:
#             df_exp = df_exp.rename(columns={cols[0]: 'total_time'})
#     df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
#     df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
#     cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
#     target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std',
#                    'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
#     cols_i = [c for c in target_cols if c in df.columns]
#     Xc_raw = df[cols_c].values
#     Xi_raw = df[cols_i].values
#     y_raw_log = np.log1p(df['total_time'].values)
#     y_raw_orig = df['total_time'].values
#     algo_names_raw = df['algo_name'].values
#     return Xc_raw, Xi_raw, algo_names_raw, y_raw_log, cols_c, cols_i, y_raw_orig

# # ==============================================================================
# # 4. è¯„ä¼°ä¸»ç±»ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰
# # ==============================================================================
# class ModelEvaluator:
#     def __init__(self, model_path, seed=42):
#         self.seed = seed
#         np.random.seed(seed)
#         self.prep = load_preprocessing_objects()
#         self.scaler_c = self.prep['scaler_c']
#         self.scaler_i = self.prep['scaler_i']
#         self.enc = self.prep['enc']
#         self.cols_c = self.prep.get('cols_c', ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb'])
#         self.cols_i = self.prep.get('cols_i', ['total_size_mb', 'avg_layer_entropy', 'layer_count', 'text_ratio', 'zero_ratio'])
#         self.default_algo = self.prep.get('most_common_algo', self.enc.classes_[0])
#         self.default_idx = self.enc.transform([self.default_algo])[0]
        
#         # åŠ è½½æ•°æ®
#         Xc_raw, Xi_raw, algo_names_raw, y_log, _, _, y_orig = load_data()
#         N = len(y_log)
#         idx = np.random.permutation(N)
#         n_tr = int(N * 0.7)
#         n_val = int(N * 0.15)
#         self.tr_idx = idx[:n_tr]
#         self.val_idx = idx[n_tr:n_tr+n_val]
#         self.te_idx = idx[n_tr+n_val:]
        
#         # æ ‡å‡†åŒ–
#         self.Xc_train = self.scaler_c.transform(Xc_raw[self.tr_idx])
#         self.Xc_val = self.scaler_c.transform(Xc_raw[self.val_idx])
#         self.Xc_test = self.scaler_c.transform(Xc_raw[self.te_idx])
#         self.Xi_train = self.scaler_i.transform(Xi_raw[self.tr_idx])
#         self.Xi_val = self.scaler_i.transform(Xi_raw[self.val_idx])
#         self.Xi_test = self.scaler_i.transform(Xi_raw[self.te_idx])
        
#         # ç®—æ³•ç¼–ç 
#         def safe_transform(labels):
#             known = set(self.enc.classes_)
#             return np.array([self.enc.transform([l])[0] if l in known else self.default_idx for l in labels])
        
#         self.Xa_train = self.enc.transform(algo_names_raw[self.tr_idx])
#         self.Xa_val = safe_transform(algo_names_raw[self.val_idx])
#         self.Xa_test = safe_transform(algo_names_raw[self.te_idx])
        
#         self.y_train_log = y_log[self.tr_idx]
#         self.y_val_log = y_log[self.val_idx]
#         self.y_test_log = y_log[self.te_idx]
#         self.y_train_orig = y_orig[self.tr_idx]
#         self.y_val_orig = y_orig[self.val_idx]
#         self.y_test_orig = y_orig[self.te_idx]
        
#         # åŸºçº¿æ¨¡å‹ä½¿ç”¨One-Hotç¼–ç ç®—æ³•ç‰¹å¾ï¼Œé¿å…æ•°å€¼é¡ºåºè¯¯å¯¼
#         self.algo_onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
#         self.algo_onehot.fit(self.Xa_train.reshape(-1, 1))
        
#         Xa_train_oh = self.algo_onehot.transform(self.Xa_train.reshape(-1, 1))
#         Xa_val_oh = self.algo_onehot.transform(self.Xa_val.reshape(-1, 1))
#         Xa_test_oh = self.algo_onehot.transform(self.Xa_test.reshape(-1, 1))
        
#         self.X_train_comb = np.hstack([self.Xc_train, self.Xi_train, Xa_train_oh])
#         self.X_val_comb = np.hstack([self.Xc_val, self.Xi_val, Xa_val_oh])
#         self.X_test_comb = np.hstack([self.Xc_test, self.Xi_test, Xa_test_oh])
        
#         print(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒ {len(self.tr_idx)} | éªŒè¯ {len(self.val_idx)} | æµ‹è¯• {len(self.te_idx)}")
#         print(f"åŸºçº¿æ¨¡å‹ç‰¹å¾ç»´åº¦: {self.X_train_comb.shape[1]} (åŒ…å«{len(self.enc.classes_)}ä¸ªç®—æ³•çš„One-Hotç¼–ç )")
        
#         # åŠ è½½CFT-Netæ¨¡å‹ï¼ˆä¸è®­ç»ƒå®Œå…¨ä¸€è‡´çš„æ¶æ„ï¼‰
#         self.embed_dim = MODEL_CONFIG['embed_dim']
#         self.cftnet = CompactCFTNet(
#             client_feats=len(self.cols_c), 
#             image_feats=len(self.cols_i), 
#             num_algos=len(self.enc.classes_), 
#             embed_dim=self.embed_dim
#         ).to(device)
        
#         # åŠ è½½æƒé‡ï¼Œå…¼å®¹ä¸¤ç§ä¿å­˜æ ¼å¼
#         checkpoint = torch.load(model_path, map_location=device)
#         if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
#             state_dict = checkpoint['model_state_dict']
#         else:
#             state_dict = checkpoint
#         self.cftnet.load_state_dict(state_dict)
#         self.cftnet.eval()
#         print("âœ… CFT-Net V2 æ¨¡å‹åŠ è½½æˆåŠŸ")
        
#         self.results = {}
#         self.calibration_params = {}
    
#     def predict_cftnet(self, Xc, Xi, Xa):
#         """
#         æ‰¹é‡é¢„æµ‹ï¼Œä½¿ç”¨Delta Methodä¿®æ­£åŸå§‹ç©ºé—´ä¸ç¡®å®šæ€§
#         ä¸è®­ç»ƒæ—¶çš„è®¡ç®—é€»è¾‘å®Œå…¨ä¸€è‡´
#         """
#         batch_size = 1024
#         n = len(Xc)
#         preds_orig = []
#         uncs_log = []
#         uncs_orig = []
        
#         with torch.no_grad():
#             for i in range(0, n, batch_size):
#                 cx = torch.FloatTensor(Xc[i:i+batch_size]).to(device)
#                 ix = torch.FloatTensor(Xi[i:i+batch_size]).to(device)
#                 ax = torch.LongTensor(Xa[i:i+batch_size]).to(device)
                
#                 out = self.cftnet(cx, ix, ax)
#                 gamma, v, alpha, beta = out[:, 0], out[:, 1], out[:, 2], out[:, 3]
                
#                 # 1. å‡å€¼è½¬æ¢å›åŸå§‹ç©ºé—´
#                 pred_log = gamma
#                 pred_orig = torch.expm1(pred_log)
                
#                 # 2. ä¸ç¡®å®šæ€§ä¼ æ’­ï¼ˆDelta Methodï¼‰
#                 # Var(exp(x)-1) â‰ˆ (exp(x))Â² * Var(x)
#                 var_log = beta / (v * (alpha - 1) + 1e-6)
#                 std_log = torch.sqrt(var_log + 1e-6)
#                 std_orig = torch.exp(pred_log) * std_log  # æ ¸å¿ƒä¿®æ­£ï¼šå°ºåº¦å¯¹é½
                
#                 preds_orig.append(pred_orig.cpu().numpy())
#                 uncs_log.append(std_log.cpu().numpy())
#                 uncs_orig.append(std_orig.cpu().numpy())
        
#         return np.concatenate(preds_orig), np.concatenate(uncs_orig), np.concatenate(uncs_log)
    
#     def calibrate_cftnet(self):
#         """åœ¨éªŒè¯é›†ä¸Šå­¦ä¹ æ ¡å‡†å‚æ•°"""
#         print("\n" + "="*60)
#         print("ğŸ”§ CFT-Net äº‹åæ ¡å‡†ï¼ˆéªŒè¯é›†ï¼‰")
#         print("="*60)
        
#         pred_val, unc_val_raw, _ = self.predict_cftnet(self.Xc_val, self.Xi_val, self.Xa_val)
#         picp_val_raw, _ = calculate_picp_mpiw(self.y_val_orig, pred_val, unc_val_raw, 0.8)
#         print(f"éªŒè¯é›†åŸå§‹PICP: {picp_val_raw:.1f}%")
        
#         # ä¼˜å…ˆä½¿ç”¨åˆ†å±‚æ ¡å‡†
#         print("\n--- åˆ†å±‚æ ¡å‡†å­¦ä¹  ---")
#         unc_val_cal, scales, bin_edges = hierarchical_calibration(
#             self.y_val_orig, pred_val, unc_val_raw, target_coverage=0.8, n_bins=5
#         )
#         picp_val_cal, _ = calculate_picp_mpiw(self.y_val_orig, pred_val, unc_val_cal, 0.8)
#         print(f"åˆ†å±‚æ ¡å‡†åPICP: {picp_val_cal:.1f}%")
        
#         # ä¿å­˜æ ¡å‡†å‚æ•°
#         self.calibration_params = {
#             'hierarchical_scales': scales,
#             'bin_edges': bin_edges,
#             'global_scale': post_hoc_calibration(self.y_val_orig, pred_val, unc_val_raw)
#         }
#         print(f"å…¨å±€æ ¡å‡†ç¼©æ”¾å› å­: {self.calibration_params['global_scale']:.3f}")
#         print("="*60)
        
#         return self.calibration_params
    
#     def evaluate_cftnet(self):
#         """CFT-Net å®Œæ•´æµ‹è¯•é›†è¯„ä¼°"""
#         pred_test, unc_test_raw, _ = self.predict_cftnet(self.Xc_test, self.Xi_test, self.Xa_test)
        
#         # åº”ç”¨åˆ†å±‚æ ¡å‡†
#         unc_test_cal = apply_hierarchical_calibration(
#             unc_test_raw, 
#             self.calibration_params['bin_edges'], 
#             self.calibration_params['hierarchical_scales']
#         )
        
#         errors_test = np.abs(self.y_test_orig - pred_test)
        
#         # å…¨é‡æŒ‡æ ‡è®¡ç®—
#         mae = mean_absolute_error(self.y_test_orig, pred_test)
#         rmse = np.sqrt(mean_squared_error(self.y_test_orig, pred_test))
#         smape = calculate_smape(self.y_test_orig, pred_test)
#         mape = calculate_mape(self.y_test_orig, pred_test)
#         r2 = r2_score(self.y_test_orig, pred_test)
        
#         # ä¸ç¡®å®šæ€§æŒ‡æ ‡
#         corr, _ = spearmanr(unc_test_cal, errors_test)
#         corr = 0.0 if np.isnan(corr) else corr
#         picp, mpiw = calculate_picp_mpiw(self.y_test_orig, pred_test, unc_test_cal, 0.8)
#         ece = calculate_ece_quantile(errors_test, unc_test_cal)
        
#         # æ¨ç†æ—¶é—´ï¼ˆç»Ÿä¸€åœ¨CPUä¸Šæµ‹é‡ï¼Œå…¬å¹³å¯¹æ¯”ï¼‰
#         infer_time = self.measure_inference_time_cftnet_cpu()
        
#         # å‚æ•°é‡ç»Ÿè®¡
#         params_k = sum(p.numel() for p in self.cftnet.parameters()) / 1000
        
#         self.results['CFT-Net'] = {
#             'MAE': mae, 'RMSE': rmse, 'sMAPE': smape, 'MAPE': mape,
#             'R2': r2, 'Corr': corr, 'PICP_80': picp, 'MPIW_80': mpiw, 'ECE': ece,
#             'Inference_ms': infer_time * 1000,
#             'Params_K': params_k,
#             'predictions': pred_test,
#             'uncertainties': unc_test_cal,
#             'raw_uncertainties': unc_test_raw
#         }
        
#         print(f"\nâœ… CFT-Net æµ‹è¯•é›†è¯„ä¼°å®Œæˆ")
#         print(f"  ç²¾åº¦æŒ‡æ ‡: sMAPE={smape:.2f}%, RMSE={rmse:.2f}s, RÂ²={r2:.4f}")
#         print(f"  ä¸ç¡®å®šæ€§: Corr={corr:.3f}, PICP={picp:.1f}%, MPIW={mpiw:.2f}s, ECE={ece:.3f}")
#         print(f"  æ¨ç†æ€§èƒ½: å•æ ·æœ¬æ¨ç†={infer_time*1000:.3f}ms, å‚æ•°é‡={params_k:.1f}K")
        
#         return self.results['CFT-Net']
    
#     def measure_inference_time_cftnet_cpu(self):
#         """åœ¨CPUä¸Šæµ‹é‡CFT-Netæ¨ç†æ—¶é—´ï¼Œç¡®ä¿ä¸åŸºçº¿æ¨¡å‹å…¬å¹³å¯¹æ¯”"""
#         self.cftnet.cpu()
#         batch_size = 256
#         n = len(self.Xc_test)
        
#         # Warmup
#         with torch.no_grad():
#             for i in range(0, min(500, n), batch_size):
#                 cx = torch.FloatTensor(self.Xc_test[i:i+batch_size])
#                 ix = torch.FloatTensor(self.Xi_test[i:i+batch_size])
#                 ax = torch.LongTensor(self.Xa_test[i:i+batch_size])
#                 _ = self.cftnet(cx, ix, ax)
        
#         # æ­£å¼è®¡æ—¶
#         times = []
#         with torch.no_grad():
#             for i in range(0, n, batch_size):
#                 cx = torch.FloatTensor(self.Xc_test[i:i+batch_size])
#                 ix = torch.FloatTensor(self.Xi_test[i:i+batch_size])
#                 ax = torch.LongTensor(self.Xa_test[i:i+batch_size])
                
#                 start = time.perf_counter()
#                 _ = self.cftnet(cx, ix, ax)
#                 times.append(time.perf_counter() - start)
        
#         # ç§»å›åŸè®¾å¤‡
#         self.cftnet.to(device)
        
#         total_time = np.sum(times)
#         return total_time / n
    
#     def train_baselines(self):
#         """è®­ç»ƒå¹¶è¯„ä¼°åŸºçº¿æ¨¡å‹ï¼ˆRandomForest/XGBoost/LightGBMï¼‰"""
#         models = {
#             'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=self.seed, n_jobs=-1),
#             'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=self.seed, n_jobs=-1),
#             'LightGBM': lgb.LGBMRegressor(n_estimators=100, num_leaves=31, learning_rate=0.1, random_state=self.seed, n_jobs=-1, verbose=-1)
#         }
#         print("\nğŸš€ è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä½¿ç”¨One-Hotç¼–ç ç®—æ³•ç‰¹å¾ï¼‰...")
        
#         for name, model in models.items():
#             print(f"  è®­ç»ƒ {name}...")
#             start = time.perf_counter()
#             model.fit(self.X_train_comb, self.y_train_log)
#             train_time = time.perf_counter() - start
            
#             # é¢„æµ‹å¹¶è½¬æ¢å›åŸå§‹ç©ºé—´
#             pred_log = model.predict(self.X_test_comb)
#             pred_orig = np.expm1(pred_log)
            
#             # ç²¾åº¦æŒ‡æ ‡
#             mae = mean_absolute_error(self.y_test_orig, pred_orig)
#             rmse = np.sqrt(mean_squared_error(self.y_test_orig, pred_orig))
#             smape = calculate_smape(self.y_test_orig, pred_orig)
#             mape = calculate_mape(self.y_test_orig, pred_orig)
#             r2 = r2_score(self.y_test_orig, pred_orig)
            
#             # æ¨ç†æ—¶é—´
#             infer_time = self.measure_inference_time_sklearn(model, self.X_test_comb)
            
#             self.results[name] = {
#                 'MAE': mae, 'RMSE': rmse, 'sMAPE': smape, 'MAPE': mape, 'R2': r2,
#                 'Corr': None, 'PICP_80': None, 'MPIW_80': None, 'ECE': None,
#                 'Inference_ms': infer_time * 1000,
#                 'Params_K': None,
#                 'predictions': pred_orig
#             }
#             print(f"    å®Œæˆ: RÂ²={r2:.4f}, sMAPE={smape:.2f}%, å•æ ·æœ¬æ¨ç†={infer_time*1000:.3f}ms")
    
#     def measure_inference_time_sklearn(self, model, X):
#         """æµ‹é‡sklearnç³»åˆ—æ¨¡å‹çš„æ¨ç†æ—¶é—´"""
#         batch_size = 256
#         n = len(X)
#         times = []
#         for i in range(0, n, batch_size):
#             X_batch = X[i:i+batch_size]
#             start = time.perf_counter()
#             _ = model.predict(X_batch)
#             times.append(time.perf_counter() - start)
#         total_time = np.sum(times)
#         return total_time / n
    
#     def generate_comparison_table(self):
#         """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆCSV+LaTeXï¼‰"""
#         rows = []
#         for model, metrics in self.results.items():
#             row = {
#                 'Model': model,
#                 'R2': f"{metrics['R2']:.4f}" if metrics.get('R2') is not None else '-',
#                 'sMAPE(%)': f"{metrics['sMAPE']:.2f}",
#                 'MAE(s)': f"{metrics['MAE']:.2f}",
#                 'RMSE(s)': f"{metrics['RMSE']:.2f}",
#                 'Corr': f"{metrics['Corr']:.3f}" if metrics['Corr'] is not None else '-',
#                 'PICP-80(%)': f"{metrics['PICP_80']:.1f}" if metrics['PICP_80'] is not None else '-',
#                 'MPIW(s)': f"{metrics['MPIW_80']:.2f}" if metrics['MPIW_80'] is not None else '-',
#                 'ECE': f"{metrics['ECE']:.3f}" if metrics['ECE'] is not None else '-',
#                 'Params(K)': f"{metrics['Params_K']:.1f}" if metrics.get('Params_K') else '-',
#                 'Time(ms)': f"{metrics['Inference_ms']:.3f}"
#             }
#             rows.append(row)
        
#         df = pd.DataFrame(rows)
#         df.to_csv('evaluation_results/comparison_table.csv', index=False)
        
#         print("\n" + "="*120)
#         print("ğŸ“Š æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”è¡¨")
#         print("="*120)
#         print(df.to_string(index=False))
#         print("="*120)
        
#         # ç”ŸæˆLaTeXè¡¨æ ¼
#         latex = self._generate_latex_table(rows)
#         with open('evaluation_results/comparison_table.tex', 'w') as f:
#             f.write(latex)
#         print("LaTeXè¡¨æ ¼å·²ä¿å­˜è‡³ evaluation_results/comparison_table.tex")
        
#         return df
    
#     def _generate_latex_table(self, rows):
#         """ç”Ÿæˆè®ºæ–‡ç”¨LaTeXè¡¨æ ¼"""
#         latex = r"""\begin{table}[htbp]
# \centering
# \caption{æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”}
# \label{tab:model_comparison}
# \resizebox{\textwidth}{!}{
# \begin{tabular}{lccccccccc}
# \toprule
# \textbf{Model} & \textbf{R\textsuperscript{2}} & \textbf{sMAPE(\%)} & \textbf{RMSE(s)} & \textbf{Corr} & \textbf{PICP-80(\%)} & \textbf{MPIW(s)} & \textbf{ECE} & \textbf{Params(K)} & \textbf{Time(ms)} \\
# \midrule
# """
#         for row in rows:
#             latex += f"{row['Model']} & {row['R2']} & {row['sMAPE(%)']} & {row['RMSE(s)']} & {row['Corr']} & {row['PICP-80(%)']} & {row['MPIW(s)']} & {row['ECE']} & {row['Params(K)']} & {row['Time(ms)']} \\\\\n"
        
#         latex += r"""\bottomrule
# \end{tabular}
# }
# \begin{tablenotes}
# \footnotesize
# \item[1] R\textsuperscript{2} æ¥è¿‘1.0æºäºä¼ è¾“æ—¶é—´çš„å¼ºç‰©ç†ç¡®å®šæ€§ï¼ˆæ–‡ä»¶å¤§å°/å¸¦å®½ï¼‰ã€‚
# \item[2] Corrã€PICPã€MPIWã€ECE ä¸ºä¸ç¡®å®šæ€§é‡åŒ–ä¸“å±æŒ‡æ ‡ï¼Œä¼ ç»Ÿæ ‘æ¨¡å‹æ— æ³•æä¾›ã€‚
# \end{tablenotes}
# \end{table}"""
#         return latex
    
#     def generate_radar_chart(self):
#         """ç”Ÿæˆæ¨¡å‹ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾"""
#         models = list(self.results.keys())
#         # é›·è¾¾å›¾ç»´åº¦ï¼šç²¾åº¦ã€é£é™©æ„ŸçŸ¥ã€å¯é æ€§ã€è½»é‡åŒ–ã€æ¨ç†é€Ÿåº¦
#         smapes = [self.results[m]['sMAPE'] for m in models]
#         corrs = [self.results[m]['Corr'] if self.results[m]['Corr'] is not None else 0 for m in models]
#         picps = [self.results[m]['PICP_80'] if self.results[m]['PICP_80'] is not None else 0 for m in models]
#         inf_times = [self.results[m]['Inference_ms'] for m in models]
#         params = [self.results[m]['Params_K'] if self.results[m]['Params_K'] is not None else 1000 for m in models]
        
#         # å½’ä¸€åŒ–ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
#         smape_norm = [max(0, 1 - s/50) for s in smapes]  # sMAPEè¶Šå°è¶Šå¥½
#         corr_norm = [max(0, c) for c in corrs]  # Corrè¶Šå¤§è¶Šå¥½
#         picp_norm = [p/100 for p in picps]  # PICPè¶Šå¤§è¶Šå¥½
#         inf_norm = [1 - t/max(inf_times) for t in inf_times]  # æ¨ç†æ—¶é—´è¶Šå°è¶Šå¥½
#         param_norm = [1 - p/max(params) for p in params]  # å‚æ•°é‡è¶Šå°è¶Šå¥½
        
#         categories = [
#             'é¢„æµ‹ç²¾åº¦\n(sMAPEâ†“)', 
#             'é£é™©æ„ŸçŸ¥\n(Corrâ†‘)', 
#             'å¯é æ€§\n(PICPâ†‘)', 
#             'æ¨ç†é€Ÿåº¦\n(Timeâ†“)',
#             'è½»é‡åŒ–\n(Paramsâ†“)'
#         ]
#         N = len(categories)
#         angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
#         angles += angles[:1]
        
#         fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))
        
#         # é…è‰²ï¼šCFT-Netç”¨çªå‡ºçš„ç»¿è‰²ï¼ŒåŸºçº¿ç”¨ç°è‰²ç³»
#         colors = ['#808080', '#808080', '#808080', '#2ca02c'] if len(models) == 4 else ['#808080']*(len(models)-1) + ['#2ca02c']
        
#         for i, model in enumerate(models):
#             values = [smape_norm[i], corr_norm[i], picp_norm[i], inf_norm[i], param_norm[i]]
#             values += values[:1]
#             linewidth = 3 if model == 'CFT-Net' else 1.5
#             alpha = 0.2 if model == 'CFT-Net' else 0.05
#             ax.plot(angles, values, 'o-', linewidth=linewidth, label=model, color=colors[i])
#             ax.fill(angles, values, alpha=alpha, color=colors[i])
        
#         ax.set_xticks(angles[:-1])
#         ax.set_xticklabels(categories, fontsize=13, fontweight='bold')
#         ax.set_ylim(0, 1)
#         ax.set_title('æ¨¡å‹ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾\nï¼ˆCFT-Net vs ä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼‰', fontsize=16, fontweight='bold', pad=40)
#         ax.legend(loc='upper right', bbox_to_anchor=(1.5, 1.1), fontsize=12)
#         plt.tight_layout()
#         plt.savefig('evaluation_results/radar_chart.png', dpi=300, bbox_inches='tight')
#         plt.close()
#         print("âœ… é›·è¾¾å›¾å·²ä¿å­˜è‡³ evaluation_results/radar_chart.png")
    
#     def plot_calibration_curve(self):
#         """ç»˜åˆ¶æ ¡å‡†æ›²çº¿ä¸æ®‹å·®åˆ†å¸ƒ"""
#         if 'CFT-Net' not in self.results:
#             return
        
#         preds = self.results['CFT-Net']['predictions']
#         uncs = self.results['CFT-Net']['uncertainties']
#         errors = np.abs(self.y_test_orig - preds)
        
#         # åˆ†ç®±è®¡ç®—æ ¡å‡†æ›²çº¿
#         n_bins = 10
#         quantiles = np.linspace(0, 100, n_bins + 1)
#         bin_edges = np.percentile(uncs, quantiles)
#         bin_edges[-1] += 1e-8
        
#         bin_centers = []
#         avg_errors = []
#         avg_uncertainties = []
        
#         for i in range(n_bins):
#             in_bin = (uncs >= bin_edges[i]) & (uncs < bin_edges[i+1])
#             if i == n_bins - 1:
#                 in_bin = (uncs >= bin_edges[i]) & (uncs <= bin_edges[i+1])
#             if in_bin.sum() > 0:
#                 bin_centers.append((bin_edges[i] + bin_edges[i+1]) / 2)
#                 avg_errors.append(errors[in_bin].mean())
#                 avg_uncertainties.append(uncs[in_bin].mean())
        
#         fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
#         # å·¦å›¾ï¼šæ ¡å‡†æ›²çº¿
#         ax1.plot(avg_uncertainties, avg_errors, 'o-', linewidth=2, markersize=8, label='å®é™…ç»å¯¹è¯¯å·®', color='#2ca02c')
#         ax1.plot(avg_uncertainties, avg_uncertainties, 'r--', linewidth=2, label='å®Œç¾æ ¡å‡†çº¿')
#         ax1.fill_between(avg_uncertainties, avg_errors, avg_uncertainties, alpha=0.2, color='red')
#         ax1.set_xlabel('å¹³å‡é¢„æµ‹ä¸ç¡®å®šæ€§ (s)', fontsize=12)
#         ax1.set_ylabel('å¹³å‡ç»å¯¹è¯¯å·® (s)', fontsize=12)
#         ax1.set_title('CFT-Net æ ¡å‡†æ›²çº¿', fontsize=14, fontweight='bold')
#         ax1.legend(fontsize=11)
#         ax1.grid(alpha=0.3)
        
#         # å³å›¾ï¼šæ®‹å·®åˆ†å¸ƒ
#         residuals = self.y_test_orig - preds
#         ax2.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='#2ca02c')
#         ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)
#         ax2.set_xlabel('é¢„æµ‹æ®‹å·® (çœŸå®å€¼-é¢„æµ‹å€¼, s)', fontsize=12)
#         ax2.set_ylabel('æ ·æœ¬é¢‘æ•°', fontsize=12)
#         ax2.set_title('é¢„æµ‹æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
#         ax2.grid(alpha=0.3)
        
#         plt.tight_layout()
#         plt.savefig('evaluation_results/calibration_analysis.png', dpi=300)
#         plt.close()
#         print("âœ… æ ¡å‡†åˆ†æå›¾å·²ä¿å­˜è‡³ evaluation_results/calibration_analysis.png")
    
#     def plot_prediction_intervals(self):
#         """ç»˜åˆ¶å…¨é‡æµ‹è¯•é›†é¢„æµ‹åŒºé—´å›¾"""
#         if 'CFT-Net' not in self.results:
#             return
        
#         # ä½¿ç”¨å…¨é‡æµ‹è¯•é›†ï¼ŒæŒ‰çœŸå®å€¼æ’åºä¾¿äºè§‚å¯Ÿ
#         indices = np.argsort(self.y_test_orig)
#         n_samples = len(indices)
        
#         preds = self.results['CFT-Net']['predictions'][indices]
#         uncs = self.results['CFT-Net']['uncertainties'][indices]
#         y_true = self.y_test_orig[indices]
        
#         # 80%ç½®ä¿¡åŒºé—´
#         z = norm.ppf((1 + 0.8) / 2)
#         lower = preds - z * uncs
#         upper = preds + z * uncs
        
#         # è®¡ç®—å®é™…PICP
#         covered = (y_true >= lower) & (y_true <= upper)
#         picp_actual = covered.mean() * 100
#         not_covered_count = (~covered).sum()
        
#         plt.figure(figsize=(16, 7))
#         x = np.arange(n_samples)
        
#         # ç»˜åˆ¶é¢„æµ‹åŒºé—´ã€é¢„æµ‹å€¼ã€çœŸå®å€¼
#         plt.fill_between(x, lower, upper, alpha=0.3, color='#1f77b4', label='80%é¢„æµ‹åŒºé—´')
#         plt.plot(x, preds, 'b-', linewidth=1.2, label='é¢„æµ‹å€¼', alpha=0.8)
#         plt.scatter(x, y_true, c='black', s=2, zorder=5, label='çœŸå®å€¼', alpha=0.4)
        
#         # æ ‡è®°æœªè¦†ç›–çš„ç‚¹
#         not_covered_idx = np.where(~covered)[0]
#         if len(not_covered_idx) > 0:
#             # æ ·æœ¬è¿‡å¤šæ—¶éšæœºé‡‡æ ·æ˜¾ç¤ºï¼Œé¿å…è¿‡äºå¯†é›†
#             display_count = min(200, len(not_covered_idx))
#             np.random.seed(self.seed)
#             display_idx = np.random.choice(not_covered_idx, display_count, replace=False)
#             plt.scatter(display_idx, y_true[display_idx], c='red', s=25, marker='x', 
#                        linewidth=2, label=f'æœªè¦†ç›–æ ·æœ¬ (n={not_covered_count})', zorder=6)
        
#         plt.xlabel('æ ·æœ¬ç´¢å¼•ï¼ˆæŒ‰çœŸå®ä¼ è¾“æ—¶é—´å‡åºæ’åˆ—ï¼‰', fontsize=12)
#         plt.ylabel('ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
#         plt.title(f'CFT-Net å…¨é‡æµ‹è¯•é›†é¢„æµ‹åŒºé—´å¯è§†åŒ– (n={n_samples}, å®é™…PICP={picp_actual:.1f}%)', 
#                  fontsize=14, fontweight='bold')
#         plt.legend(fontsize=11, loc='upper left')
#         plt.grid(alpha=0.3)
#         plt.tight_layout()
#         plt.savefig('evaluation_results/prediction_intervals.png', dpi=300)
#         plt.close()
#         print(f"âœ… é¢„æµ‹åŒºé—´å›¾å·²ä¿å­˜ (å…¨é‡{n_samples}ä¸ªæ ·æœ¬, å®é™…PICP={picp_actual:.1f}%)")
    
#     def plot_pred_vs_actual(self):
#         """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾"""
#         if 'CFT-Net' not in self.results:
#             return
        
#         fig, axes = plt.subplots(2, 2, figsize=(14, 12))
#         fig.suptitle('é¢„æµ‹å€¼ vs çœŸå®å€¼å¯¹æ¯” (Prediction vs Actual)', fontsize=16, fontweight='bold', y=0.98)
        
#         models_to_plot = ['CFT-Net', 'RandomForest', 'XGBoost', 'LightGBM']
#         colors = ['#2ca02c', '#808080', '#808080', '#808080']
        
#         for idx, (model, color) in enumerate(zip(models_to_plot, colors)):
#             if model not in self.results:
#                 continue
            
#             ax = axes[idx // 2, idx % 2]
#             preds = self.results[model]['predictions']
#             y_true = self.y_test_orig
            
#             # æ ¸å¿ƒæŒ‡æ ‡
#             r2 = self.results[model]['R2']
#             smape = self.results[model]['sMAPE']
            
#             # æ•£ç‚¹å›¾
#             ax.scatter(y_true, preds, alpha=0.4, s=10, c=color, edgecolors='none')
            
#             # å®Œç¾é¢„æµ‹çº¿
#             min_val = min(y_true.min(), preds.min())
#             max_val = max(y_true.max(), preds.max())
#             ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹çº¿')
            
#             # Â±20%è¯¯å·®å¸¦
#             ax.plot([min_val, max_val], [min_val*0.8, max_val*0.8], 'k:', linewidth=1, alpha=0.5, label='Â±20%è¯¯å·®å¸¦')
#             ax.plot([min_val, max_val], [min_val*1.2, max_val*1.2], 'k:', linewidth=1, alpha=0.5)
            
#             ax.set_xlabel('çœŸå®ä¼ è¾“æ—¶é—´ (s)', fontsize=11)
#             ax.set_ylabel('é¢„æµ‹ä¼ è¾“æ—¶é—´ (s)', fontsize=11)
#             ax.set_title(f'{model}\nRÂ²={r2:.4f}, sMAPE={smape:.2f}%', fontsize=12, fontweight='bold')
#             ax.legend(loc='upper left', fontsize=9)
#             ax.grid(alpha=0.3)
#             ax.set_xlim(min_val, max_val)
#             ax.set_ylim(min_val, max_val)
        
#         plt.tight_layout()
#         plt.savefig('evaluation_results/pred_vs_actual_all.png', dpi=300, bbox_inches='tight')
#         plt.close()
        
#         # ç»˜åˆ¶CFT-Netä¸“å±å¸¦ä¸ç¡®å®šæ€§ç€è‰²çš„ç‰ˆæœ¬
#         self._plot_cftnet_detailed_scatter()
#         print("âœ… Pred vs Actual å¯¹æ¯”å›¾å·²ä¿å­˜")
    
#     def _plot_cftnet_detailed_scatter(self):
#         """CFT-Netä¸“å±æ•£ç‚¹å›¾ï¼ŒæŒ‰ä¸ç¡®å®šæ€§å¤§å°ç€è‰²"""
#         fig, ax = plt.subplots(figsize=(10, 10))
        
#         preds = self.results['CFT-Net']['predictions']
#         uncs = self.results['CFT-Net']['uncertainties']
#         y_true = self.y_test_orig
        
#         # æŒ‰ä¸ç¡®å®šæ€§ç€è‰²
#         scatter = ax.scatter(y_true, preds, c=uncs, cmap='viridis', alpha=0.6, s=15, edgecolors='none')
#         plt.colorbar(scatter, ax=ax, label='é¢„æµ‹ä¸ç¡®å®šæ€§ (s)')
        
#         # å®Œç¾é¢„æµ‹çº¿
#         min_val = min(y_true.min(), preds.min())
#         max_val = max(y_true.max(), preds.max())
#         ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹çº¿')
        
#         # æŒ‡æ ‡æ ‡æ³¨
#         r2 = self.results['CFT-Net']['R2']
#         smape = self.results['CFT-Net']['sMAPE']
#         corr = self.results['CFT-Net']['Corr']
#         picp = self.results['CFT-Net']['PICP_80']
        
#         ax.set_xlabel('çœŸå®ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
#         ax.set_ylabel('é¢„æµ‹ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
#         ax.set_title(
#             f'CFT-Net é¢„æµ‹è¯¦æƒ…ï¼ˆæŒ‰ä¸ç¡®å®šæ€§ç€è‰²ï¼‰\n'
#             f'RÂ²={r2:.4f}, sMAPE={smape:.2f}%, Corr={corr:.3f}, PICP={picp:.1f}%', 
#             fontsize=13, fontweight='bold'
#         )
#         ax.legend(loc='upper left', fontsize=10)
#         ax.grid(alpha=0.3)
#         ax.set_xlim(min_val, max_val)
#         ax.set_ylim(min_val, max_val)
        
#         plt.tight_layout()
#         plt.savefig('evaluation_results/pred_vs_actual_cftnet_detailed.png', dpi=300)
#         plt.close()
#         print("âœ… CFT-Net è¯¦ç»†æ•£ç‚¹å›¾å·²ä¿å­˜")
    
#     def analyze_physical_determinism(self):
#         """åˆ†æä¼ è¾“æ—¶é—´çš„ç‰©ç†ç¡®å®šæ€§ï¼Œè§£é‡Šé«˜RÂ²çš„åˆç†æ€§"""
#         print("\n" + "="*60)
#         print("ğŸ” ä¼ è¾“æ—¶é—´ç‰©ç†ç¡®å®šæ€§åˆ†æ")
#         print("="*60)
        
#         # åæ ‡å‡†åŒ–è·å–åŸå§‹ç‰¹å¾
#         Xc_test_orig = self.scaler_c.inverse_transform(self.Xc_test)
#         Xi_test_orig = self.scaler_i.inverse_transform(self.Xi_test)
        
#         # æ‰¾åˆ°æ ¸å¿ƒç‰©ç†ç‰¹å¾çš„ç´¢å¼•
#         size_idx = self.cols_i.index('total_size_mb') if 'total_size_mb' in self.cols_i else -1
#         bw_idx = self.cols_c.index('bandwidth_mbps') if 'bandwidth_mbps' in self.cols_c else -1
        
#         if size_idx >= 0 and bw_idx >= 0:
#             total_size_mb = Xi_test_orig[:, size_idx]
#             bandwidth_mbps = Xc_test_orig[:, bw_idx]
            
#             # ç†è®ºä¼ è¾“æ—¶é—´ï¼ˆå¿½ç•¥å‹ç¼©ã€åè®®å¼€é”€ï¼‰
#             # å…¬å¼ï¼šæ—¶é—´(s) = æ–‡ä»¶å¤§å°(MB) / å¸¦å®½(MB/s) = æ–‡ä»¶å¤§å°(MB) / (å¸¦å®½(Mbps)/8)
#             theoretical_time = total_size_mb / (bandwidth_mbps / 8)
#             actual_time = self.y_test_orig
            
#             # ç›¸å…³æ€§åˆ†æ
#             correlation = np.corrcoef(theoretical_time, actual_time)[0, 1]
#             r2_theoretical = correlation ** 2
            
#             print(f"ç†è®ºä¼ è¾“æ—¶é—´ vs å®é™…ä¼ è¾“æ—¶é—´ çš®å°”é€Šç›¸å…³ç³»æ•°: {correlation:.4f}")
#             print(f"ç†è®ºå…¬å¼å¯è§£é‡Šçš„RÂ²: {r2_theoretical:.4f}")
#             print(f"ç†è®ºæ—¶é—´èŒƒå›´: [{theoretical_time.min():.2f}, {theoretical_time.max():.2f}] s")
#             print(f"å®é™…æ—¶é—´èŒƒå›´: [{actual_time.min():.2f}, {actual_time.max():.2f}] s")
            
#             print("\nğŸ’¡ é«˜RÂ²åˆç†æ€§è¯´æ˜ï¼š")
#             print("å®¹å™¨é•œåƒä¼ è¾“æ—¶é—´ç”±å¼ºç‰©ç†è§„å¾‹ä¸»å¯¼ï¼Œæ ¸å¿ƒå…¬å¼ä¸ºï¼š")
#             print("  ä¼ è¾“æ—¶é—´ â‰ˆ é•œåƒæ€»å¤§å° / æœ‰æ•ˆä¼ è¾“å¸¦å®½ + å›ºå®šå¼€é”€")
#             print(f"ä»…ã€Œå¤§å°/å¸¦å®½ã€çš„åŸºç¡€å…¬å¼å³å¯è§£é‡Š {r2_theoretical*100:.1f}% çš„æ—¶é—´æ³¢åŠ¨ï¼Œ")
#             print("å› æ­¤æ¨¡å‹RÂ²æ¥è¿‘1.0æ˜¯ç¬¦åˆç‰©ç†è§„å¾‹çš„ï¼Œå¹¶éè¿‡æ‹Ÿåˆã€‚")
#             print("CFT-Netçš„æ ¸å¿ƒä»·å€¼åœ¨äºé‡åŒ–å…¬å¼æ— æ³•è¦†ç›–çš„éšæœºæ³¢åŠ¨ï¼ˆå‹ç¼©ç‡ã€ç½‘ç»œæŠ–åŠ¨ã€å®¿ä¸»æœºè´Ÿè½½ç­‰ï¼‰ï¼Œ")
#             print("æä¾›å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä¸ºè°ƒåº¦å†³ç­–æä¾›é£é™©æ„ŸçŸ¥èƒ½åŠ›ã€‚")
        
#         print("="*60)
    
#     def run_full_evaluation(self):
#         """æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
#         self.calibrate_cftnet()
#         self.evaluate_cftnet()
#         self.train_baselines()
#         self.analyze_physical_determinism()
#         self.generate_comparison_table()
#         self.generate_radar_chart()
#         self.plot_calibration_curve()
#         self.plot_prediction_intervals()
#         self.plot_pred_vs_actual()
        
#         # ä¿å­˜å®Œæ•´ç»“æœ
#         with open('evaluation_results/full_evaluation_results.json', 'w') as f:
#             json.dump({k: {kk: vv for kk, vv in v.items() if kk not in ['predictions', 'uncertainties', 'raw_uncertainties']} for k, v in self.results.items()}, f, indent=2)
        
#         print("\nğŸ‰ æ‰€æœ‰è¯„ä¼°æµç¨‹å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ evaluation_results/ ç›®å½•")

# # ==============================================================================
# # 5. ä¸»ç¨‹åºå…¥å£
# # ==============================================================================
# if __name__ == "__main__":
#     # è¯·ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
#     MODEL_PATH = "cts_optimized_0218_2125_seed42.pth"
    
#     if not os.path.exists(MODEL_PATH):
#         print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}")
#         print("è¯·ä¿®æ”¹è„šæœ¬ä¸­ MODEL_PATH ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„")
#         exit(1)
    
#     # åˆå§‹åŒ–è¯„ä¼°å™¨å¹¶æ‰§è¡Œå®Œæ•´è¯„ä¼°
#     evaluator = ModelEvaluator(MODEL_PATH, seed=SEED)
#     evaluator.run_full_evaluation()

"""
CFT-Net V2 å®Œæ•´å¯¹æ¯”è¯„æµ‹è„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆé€‚é…ï¼Œæ–°å¢ç‰©ç†ç‰¹å¾ï¼‰
ç”Ÿæˆç”¨äºè®ºæ–‡çš„å¯¹æ¯”è¡¨æ ¼å’Œé›·è¾¾å›¾ï¼ˆç²¾åº¦ã€é£é™©æ„ŸçŸ¥ã€å¯é æ€§ã€è½»é‡åŒ–ï¼‰
ä¿®å¤å†…å®¹ï¼š
1. å®Œå…¨å¯¹é½è®­ç»ƒæ—¶çš„ CompactCFTNetV2 æ¨¡å‹æ¶æ„ï¼Œè§£å†³æƒé‡åŠ è½½æŠ¥é”™
2. ä¿®æ­£ä¸ç¡®å®šæ€§ä¼ æ’­ï¼ˆDelta Methodï¼‰ï¼Œè§£å†³åŸå§‹ç©ºé—´å°ºåº¦ä¸åŒ¹é…é—®é¢˜
3. ã€å…³é”®ã€‘æ–°å¢ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„ç‰©ç†äº¤å‰ç‰¹å¾ï¼Œè§£å†³ç»´åº¦ä¸åŒ¹é…æŠ¥é”™
4. ç®—æ³•ç‰¹å¾ä½¿ç”¨One-Hotç¼–ç ï¼Œé¿å…æ•°å€¼é¡ºåºè¯¯å¯¼
5. ç»Ÿä¸€æ¨ç†æ—¶é—´æµ‹é‡æ ‡å‡†ï¼ˆå…¨éƒ¨åœ¨CPUä¸Šæµ‹é‡ï¼‰
6. å®Œæ•´ä¿ç•™åˆ†å±‚æ ¡å‡†ã€å…¨é‡è¯„ä¼°ã€æ‰€æœ‰å¯è§†åŒ–åŠŸèƒ½
7. ä¿®å¤é¢„æµ‹åŒºé—´PICPä¸è¡¨æ ¼ä¸ä¸€è‡´çš„é—®é¢˜
8. æ–°å¢ Pred vs Actual æ•£ç‚¹å›¾ä¸ç‰©ç†ç¡®å®šæ€§åˆ†æ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pandas as pd
import os
import time
import pickle
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
import lightgbm as lgb
from scipy.stats import spearmanr, norm, wilcoxon
from scipy.optimize import brentq
from collections import Counter
import warnings
import platform

warnings.filterwarnings('ignore')

# ==============================================================================
# 0. åŸºç¡€é…ç½®
# ==============================================================================
system = platform.system()
if system == 'Windows':
    plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'Arial Unicode MS']
elif system == 'Darwin':
    plt.rcParams['font.sans-serif'] = ['Heiti TC', 'PingFang HK', 'Arial Unicode MS']
else:
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'Droid Sans Fallback', 'DejaVu Sans']

plt.rcParams['axes.unicode_minus'] = False

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

os.makedirs("evaluation_results", exist_ok=True)

# ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´çš„æ¨¡å‹è¶…å‚æ•°
MODEL_CONFIG = {
    "embed_dim": 64,
    "nhead": 4,
    "num_layers": 2,
    "dim_feedforward": 128,
    "alpha_init": 2.0,
    "beta_init": 1.0,
    "v_init": 1.0,
}

# ==============================================================================
# 1. æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒè„šæœ¬ CompactCFTNetV2 100% ä¸€è‡´ï¼‰
# ==============================================================================
class LightweightFeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.embeddings = nn.Parameter(torch.empty(num_features, embed_dim))
        self.bias = nn.Parameter(torch.zeros(num_features, embed_dim))
        self.norm = nn.LayerNorm(embed_dim)
        nn.init.xavier_normal_(self.embeddings)
        
    def forward(self, x):
        x = x.unsqueeze(-1)
        out = x * self.embeddings + self.bias
        return self.norm(out)

class LightweightTransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim=64, nhead=4, num_layers=2, dim_feedforward=128):
        super().__init__()
        self.tokenizer = LightweightFeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, 
            nhead=nhead, 
            dim_feedforward=dim_feedforward,
            batch_first=True, 
            dropout=0.2,
            activation="gelu"
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
    def forward(self, x):
        tokens = self.tokenizer(x)
        cls = self.cls_token.expand(x.size(0), -1, -1)
        x = torch.cat([cls, tokens], dim=1)
        out = self.encoder(x)
        return out[:, 0, :]

class CompactCFTNet(nn.Module):
    """
    ä¸è®­ç»ƒè„šæœ¬ CompactCFTNetV2 å®Œå…¨ä¸€è‡´ï¼Œä»…ä¿ç•™ç±»åå…¼å®¹åŸæœ‰ä»£ç 
    """
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=64):
        super().__init__()
        self.client_tower = LightweightTransformerTower(
            client_feats, embed_dim, 
            nhead=MODEL_CONFIG['nhead'], 
            num_layers=MODEL_CONFIG['num_layers'],
            dim_feedforward=MODEL_CONFIG['dim_feedforward']
        )
        self.image_tower = LightweightTransformerTower(
            image_feats, embed_dim, 
            nhead=MODEL_CONFIG['nhead'], 
            num_layers=MODEL_CONFIG['num_layers'],
            dim_feedforward=MODEL_CONFIG['dim_feedforward']
        )
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
        # å…±äº«èåˆå±‚
        self.shared_fusion = nn.Sequential(
            nn.Linear(embed_dim * 3, embed_dim * 2),
            nn.LayerNorm(embed_dim * 2),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(embed_dim * 2, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU()
        )
        
        # è§£è€¦å¤´ï¼šå‡å€¼é¢„æµ‹åˆ†æ”¯
        self.head_mean = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Linear(embed_dim // 2, 1)
        )
        
        # è§£è€¦å¤´ï¼šä¸ç¡®å®šæ€§é¢„æµ‹åˆ†æ”¯
        self.head_uncertainty = nn.Sequential(
            nn.Linear(embed_dim, embed_dim // 2),
            nn.LayerNorm(embed_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(embed_dim // 2, 3)
        )
        
        # ä¸è®­ç»ƒä¸€è‡´çš„åˆå§‹åŒ–å‚æ•°
        self.alpha_init = MODEL_CONFIG['alpha_init']
        self.beta_init = MODEL_CONFIG['beta_init']
        self.v_init = MODEL_CONFIG['v_init']
        
    def forward(self, cx, ix, ax):
        c = self.client_tower(cx)
        i = self.image_tower(ix)
        a = self.algo_embed(ax)
        
        fused = torch.cat([c, i, a], dim=-1)
        shared = self.shared_fusion(fused)
        
        # è§£è€¦è¾“å‡º
        gamma = self.head_mean(shared).squeeze(-1)
        unc_out = self.head_uncertainty(shared)
        
        # ä¸è®­ç»ƒä¸€è‡´çš„å‚æ•°çº¦æŸ
        v = F.softplus(unc_out[:, 0]) + self.v_init
        alpha = F.softplus(unc_out[:, 1]) + self.alpha_init
        beta = F.softplus(unc_out[:, 2]) + self.beta_init
        
        return torch.stack([gamma, v, alpha, beta], dim=1)

# ==============================================================================
# 2. è¯„ä¼°æŒ‡æ ‡å‡½æ•°
# ==============================================================================
def calculate_smape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    denominator = np.abs(y_true) + np.abs(y_pred) + 1e-8
    smape = np.mean(2 * np.abs(y_true - y_pred) / denominator) * 100
    return smape

def calculate_mape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100

def calculate_picp_mpiw(y_true, y_pred, unc, confidence=0.8):
    z = norm.ppf((1 + confidence) / 2)
    lower = y_pred - z * unc
    upper = y_pred + z * unc
    picp = np.mean((y_true >= lower) & (y_true <= upper)) * 100
    mpiw = np.mean(upper - lower)
    return picp, mpiw

def calculate_ece_quantile(errors, uncertainties, n_bins=10):
    if len(errors) == 0:
        return 0.0
    quantiles = np.linspace(0, 100, n_bins + 1)
    bin_edges = np.percentile(uncertainties, quantiles)
    bin_edges[-1] += 1e-8
    ece = 0.0
    for i in range(n_bins):
        in_bin = (uncertainties >= bin_edges[i]) & (uncertainties < bin_edges[i+1])
        if i == n_bins - 1:
            in_bin = (uncertainties >= bin_edges[i]) & (uncertainties <= bin_edges[i+1])
        prop = in_bin.sum() / len(errors)
        if prop > 0:
            avg_unc = uncertainties[in_bin].mean()
            avg_err = errors[in_bin].mean()
            ece += np.abs(avg_err - avg_unc) * prop
    return ece

def hierarchical_calibration(y_true, y_pred, unc_raw, target_coverage=0.8, n_bins=5):
    """
    åˆ†å±‚æ ¡å‡†ï¼šå¯¹ä¸åŒä¸ç¡®å®šæ€§æ°´å¹³ä½¿ç”¨ä¸åŒç¼©æ”¾å› å­
    è§£å†³é«˜ä¸ç¡®å®šæ€§åŒºåŸŸæ ¡å‡†ä¸è¶³çš„é—®é¢˜
    """
    quantiles = np.percentile(unc_raw, np.linspace(0, 100, n_bins + 1))
    scales = []
    bin_edges = []
    
    print(f"{'åŒºé—´':<15} {'æ ·æœ¬æ•°':<8} {'åŸå§‹ä¸ç¡®å®š':<12} {'å®é™…è¯¯å·®':<12} {'ç¼©æ”¾å› å­':<10}")
    print("-" * 70)
    
    for i in range(n_bins):
        low, high = quantiles[i], quantiles[i+1]
        bin_edges.append((low, high))
        mask = (unc_raw >= low) & (unc_raw <= high)
        n_samples = mask.sum()
        
        if n_samples > 10:
            def picp_with_scale(s):
                z = norm.ppf((1 + target_coverage) / 2)
                lower = y_pred[mask] - z * s * unc_raw[mask]
                upper = y_pred[mask] + z * s * unc_raw[mask]
                return np.mean((y_true[mask] >= lower) & (y_true[mask] <= upper))
            
            try:
                s_opt = brentq(lambda s: picp_with_scale(s) - target_coverage, 0.1, 100)
            except:
                test_scales = np.linspace(0.1, 100, 500)
                picps = [picp_with_scale(s) for s in test_scales]
                s_opt = test_scales[np.argmin(np.abs(np.array(picps) - target_coverage))]
            scales.append(s_opt)
            
            print(f"[{low:.2f}, {high:.2f}]  "
                  f"{n_samples:<8} {unc_raw[mask].mean():>10.2f}s  "
                  f"{np.abs(y_true[mask]-y_pred[mask]).mean():>10.2f}s  {s_opt:>8.2f}x")
        else:
            scales.append(1.0)
            print(f"[{low:.2f}, {high:.2f}]  "
                  f"{n_samples:<8} {'-':>10}  {'-':>10}  {1.0:>8.2f}x")
    
    # åº”ç”¨åˆ†å±‚ç¼©æ”¾
    unc_cal = unc_raw.copy()
    for i, (low, high) in enumerate(bin_edges):
        mask = (unc_raw >= low) & (unc_raw <= high)
        unc_cal[mask] = unc_raw[mask] * scales[i]
    
    return unc_cal, scales, bin_edges

def apply_hierarchical_calibration(unc_raw, bin_edges, scales):
    """å°†éªŒè¯é›†å­¦åˆ°çš„åˆ†å±‚æ ¡å‡†åº”ç”¨åˆ°æµ‹è¯•é›†"""
    unc_cal = unc_raw.copy()
    for i, (low, high) in enumerate(bin_edges):
        mask = (unc_raw >= low) & (unc_raw <= high)
        unc_cal[mask] = unc_raw[mask] * scales[i]
    return unc_cal

def post_hoc_calibration(y_true, y_pred, unc_raw, target_coverage=0.8, search_range=(0.1, 100)):
    """å…¨å±€å•å› å­æ ¡å‡†ï¼ˆä½œä¸ºå›é€€æ–¹æ¡ˆï¼‰"""
    def picp_with_scale(s):
        z = norm.ppf((1 + target_coverage) / 2)
        lower = y_pred - z * s * unc_raw
        upper = y_pred + z * s * unc_raw
        return np.mean((y_true >= lower) & (y_true <= upper))
    s_min, s_max = search_range
    try:
        s_opt = brentq(lambda s: picp_with_scale(s) - target_coverage, s_min, s_max)
        return s_opt
    except:
        scales = np.linspace(s_min, s_max, 500)
        picps = [picp_with_scale(s) for s in scales]
        best_idx = np.argmin(np.abs(np.array(picps) - target_coverage))
        return scales[best_idx]

# ==============================================================================
# 3. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼ˆã€å…³é”®ä¿®æ”¹ã€‘ä¸è®­ç»ƒè„šæœ¬ä¸€è‡´çš„ç‰¹å¾å·¥ç¨‹ï¼‰
# ==============================================================================
def load_preprocessing_objects():
    # ã€ä¿®æ”¹ã€‘å°è¯•åŠ è½½ä¼˜åŒ–ç‰ˆçš„é¢„å¤„ç†å¯¹è±¡ï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€
    if os.path.exists('preprocessing_objects_optimized.pkl'):
        print("ğŸ“¦ åŠ è½½ä¼˜åŒ–ç‰ˆé¢„å¤„ç†å¯¹è±¡ (preprocessing_objects_optimized.pkl)")
        with open('preprocessing_objects_optimized.pkl', 'rb') as f:
            prep = pickle.load(f)
    else:
        print("âš ï¸  æœªæ‰¾åˆ°ä¼˜åŒ–ç‰ˆé¢„å¤„ç†å¯¹è±¡ï¼ŒåŠ è½½é»˜è®¤ç‰ˆ (preprocessing_objects.pkl)")
        with open('preprocessing_objects.pkl', 'rb') as f:
            prep = pickle.load(f)
    return prep

def load_data():
    """ã€æ ¸å¿ƒä¿®æ”¹ã€‘ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´çš„ç‰¹å¾å·¥ç¨‹ï¼Œæ–°å¢4ä¸ªç‰©ç†ç‰¹å¾"""
    df_exp = pd.read_excel("cts_data.xlsx")
    df_feat = pd.read_csv("image_features_database.csv")
    rename_map = {
        "image": "image_name", "method": "algo_name",
        "network_bw": "bandwidth_mbps", "network_delay": "network_rtt",
        "mem_limit": "mem_limit_mb"
    }
    df_exp = df_exp.rename(columns=rename_map)
    if 'total_time' not in df_exp.columns:
        cols = [c for c in df_exp.columns if 'total_tim' in c]
        if cols:
            df_exp = df_exp.rename(columns={cols[0]: 'total_time'})
    df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
    df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
    
    # ==========================================
    # ã€å…³é”®ä¿®å¤ã€‘å’Œè®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼šæ–°å¢ç‰©ç†äº¤å‰ç‰¹å¾
    # ==========================================
    print("ğŸ”§ åŠ è½½æ•°æ®ï¼šæ–°å¢ç‰©ç†äº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼‰")
    
    # 1. æœ€æ ¸å¿ƒç‰¹å¾ï¼šç†è®ºä¼ è¾“æ—¶é—´ (æ–‡ä»¶å¤§å° / æœ‰æ•ˆå¸¦å®½)
    df['theoretical_time'] = df['total_size_mb'] / (df['bandwidth_mbps'] / 8 + 1e-8)
    
    # 2. èµ„æºå‹åŠ›ç‰¹å¾
    df['cpu_to_size_ratio'] = df['cpu_limit'] / (df['total_size_mb'] + 1e-8)
    df['mem_to_size_ratio'] = df['mem_limit_mb'] / (df['total_size_mb'] + 1e-8)
    
    # 3. ç½‘ç»œç»¼åˆæŒ‡æ ‡
    df['network_score'] = df['bandwidth_mbps'] / (df['network_rtt'] + 1e-8)
    
    # æ›´æ–°ç‰¹å¾åˆ—è¡¨ï¼Œå¿…é¡»å’Œè®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼
    cols_c = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb', 
              'theoretical_time', 'cpu_to_size_ratio', 'mem_to_size_ratio', 'network_score']
    
    # é•œåƒç‰¹å¾ä¿æŒä¸å˜
    target_cols = ['total_size_mb', 'avg_layer_entropy', 'entropy_std',
                   'layer_count', 'size_std_mb', 'text_ratio', 'zero_ratio']
    cols_i = [c for c in target_cols if c in df.columns]
    
    Xc_raw = df[cols_c].values
    Xi_raw = df[cols_i].values
    y_raw_log = np.log1p(df['total_time'].values)
    y_raw_orig = df['total_time'].values
    algo_names_raw = df['algo_name'].values
    return Xc_raw, Xi_raw, algo_names_raw, y_raw_log, cols_c, cols_i, y_raw_orig

# ==============================================================================
# 4. è¯„ä¼°ä¸»ç±»ï¼ˆå®Œæ•´ä¿®å¤ç‰ˆï¼‰
# ==============================================================================
class ModelEvaluator:
    def __init__(self, model_path, seed=42):
        self.seed = seed
        np.random.seed(seed)
        self.prep = load_preprocessing_objects()
        self.scaler_c = self.prep['scaler_c']
        self.scaler_i = self.prep['scaler_i']
        self.enc = self.prep['enc']
        
        # ã€ä¿®æ”¹ã€‘ä¼˜å…ˆä»é¢„å¤„ç†å¯¹è±¡ä¸­è¯»å–ç‰¹å¾åˆ—åï¼Œç¡®ä¿å’Œè®­ç»ƒä¸€è‡´
        self.cols_c = self.prep.get('cols_c', ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb', 
                                                 'theoretical_time', 'cpu_to_size_ratio', 'mem_to_size_ratio', 'network_score'])
        self.cols_i = self.prep.get('cols_i', ['total_size_mb', 'avg_layer_entropy', 'layer_count', 'text_ratio', 'zero_ratio'])
        
        self.default_algo = self.prep.get('most_common_algo', self.enc.classes_[0])
        self.default_idx = self.enc.transform([self.default_algo])[0]
        
        # åŠ è½½æ•°æ®
        Xc_raw, Xi_raw, algo_names_raw, y_log, _, _, y_orig = load_data()
        N = len(y_log)
        idx = np.random.permutation(N)
        n_tr = int(N * 0.7)
        n_val = int(N * 0.15)
        self.tr_idx = idx[:n_tr]
        self.val_idx = idx[n_tr:n_tr+n_val]
        self.te_idx = idx[n_tr+n_val:]
        
        # æ ‡å‡†åŒ–
        self.Xc_train = self.scaler_c.transform(Xc_raw[self.tr_idx])
        self.Xc_val = self.scaler_c.transform(Xc_raw[self.val_idx])
        self.Xc_test = self.scaler_c.transform(Xc_raw[self.te_idx])
        self.Xi_train = self.scaler_i.transform(Xi_raw[self.tr_idx])
        self.Xi_val = self.scaler_i.transform(Xi_raw[self.val_idx])
        self.Xi_test = self.scaler_i.transform(Xi_raw[self.te_idx])
        
        # ç®—æ³•ç¼–ç 
        def safe_transform(labels):
            known = set(self.enc.classes_)
            return np.array([self.enc.transform([l])[0] if l in known else self.default_idx for l in labels])
        
        self.Xa_train = self.enc.transform(algo_names_raw[self.tr_idx])
        self.Xa_val = safe_transform(algo_names_raw[self.val_idx])
        self.Xa_test = safe_transform(algo_names_raw[self.te_idx])
        
        self.y_train_log = y_log[self.tr_idx]
        self.y_val_log = y_log[self.val_idx]
        self.y_test_log = y_log[self.te_idx]
        self.y_train_orig = y_orig[self.tr_idx]
        self.y_val_orig = y_orig[self.val_idx]
        self.y_test_orig = y_orig[self.te_idx]
        
        # åŸºçº¿æ¨¡å‹ä½¿ç”¨One-Hotç¼–ç ç®—æ³•ç‰¹å¾ï¼Œé¿å…æ•°å€¼é¡ºåºè¯¯å¯¼
        self.algo_onehot = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
        self.algo_onehot.fit(self.Xa_train.reshape(-1, 1))
        
        Xa_train_oh = self.algo_onehot.transform(self.Xa_train.reshape(-1, 1))
        Xa_val_oh = self.algo_onehot.transform(self.Xa_val.reshape(-1, 1))
        Xa_test_oh = self.algo_onehot.transform(self.Xa_test.reshape(-1, 1))
        
        self.X_train_comb = np.hstack([self.Xc_train, self.Xi_train, Xa_train_oh])
        self.X_val_comb = np.hstack([self.Xc_val, self.Xi_val, Xa_val_oh])
        self.X_test_comb = np.hstack([self.Xc_test, self.Xi_test, Xa_test_oh])
        
        print(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒ {len(self.tr_idx)} | éªŒè¯ {len(self.val_idx)} | æµ‹è¯• {len(self.te_idx)}")
        print(f"åŸºçº¿æ¨¡å‹ç‰¹å¾ç»´åº¦: {self.X_train_comb.shape[1]} (åŒ…å«{len(self.enc.classes_)}ä¸ªç®—æ³•çš„One-Hotç¼–ç )")
        print(f"CFT-Net è¾“å…¥ç»´åº¦: å®¢æˆ·ç«¯ç‰¹å¾={len(self.cols_c)}, é•œåƒç‰¹å¾={len(self.cols_i)}")
        
        # åŠ è½½CFT-Netæ¨¡å‹ï¼ˆä¸è®­ç»ƒå®Œå…¨ä¸€è‡´çš„æ¶æ„ï¼‰
        self.embed_dim = MODEL_CONFIG['embed_dim']
        self.cftnet = CompactCFTNet(
            client_feats=len(self.cols_c),  # ã€å…³é”®ã€‘è¿™é‡Œä¼šè‡ªåŠ¨è¯»å–æ–°çš„8ç»´ç‰¹å¾
            image_feats=len(self.cols_i), 
            num_algos=len(self.enc.classes_), 
            embed_dim=self.embed_dim
        ).to(device)
        
        # åŠ è½½æƒé‡ï¼Œå…¼å®¹ä¸¤ç§ä¿å­˜æ ¼å¼
        checkpoint = torch.load(model_path, map_location=device)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        else:
            state_dict = checkpoint
        self.cftnet.load_state_dict(state_dict)
        self.cftnet.eval()
        print("âœ… CFT-Net V2 æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        self.results = {}
        self.calibration_params = {}
    
    def predict_cftnet(self, Xc, Xi, Xa):
        """
        æ‰¹é‡é¢„æµ‹ï¼Œä½¿ç”¨Delta Methodä¿®æ­£åŸå§‹ç©ºé—´ä¸ç¡®å®šæ€§
        ä¸è®­ç»ƒæ—¶çš„è®¡ç®—é€»è¾‘å®Œå…¨ä¸€è‡´
        """
        batch_size = 1024
        n = len(Xc)
        preds_orig = []
        uncs_log = []
        uncs_orig = []
        
        with torch.no_grad():
            for i in range(0, n, batch_size):
                cx = torch.FloatTensor(Xc[i:i+batch_size]).to(device)
                ix = torch.FloatTensor(Xi[i:i+batch_size]).to(device)
                ax = torch.LongTensor(Xa[i:i+batch_size]).to(device)
                
                out = self.cftnet(cx, ix, ax)
                gamma, v, alpha, beta = out[:, 0], out[:, 1], out[:, 2], out[:, 3]
                
                # 1. å‡å€¼è½¬æ¢å›åŸå§‹ç©ºé—´
                pred_log = gamma
                pred_orig = torch.expm1(pred_log)
                
                # 2. ä¸ç¡®å®šæ€§ä¼ æ’­ï¼ˆDelta Methodï¼‰
                # Var(exp(x)-1) â‰ˆ (exp(x))Â² * Var(x)
                var_log = beta / (v * (alpha - 1) + 1e-6)
                std_log = torch.sqrt(var_log + 1e-6)
                std_orig = torch.exp(pred_log) * std_log  # æ ¸å¿ƒä¿®æ­£ï¼šå°ºåº¦å¯¹é½
                
                preds_orig.append(pred_orig.cpu().numpy())
                uncs_log.append(std_log.cpu().numpy())
                uncs_orig.append(std_orig.cpu().numpy())
        
        return np.concatenate(preds_orig), np.concatenate(uncs_orig), np.concatenate(uncs_log)
    
    def calibrate_cftnet(self):
        """åœ¨éªŒè¯é›†ä¸Šå­¦ä¹ æ ¡å‡†å‚æ•°"""
        print("\n" + "="*60)
        print("ğŸ”§ CFT-Net äº‹åæ ¡å‡†ï¼ˆéªŒè¯é›†ï¼‰")
        print("="*60)
        
        pred_val, unc_val_raw, _ = self.predict_cftnet(self.Xc_val, self.Xi_val, self.Xa_val)
        picp_val_raw, _ = calculate_picp_mpiw(self.y_val_orig, pred_val, unc_val_raw, 0.8)
        print(f"éªŒè¯é›†åŸå§‹PICP: {picp_val_raw:.1f}%")
        
        # ä¼˜å…ˆä½¿ç”¨åˆ†å±‚æ ¡å‡†
        print("\n--- åˆ†å±‚æ ¡å‡†å­¦ä¹  ---")
        unc_val_cal, scales, bin_edges = hierarchical_calibration(
            self.y_val_orig, pred_val, unc_val_raw, target_coverage=0.8, n_bins=5
        )
        picp_val_cal, _ = calculate_picp_mpiw(self.y_val_orig, pred_val, unc_val_cal, 0.8)
        print(f"åˆ†å±‚æ ¡å‡†åPICP: {picp_val_cal:.1f}%")
        
        # ä¿å­˜æ ¡å‡†å‚æ•°
        self.calibration_params = {
            'hierarchical_scales': scales,
            'bin_edges': bin_edges,
            'global_scale': post_hoc_calibration(self.y_val_orig, pred_val, unc_val_raw)
        }
        print(f"å…¨å±€æ ¡å‡†ç¼©æ”¾å› å­: {self.calibration_params['global_scale']:.3f}")
        print("="*60)
        
        return self.calibration_params
    
    def evaluate_cftnet(self):
        """CFT-Net å®Œæ•´æµ‹è¯•é›†è¯„ä¼°"""
        pred_test, unc_test_raw, _ = self.predict_cftnet(self.Xc_test, self.Xi_test, self.Xa_test)
        
        # åº”ç”¨åˆ†å±‚æ ¡å‡†
        unc_test_cal = apply_hierarchical_calibration(
            unc_test_raw, 
            self.calibration_params['bin_edges'], 
            self.calibration_params['hierarchical_scales']
        )
        
        errors_test = np.abs(self.y_test_orig - pred_test)
        
        # å…¨é‡æŒ‡æ ‡è®¡ç®—
        mae = mean_absolute_error(self.y_test_orig, pred_test)
        rmse = np.sqrt(mean_squared_error(self.y_test_orig, pred_test))
        smape = calculate_smape(self.y_test_orig, pred_test)
        mape = calculate_mape(self.y_test_orig, pred_test)
        r2 = r2_score(self.y_test_orig, pred_test)
        
        # ä¸ç¡®å®šæ€§æŒ‡æ ‡
        corr, _ = spearmanr(unc_test_cal, errors_test)
        corr = 0.0 if np.isnan(corr) else corr
        picp, mpiw = calculate_picp_mpiw(self.y_test_orig, pred_test, unc_test_cal, 0.8)
        ece = calculate_ece_quantile(errors_test, unc_test_cal)
        
        # æ¨ç†æ—¶é—´ï¼ˆç»Ÿä¸€åœ¨CPUä¸Šæµ‹é‡ï¼Œå…¬å¹³å¯¹æ¯”ï¼‰
        infer_time = self.measure_inference_time_cftnet_cpu()
        
        # å‚æ•°é‡ç»Ÿè®¡
        params_k = sum(p.numel() for p in self.cftnet.parameters()) / 1000
        
        self.results['CFT-Net'] = {
            'MAE': mae, 'RMSE': rmse, 'sMAPE': smape, 'MAPE': mape,
            'R2': r2, 'Corr': corr, 'PICP_80': picp, 'MPIW_80': mpiw, 'ECE': ece,
            'Inference_ms': infer_time * 1000,
            'Params_K': params_k,
            'predictions': pred_test,
            'uncertainties': unc_test_cal,
            'raw_uncertainties': unc_test_raw
        }
        
        print(f"\nâœ… CFT-Net æµ‹è¯•é›†è¯„ä¼°å®Œæˆ")
        print(f"  ç²¾åº¦æŒ‡æ ‡: sMAPE={smape:.2f}%, RMSE={rmse:.2f}s, RÂ²={r2:.4f}")
        print(f"  ä¸ç¡®å®šæ€§: Corr={corr:.3f}, PICP={picp:.1f}%, MPIW={mpiw:.2f}s, ECE={ece:.3f}")
        print(f"  æ¨ç†æ€§èƒ½: å•æ ·æœ¬æ¨ç†={infer_time*1000:.3f}ms, å‚æ•°é‡={params_k:.1f}K")
        
        return self.results['CFT-Net']
    
    def measure_inference_time_cftnet_cpu(self):
        """åœ¨CPUä¸Šæµ‹é‡CFT-Netæ¨ç†æ—¶é—´ï¼Œç¡®ä¿ä¸åŸºçº¿æ¨¡å‹å…¬å¹³å¯¹æ¯”"""
        self.cftnet.cpu()
        batch_size = 256
        n = len(self.Xc_test)
        
        # Warmup
        with torch.no_grad():
            for i in range(0, min(500, n), batch_size):
                cx = torch.FloatTensor(self.Xc_test[i:i+batch_size])
                ix = torch.FloatTensor(self.Xi_test[i:i+batch_size])
                ax = torch.LongTensor(self.Xa_test[i:i+batch_size])
                _ = self.cftnet(cx, ix, ax)
        
        # æ­£å¼è®¡æ—¶
        times = []
        with torch.no_grad():
            for i in range(0, n, batch_size):
                cx = torch.FloatTensor(self.Xc_test[i:i+batch_size])
                ix = torch.FloatTensor(self.Xi_test[i:i+batch_size])
                ax = torch.LongTensor(self.Xa_test[i:i+batch_size])
                
                start = time.perf_counter()
                _ = self.cftnet(cx, ix, ax)
                times.append(time.perf_counter() - start)
        
        # ç§»å›åŸè®¾å¤‡
        self.cftnet.to(device)
        
        total_time = np.sum(times)
        return total_time / n
    
    def train_baselines(self):
        """è®­ç»ƒå¹¶è¯„ä¼°åŸºçº¿æ¨¡å‹ï¼ˆRandomForest/XGBoost/LightGBMï¼‰"""
        models = {
            'RandomForest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=self.seed, n_jobs=-1),
            'XGBoost': xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=self.seed, n_jobs=-1),
            'LightGBM': lgb.LGBMRegressor(n_estimators=100, num_leaves=31, learning_rate=0.1, random_state=self.seed, n_jobs=-1, verbose=-1)
        }
        print("\nğŸš€ è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆä½¿ç”¨One-Hotç¼–ç ç®—æ³•ç‰¹å¾ï¼‰...")
        
        for name, model in models.items():
            print(f"  è®­ç»ƒ {name}...")
            start = time.perf_counter()
            model.fit(self.X_train_comb, self.y_train_log)
            train_time = time.perf_counter() - start
            
            # é¢„æµ‹å¹¶è½¬æ¢å›åŸå§‹ç©ºé—´
            pred_log = model.predict(self.X_test_comb)
            pred_orig = np.expm1(pred_log)
            
            # ç²¾åº¦æŒ‡æ ‡
            mae = mean_absolute_error(self.y_test_orig, pred_orig)
            rmse = np.sqrt(mean_squared_error(self.y_test_orig, pred_orig))
            smape = calculate_smape(self.y_test_orig, pred_orig)
            mape = calculate_mape(self.y_test_orig, pred_orig)
            r2 = r2_score(self.y_test_orig, pred_orig)
            
            # æ¨ç†æ—¶é—´
            infer_time = self.measure_inference_time_sklearn(model, self.X_test_comb)
            
            self.results[name] = {
                'MAE': mae, 'RMSE': rmse, 'sMAPE': smape, 'MAPE': mape, 'R2': r2,
                'Corr': None, 'PICP_80': None, 'MPIW_80': None, 'ECE': None,
                'Inference_ms': infer_time * 1000,
                'Params_K': None,
                'predictions': pred_orig
            }
            print(f"    å®Œæˆ: RÂ²={r2:.4f}, sMAPE={smape:.2f}%, å•æ ·æœ¬æ¨ç†={infer_time*1000:.3f}ms")
    
    def measure_inference_time_sklearn(self, model, X):
        """æµ‹é‡sklearnç³»åˆ—æ¨¡å‹çš„æ¨ç†æ—¶é—´"""
        batch_size = 256
        n = len(X)
        times = []
        for i in range(0, n, batch_size):
            X_batch = X[i:i+batch_size]
            start = time.perf_counter()
            _ = model.predict(X_batch)
            times.append(time.perf_counter() - start)
        total_time = np.sum(times)
        return total_time / n
    
    def generate_comparison_table(self):
        """ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼ï¼ˆCSV+LaTeXï¼‰"""
        rows = []
        for model, metrics in self.results.items():
            row = {
                'Model': model,
                'R2': f"{metrics['R2']:.4f}" if metrics.get('R2') is not None else '-',
                'sMAPE(%)': f"{metrics['sMAPE']:.2f}",
                'MAE(s)': f"{metrics['MAE']:.2f}",
                'RMSE(s)': f"{metrics['RMSE']:.2f}",
                'Corr': f"{metrics['Corr']:.3f}" if metrics['Corr'] is not None else '-',
                'PICP-80(%)': f"{metrics['PICP_80']:.1f}" if metrics['PICP_80'] is not None else '-',
                'MPIW(s)': f"{metrics['MPIW_80']:.2f}" if metrics['MPIW_80'] is not None else '-',
                'ECE': f"{metrics['ECE']:.3f}" if metrics['ECE'] is not None else '-',
                'Params(K)': f"{metrics['Params_K']:.1f}" if metrics.get('Params_K') else '-',
                'Time(ms)': f"{metrics['Inference_ms']:.3f}"
            }
            rows.append(row)
        
        df = pd.DataFrame(rows)
        df.to_csv('evaluation_results/comparison_table.csv', index=False)
        
        print("\n" + "="*120)
        print("ğŸ“Š æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”è¡¨")
        print("="*120)
        print(df.to_string(index=False))
        print("="*120)
        
        # ç”ŸæˆLaTeXè¡¨æ ¼
        latex = self._generate_latex_table(rows)
        with open('evaluation_results/comparison_table.tex', 'w') as f:
            f.write(latex)
        print("LaTeXè¡¨æ ¼å·²ä¿å­˜è‡³ evaluation_results/comparison_table.tex")
        
        return df
    
    def _generate_latex_table(self, rows):
        """ç”Ÿæˆè®ºæ–‡ç”¨LaTeXè¡¨æ ¼"""
        latex = r"""\begin{table}[htbp]
\centering
\caption{æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”}
\label{tab:model_comparison}
\resizebox{\textwidth}{!}{
\begin{tabular}{lccccccccc}
\toprule
\textbf{Model} & \textbf{R\textsuperscript{2}} & \textbf{sMAPE(\%)} & \textbf{RMSE(s)} & \textbf{Corr} & \textbf{PICP-80(\%)} & \textbf{MPIW(s)} & \textbf{ECE} & \textbf{Params(K)} & \textbf{Time(ms)} \\
\midrule
"""
        for row in rows:
            latex += f"{row['Model']} & {row['R2']} & {row['sMAPE(%)']} & {row['RMSE(s)']} & {row['Corr']} & {row['PICP-80(%)']} & {row['MPIW(s)']} & {row['ECE']} & {row['Params(K)']} & {row['Time(ms)']} \\\\\n"
        
        latex += r"""\bottomrule
\end{tabular}
}
\begin{tablenotes}
\footnotesize
\item[1] R\textsuperscript{2} æ¥è¿‘1.0æºäºä¼ è¾“æ—¶é—´çš„å¼ºç‰©ç†ç¡®å®šæ€§ï¼ˆæ–‡ä»¶å¤§å°/å¸¦å®½ï¼‰ã€‚
\item[2] Corrã€PICPã€MPIWã€ECE ä¸ºä¸ç¡®å®šæ€§é‡åŒ–ä¸“å±æŒ‡æ ‡ï¼Œä¼ ç»Ÿæ ‘æ¨¡å‹æ— æ³•æä¾›ã€‚
\end{tablenotes}
\end{table}"""
        return latex
    
    def generate_radar_chart(self):
        """ç”Ÿæˆæ¨¡å‹ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾"""
        models = list(self.results.keys())
        # é›·è¾¾å›¾ç»´åº¦ï¼šç²¾åº¦ã€é£é™©æ„ŸçŸ¥ã€å¯é æ€§ã€è½»é‡åŒ–ã€æ¨ç†é€Ÿåº¦
        smapes = [self.results[m]['sMAPE'] for m in models]
        corrs = [self.results[m]['Corr'] if self.results[m]['Corr'] is not None else 0 for m in models]
        picps = [self.results[m]['PICP_80'] if self.results[m]['PICP_80'] is not None else 0 for m in models]
        inf_times = [self.results[m]['Inference_ms'] for m in models]
        params = [self.results[m]['Params_K'] if self.results[m]['Params_K'] is not None else 1000 for m in models]
        
        # å½’ä¸€åŒ–ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
        smape_norm = [max(0, 1 - s/50) for s in smapes]  # sMAPEè¶Šå°è¶Šå¥½
        corr_norm = [max(0, c) for c in corrs]  # Corrè¶Šå¤§è¶Šå¥½
        picp_norm = [p/100 for p in picps]  # PICPè¶Šå¤§è¶Šå¥½
        inf_norm = [1 - t/max(inf_times) for t in inf_times]  # æ¨ç†æ—¶é—´è¶Šå°è¶Šå¥½
        param_norm = [1 - p/max(params) for p in params]  # å‚æ•°é‡è¶Šå°è¶Šå¥½
        
        categories = [
            'é¢„æµ‹ç²¾åº¦\n(sMAPEâ†“)', 
            'é£é™©æ„ŸçŸ¥\n(Corrâ†‘)', 
            'å¯é æ€§\n(PICPâ†‘)', 
            'æ¨ç†é€Ÿåº¦\n(Timeâ†“)',
            'è½»é‡åŒ–\n(Paramsâ†“)'
        ]
        N = len(categories)
        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(projection='polar'))
        
        # é…è‰²ï¼šCFT-Netç”¨çªå‡ºçš„ç»¿è‰²ï¼ŒåŸºçº¿ç”¨ç°è‰²ç³»
        colors = ['#808080', '#808080', '#808080', '#2ca02c'] if len(models) == 4 else ['#808080']*(len(models)-1) + ['#2ca02c']
        
        for i, model in enumerate(models):
            values = [smape_norm[i], corr_norm[i], picp_norm[i], inf_norm[i], param_norm[i]]
            values += values[:1]
            linewidth = 3 if model == 'CFT-Net' else 1.5
            alpha = 0.2 if model == 'CFT-Net' else 0.05
            ax.plot(angles, values, 'o-', linewidth=linewidth, label=model, color=colors[i])
            ax.fill(angles, values, alpha=alpha, color=colors[i])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, fontsize=13, fontweight='bold')
        ax.set_ylim(0, 1)
        ax.set_title('æ¨¡å‹ç»¼åˆèƒ½åŠ›é›·è¾¾å›¾\nï¼ˆCFT-Net vs ä¼ ç»ŸåŸºçº¿æ¨¡å‹ï¼‰', fontsize=16, fontweight='bold', pad=40)
        ax.legend(loc='upper right', bbox_to_anchor=(1.5, 1.1), fontsize=12)
        plt.tight_layout()
        plt.savefig('evaluation_results/radar_chart.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("âœ… é›·è¾¾å›¾å·²ä¿å­˜è‡³ evaluation_results/radar_chart.png")
    
    def plot_calibration_curve(self):
        """ç»˜åˆ¶æ ¡å‡†æ›²çº¿ä¸æ®‹å·®åˆ†å¸ƒ"""
        if 'CFT-Net' not in self.results:
            return
        
        preds = self.results['CFT-Net']['predictions']
        uncs = self.results['CFT-Net']['uncertainties']
        errors = np.abs(self.y_test_orig - preds)
        
        # åˆ†ç®±è®¡ç®—æ ¡å‡†æ›²çº¿
        n_bins = 10
        quantiles = np.linspace(0, 100, n_bins + 1)
        bin_edges = np.percentile(uncs, quantiles)
        bin_edges[-1] += 1e-8
        
        bin_centers = []
        avg_errors = []
        avg_uncertainties = []
        
        for i in range(n_bins):
            in_bin = (uncs >= bin_edges[i]) & (uncs < bin_edges[i+1])
            if i == n_bins - 1:
                in_bin = (uncs >= bin_edges[i]) & (uncs <= bin_edges[i+1])
            if in_bin.sum() > 0:
                bin_centers.append((bin_edges[i] + bin_edges[i+1]) / 2)
                avg_errors.append(errors[in_bin].mean())
                avg_uncertainties.append(uncs[in_bin].mean())
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        
        # å·¦å›¾ï¼šæ ¡å‡†æ›²çº¿
        ax1.plot(avg_uncertainties, avg_errors, 'o-', linewidth=2, markersize=8, label='å®é™…ç»å¯¹è¯¯å·®', color='#2ca02c')
        ax1.plot(avg_uncertainties, avg_uncertainties, 'r--', linewidth=2, label='å®Œç¾æ ¡å‡†çº¿')
        ax1.fill_between(avg_uncertainties, avg_errors, avg_uncertainties, alpha=0.2, color='red')
        ax1.set_xlabel('å¹³å‡é¢„æµ‹ä¸ç¡®å®šæ€§ (s)', fontsize=12)
        ax1.set_ylabel('å¹³å‡ç»å¯¹è¯¯å·® (s)', fontsize=12)
        ax1.set_title('CFT-Net æ ¡å‡†æ›²çº¿', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(alpha=0.3)
        
        # å³å›¾ï¼šæ®‹å·®åˆ†å¸ƒ
        residuals = self.y_test_orig - preds
        ax2.hist(residuals, bins=50, edgecolor='black', alpha=0.7, color='#2ca02c')
        ax2.axvline(x=0, color='r', linestyle='--', linewidth=2)
        ax2.set_xlabel('é¢„æµ‹æ®‹å·® (çœŸå®å€¼-é¢„æµ‹å€¼, s)', fontsize=12)
        ax2.set_ylabel('æ ·æœ¬é¢‘æ•°', fontsize=12)
        ax2.set_title('é¢„æµ‹æ®‹å·®åˆ†å¸ƒ', fontsize=14, fontweight='bold')
        ax2.grid(alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('evaluation_results/calibration_analysis.png', dpi=300)
        plt.close()
        print("âœ… æ ¡å‡†åˆ†æå›¾å·²ä¿å­˜è‡³ evaluation_results/calibration_analysis.png")
    
    def plot_prediction_intervals(self):
        """ç»˜åˆ¶å…¨é‡æµ‹è¯•é›†é¢„æµ‹åŒºé—´å›¾"""
        if 'CFT-Net' not in self.results:
            return
        
        # ä½¿ç”¨å…¨é‡æµ‹è¯•é›†ï¼ŒæŒ‰çœŸå®å€¼æ’åºä¾¿äºè§‚å¯Ÿ
        indices = np.argsort(self.y_test_orig)
        n_samples = len(indices)
        
        preds = self.results['CFT-Net']['predictions'][indices]
        uncs = self.results['CFT-Net']['uncertainties'][indices]
        y_true = self.y_test_orig[indices]
        
        # 80%ç½®ä¿¡åŒºé—´
        z = norm.ppf((1 + 0.8) / 2)
        lower = preds - z * uncs
        upper = preds + z * uncs
        
        # è®¡ç®—å®é™…PICP
        covered = (y_true >= lower) & (y_true <= upper)
        picp_actual = covered.mean() * 100
        not_covered_count = (~covered).sum()
        
        plt.figure(figsize=(16, 7))
        x = np.arange(n_samples)
        
        # ç»˜åˆ¶é¢„æµ‹åŒºé—´ã€é¢„æµ‹å€¼ã€çœŸå®å€¼
        plt.fill_between(x, lower, upper, alpha=0.3, color='#1f77b4', label='80%é¢„æµ‹åŒºé—´')
        plt.plot(x, preds, 'b-', linewidth=1.2, label='é¢„æµ‹å€¼', alpha=0.8)
        plt.scatter(x, y_true, c='black', s=2, zorder=5, label='çœŸå®å€¼', alpha=0.4)
        
        # æ ‡è®°æœªè¦†ç›–çš„ç‚¹
        not_covered_idx = np.where(~covered)[0]
        if len(not_covered_idx) > 0:
            # æ ·æœ¬è¿‡å¤šæ—¶éšæœºé‡‡æ ·æ˜¾ç¤ºï¼Œé¿å…è¿‡äºå¯†é›†
            display_count = min(200, len(not_covered_idx))
            np.random.seed(self.seed)
            display_idx = np.random.choice(not_covered_idx, display_count, replace=False)
            plt.scatter(display_idx, y_true[display_idx], c='red', s=25, marker='x', 
                       linewidth=2, label=f'æœªè¦†ç›–æ ·æœ¬ (n={not_covered_count})', zorder=6)
        
        plt.xlabel('æ ·æœ¬ç´¢å¼•ï¼ˆæŒ‰çœŸå®ä¼ è¾“æ—¶é—´å‡åºæ’åˆ—ï¼‰', fontsize=12)
        plt.ylabel('ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
        plt.title(f'CFT-Net å…¨é‡æµ‹è¯•é›†é¢„æµ‹åŒºé—´å¯è§†åŒ– (n={n_samples}, å®é™…PICP={picp_actual:.1f}%)', 
                 fontsize=14, fontweight='bold')
        plt.legend(fontsize=11, loc='upper left')
        plt.grid(alpha=0.3)
        plt.tight_layout()
        plt.savefig('evaluation_results/prediction_intervals.png', dpi=300)
        plt.close()
        print(f"âœ… é¢„æµ‹åŒºé—´å›¾å·²ä¿å­˜ (å…¨é‡{n_samples}ä¸ªæ ·æœ¬, å®é™…PICP={picp_actual:.1f}%)")
    
    def plot_pred_vs_actual(self):
        """ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹å€¼vsçœŸå®å€¼æ•£ç‚¹å›¾"""
        if 'CFT-Net' not in self.results:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 12))
        fig.suptitle('é¢„æµ‹å€¼ vs çœŸå®å€¼å¯¹æ¯” (Prediction vs Actual)', fontsize=16, fontweight='bold', y=0.98)
        
        models_to_plot = ['CFT-Net', 'RandomForest', 'XGBoost', 'LightGBM']
        colors = ['#2ca02c', '#808080', '#808080', '#808080']
        
        for idx, (model, color) in enumerate(zip(models_to_plot, colors)):
            if model not in self.results:
                continue
            
            ax = axes[idx // 2, idx % 2]
            preds = self.results[model]['predictions']
            y_true = self.y_test_orig
            
            # æ ¸å¿ƒæŒ‡æ ‡
            r2 = self.results[model]['R2']
            smape = self.results[model]['sMAPE']
            
            # æ•£ç‚¹å›¾
            ax.scatter(y_true, preds, alpha=0.4, s=10, c=color, edgecolors='none')
            
            # å®Œç¾é¢„æµ‹çº¿
            min_val = min(y_true.min(), preds.min())
            max_val = max(y_true.max(), preds.max())
            ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹çº¿')
            
            # Â±20%è¯¯å·®å¸¦
            ax.plot([min_val, max_val], [min_val*0.8, max_val*0.8], 'k:', linewidth=1, alpha=0.5, label='Â±20%è¯¯å·®å¸¦')
            ax.plot([min_val, max_val], [min_val*1.2, max_val*1.2], 'k:', linewidth=1, alpha=0.5)
            
            ax.set_xlabel('çœŸå®ä¼ è¾“æ—¶é—´ (s)', fontsize=11)
            ax.set_ylabel('é¢„æµ‹ä¼ è¾“æ—¶é—´ (s)', fontsize=11)
            ax.set_title(f'{model}\nRÂ²={r2:.4f}, sMAPE={smape:.2f}%', fontsize=12, fontweight='bold')
            ax.legend(loc='upper left', fontsize=9)
            ax.grid(alpha=0.3)
            ax.set_xlim(min_val, max_val)
            ax.set_ylim(min_val, max_val)
        
        plt.tight_layout()
        plt.savefig('evaluation_results/pred_vs_actual_all.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # ç»˜åˆ¶CFT-Netä¸“å±å¸¦ä¸ç¡®å®šæ€§ç€è‰²çš„ç‰ˆæœ¬
        self._plot_cftnet_detailed_scatter()
        print("âœ… Pred vs Actual å¯¹æ¯”å›¾å·²ä¿å­˜")
    
    def _plot_cftnet_detailed_scatter(self):
        """CFT-Netä¸“å±æ•£ç‚¹å›¾ï¼ŒæŒ‰ä¸ç¡®å®šæ€§å¤§å°ç€è‰²"""
        fig, ax = plt.subplots(figsize=(10, 10))
        
        preds = self.results['CFT-Net']['predictions']
        uncs = self.results['CFT-Net']['uncertainties']
        y_true = self.y_test_orig
        
        # æŒ‰ä¸ç¡®å®šæ€§ç€è‰²
        scatter = ax.scatter(y_true, preds, c=uncs, cmap='viridis', alpha=0.6, s=15, edgecolors='none')
        plt.colorbar(scatter, ax=ax, label='é¢„æµ‹ä¸ç¡®å®šæ€§ (s)')
        
        # å®Œç¾é¢„æµ‹çº¿
        min_val = min(y_true.min(), preds.min())
        max_val = max(y_true.max(), preds.max())
        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='å®Œç¾é¢„æµ‹çº¿')
        
        # æŒ‡æ ‡æ ‡æ³¨
        r2 = self.results['CFT-Net']['R2']
        smape = self.results['CFT-Net']['sMAPE']
        corr = self.results['CFT-Net']['Corr']
        picp = self.results['CFT-Net']['PICP_80']
        
        ax.set_xlabel('çœŸå®ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
        ax.set_ylabel('é¢„æµ‹ä¼ è¾“æ—¶é—´ (s)', fontsize=12)
        ax.set_title(
            f'CFT-Net é¢„æµ‹è¯¦æƒ…ï¼ˆæŒ‰ä¸ç¡®å®šæ€§ç€è‰²ï¼‰\n'
            f'RÂ²={r2:.4f}, sMAPE={smape:.2f}%, Corr={corr:.3f}, PICP={picp:.1f}%', 
            fontsize=13, fontweight='bold'
        )
        ax.legend(loc='upper left', fontsize=10)
        ax.grid(alpha=0.3)
        ax.set_xlim(min_val, max_val)
        ax.set_ylim(min_val, max_val)
        
        plt.tight_layout()
        plt.savefig('evaluation_results/pred_vs_actual_cftnet_detailed.png', dpi=300)
        plt.close()
        print("âœ… CFT-Net è¯¦ç»†æ•£ç‚¹å›¾å·²ä¿å­˜")
    
    def analyze_physical_determinism(self):
        """åˆ†æä¼ è¾“æ—¶é—´çš„ç‰©ç†ç¡®å®šæ€§ï¼Œè§£é‡Šé«˜RÂ²çš„åˆç†æ€§"""
        print("\n" + "="*60)
        print("ğŸ” ä¼ è¾“æ—¶é—´ç‰©ç†ç¡®å®šæ€§åˆ†æ")
        print("="*60)
        
        # åæ ‡å‡†åŒ–è·å–åŸå§‹ç‰¹å¾
        Xc_test_orig = self.scaler_c.inverse_transform(self.Xc_test)
        Xi_test_orig = self.scaler_i.inverse_transform(self.Xi_test)
        
        # æ‰¾åˆ°æ ¸å¿ƒç‰©ç†ç‰¹å¾çš„ç´¢å¼•
        size_idx = self.cols_i.index('total_size_mb') if 'total_size_mb' in self.cols_i else -1
        bw_idx = self.cols_c.index('bandwidth_mbps') if 'bandwidth_mbps' in self.cols_c else -1
        
        if size_idx >= 0 and bw_idx >= 0:
            total_size_mb = Xi_test_orig[:, size_idx]
            bandwidth_mbps = Xc_test_orig[:, bw_idx]
            
            # ç†è®ºä¼ è¾“æ—¶é—´ï¼ˆå¿½ç•¥å‹ç¼©ã€åè®®å¼€é”€ï¼‰
            # å…¬å¼ï¼šæ—¶é—´(s) = æ–‡ä»¶å¤§å°(MB) / å¸¦å®½(MB/s) = æ–‡ä»¶å¤§å°(MB) / (å¸¦å®½(Mbps)/8)
            theoretical_time = total_size_mb / (bandwidth_mbps / 8)
            actual_time = self.y_test_orig
            
            # ç›¸å…³æ€§åˆ†æ
            correlation = np.corrcoef(theoretical_time, actual_time)[0, 1]
            r2_theoretical = correlation ** 2
            
            print(f"ç†è®ºä¼ è¾“æ—¶é—´ vs å®é™…ä¼ è¾“æ—¶é—´ çš®å°”é€Šç›¸å…³ç³»æ•°: {correlation:.4f}")
            print(f"ç†è®ºå…¬å¼å¯è§£é‡Šçš„RÂ²: {r2_theoretical:.4f}")
            print(f"ç†è®ºæ—¶é—´èŒƒå›´: [{theoretical_time.min():.2f}, {theoretical_time.max():.2f}] s")
            print(f"å®é™…æ—¶é—´èŒƒå›´: [{actual_time.min():.2f}, {actual_time.max():.2f}] s")
            
            print("\nğŸ’¡ é«˜RÂ²åˆç†æ€§è¯´æ˜ï¼š")
            print("å®¹å™¨é•œåƒä¼ è¾“æ—¶é—´ç”±å¼ºç‰©ç†è§„å¾‹ä¸»å¯¼ï¼Œæ ¸å¿ƒå…¬å¼ä¸ºï¼š")
            print("  ä¼ è¾“æ—¶é—´ â‰ˆ é•œåƒæ€»å¤§å° / æœ‰æ•ˆä¼ è¾“å¸¦å®½ + å›ºå®šå¼€é”€")
            print(f"ä»…ã€Œå¤§å°/å¸¦å®½ã€çš„åŸºç¡€å…¬å¼å³å¯è§£é‡Š {r2_theoretical*100:.1f}% çš„æ—¶é—´æ³¢åŠ¨ï¼Œ")
            print("å› æ­¤æ¨¡å‹RÂ²æ¥è¿‘1.0æ˜¯ç¬¦åˆç‰©ç†è§„å¾‹çš„ï¼Œå¹¶éè¿‡æ‹Ÿåˆã€‚")
            print("CFT-Netçš„æ ¸å¿ƒä»·å€¼åœ¨äºé‡åŒ–å…¬å¼æ— æ³•è¦†ç›–çš„éšæœºæ³¢åŠ¨ï¼ˆå‹ç¼©ç‡ã€ç½‘ç»œæŠ–åŠ¨ã€å®¿ä¸»æœºè´Ÿè½½ç­‰ï¼‰ï¼Œ")
            print("æä¾›å¯é çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œä¸ºè°ƒåº¦å†³ç­–æä¾›é£é™©æ„ŸçŸ¥èƒ½åŠ›ã€‚")
        
        print("="*60)
    
    def run_full_evaluation(self):
        """æ‰§è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        self.calibrate_cftnet()
        self.evaluate_cftnet()
        self.train_baselines()
        self.analyze_physical_determinism()
        self.generate_comparison_table()
        self.generate_radar_chart()
        self.plot_calibration_curve()
        self.plot_prediction_intervals()
        self.plot_pred_vs_actual()
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        with open('evaluation_results/full_evaluation_results.json', 'w') as f:
            json.dump({k: {kk: vv for kk, vv in v.items() if kk not in ['predictions', 'uncertainties', 'raw_uncertainties']} for k, v in self.results.items()}, f, indent=2)
        
        print("\nğŸ‰ æ‰€æœ‰è¯„ä¼°æµç¨‹å®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ evaluation_results/ ç›®å½•")

# ==============================================================================
# 5. ä¸»ç¨‹åºå…¥å£
# ==============================================================================
if __name__ == "__main__":
    # è¯·ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
    MODEL_PATH = "cts_optimized_0218_2125_seed42.pth"
    
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_PATH}")
        print("è¯·ä¿®æ”¹è„šæœ¬ä¸­ MODEL_PATH ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„")
        exit(1)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨å¹¶æ‰§è¡Œå®Œæ•´è¯„ä¼°
    evaluator = ModelEvaluator(MODEL_PATH, seed=SEED)
    evaluator.run_full_evaluation()