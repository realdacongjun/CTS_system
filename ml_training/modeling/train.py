import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split

# ==============================================================================
# 1. é…ç½®åŒºåŸŸ (Hyperparameters)
# ==============================================================================
CONFIG = {
    "data_path": "cts_data.xlsx",         
    "feature_path": "image_features_database.csv",
    "batch_size": 64,
    "lr": 0.001,
    "epochs": 200,             # å¢åŠ è®­ç»ƒè½®æ•°ï¼Œè®©æ¨¡å‹å……åˆ†è®­ç»ƒ
    "embed_dim": 32,
    "kl_coeff": 0.15,          # [è°ƒæ•´] å¢åŠ æ­£åˆ™åŒ–æƒé‡ï¼Œè®©ä¸ç¡®å®šæ€§æ›´å‡†ç¡®
    "model_save_path": "cts_best_model_full.pth" 
}

# è·¯å¾„æ£€æŸ¥ä¸è‡ªåŠ¨ä¿®æ­£ (è§£å†³ä½ çš„è·¯å¾„çƒ¦æ¼)
# å¦‚æœå½“å‰ç›®å½•ä¸‹æ‰¾ä¸åˆ°ï¼Œå°è¯•å»ä¸Šä¸€çº§ç›®å½•æ‰¾
if not os.path.exists(CONFIG["data_path"]):
    if os.path.exists(f"../{CONFIG['data_path']}"):
        CONFIG["data_path"] = f"../{CONFIG['data_path']}"
        CONFIG["feature_path"] = f"../{CONFIG['feature_path']}"
        print(f"ğŸ“‚ è‡ªåŠ¨åˆ‡æ¢æ•°æ®è·¯å¾„åˆ°ä¸Šä¸€çº§: {CONFIG['data_path']}")

# ==============================================================================
# ğŸŒŸ æ ¸å¿ƒæ–°å¢: è¯æ®æ·±åº¦å­¦ä¹ æŸå¤±å‡½æ•° (NIG Loss)
# ==============================================================================
# å‚è€ƒæ–‡çŒ®: Deep Evidential Regression (Amini et al., NeurIPS 2020)
def nig_nll_loss(y, gamma, v, alpha, beta):
    """è®¡ç®—è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤± (NLL): è®©é¢„æµ‹å€¼(gamma)æ¥è¿‘çœŸå®å€¼(y)"""
    two_blambda = 2 * beta * (1 + v)
    nll = 0.5 * torch.log(np.pi / v) \
        - alpha * torch.log(two_blambda) \
        + (alpha + 0.5) * torch.log(v * (y - gamma)**2 + two_blambda) \
        + torch.lgamma(alpha) - torch.lgamma(alpha + 0.5)
    return nll.mean()

def nig_reg_loss(y, gamma, v, alpha, beta):
    """è®¡ç®—æ­£åˆ™åŒ–æŸå¤±: æƒ©ç½šæ¨¡å‹åœ¨é¢„æµ‹é”™è¯¯æ—¶è¿˜ç›²ç›®è‡ªä¿¡"""
    error = torch.abs(y - gamma)
    evidence = 2 * v + alpha
    return (error * evidence).mean()

def evidential_loss(pred, target, epoch, total_epochs, lambda_coef=CONFIG["kl_coeff"]):
    """æ€»æŸå¤± = NLL + åŠ¨æ€æƒé‡çš„æ­£åˆ™é¡¹"""
    gamma, v, alpha, beta = pred[:, 0], pred[:, 1], pred[:, 2], pred[:, 3]
    target = target.view(-1)
    
    loss_nll = nig_nll_loss(target, gamma, v, alpha, beta)
    loss_reg = nig_reg_loss(target, gamma, v, alpha, beta)
    
    # åŠ¨æ€è°ƒæ•´æ­£åˆ™åŒ–ç³»æ•° (Annealing): å‰æœŸå…³æ³¨æ‹Ÿåˆï¼ŒåæœŸå…³æ³¨ä¸ç¡®å®šæ€§æ ¡å‡†
    annealing_coef = min(1.0, epoch / (total_epochs * 0.15))  # [è°ƒæ•´] 15%çš„è®­ç»ƒè½®æ•°ç”¨äºé€€ç«
    
    return loss_nll + lambda_coef * annealing_coef * loss_reg

# ==============================================================================
# 2. æ¨¡å‹å®šä¹‰ (å¿…é¡»ä¸ cags_run.py ä¿æŒä¸€è‡´)
# ==============================================================================
class FeatureTokenizer(nn.Module):
    def __init__(self, num_features, embed_dim):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(num_features, embed_dim))
        self.biases = nn.Parameter(torch.randn(num_features, embed_dim))
        nn.init.xavier_uniform_(self.weights)
        nn.init.zeros_(self.biases)

    def forward(self, x):
        return x.unsqueeze(-1) * self.weights + self.biases

class TransformerTower(nn.Module):
    def __init__(self, num_features, embed_dim, nhead=4, num_layers=2):
        super().__init__()
        self.tokenizer = FeatureTokenizer(num_features, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=nhead, dim_feedforward=embed_dim*4,
            batch_first=True, dropout=0.1
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        tokens = self.tokenizer(x)
        batch_size = x.shape[0]
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        tokens = torch.cat((cls_tokens, tokens), dim=1)
        out = self.transformer(tokens)
        return out[:, 0, :]

class CTSDualTowerModel(nn.Module):
    def __init__(self, client_feats, image_feats, num_algos, embed_dim=32):
        super().__init__()
        self.client_tower = TransformerTower(client_feats, embed_dim)
        self.image_tower = TransformerTower(image_feats, embed_dim)
        self.algo_embed = nn.Embedding(num_algos, embed_dim)
        
        fusion_input_dim = embed_dim * 3 
        self.hidden = nn.Sequential(
            nn.Linear(fusion_input_dim, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.2)
        )
        # [ä¿®æ”¹ç‚¹] è¾“å‡ºå±‚ 4 ä¸ªç¥ç»å…ƒ (Gamma, v, Alpha, Beta)
        self.head = nn.Linear(64, 4) 

    def forward(self, cx, ix, ax):
        c_vec = self.client_tower(cx)
        i_vec = self.image_tower(ix)
        a_vec = self.algo_embed(ax)
        combined = torch.cat([c_vec, i_vec, a_vec], dim=1)
        hidden = self.hidden(combined)
        out = self.head(hidden)
        
        # [ä¿®æ”¹ç‚¹] æ–½åŠ æ•°å­¦çº¦æŸ (Softplus)
        gamma = out[:, 0]
        v     = F.softplus(out[:, 1]) + 1e-6
        alpha = F.softplus(out[:, 2]) + 1.0 + 1e-6
        beta  = F.softplus(out[:, 3]) + 1e-6
        
        return torch.stack([gamma, v, alpha, beta], dim=1)

# ==============================================================================
# 3. æ•°æ®å¤„ç†ä¸åŠ è½½
# ==============================================================================
def load_data():
    print(f"ğŸ”„ 1. æ­£åœ¨è¯»å–æ•°æ®: {CONFIG['data_path']} ...")
    try:
        df_exp = pd.read_excel(CONFIG["data_path"])
    except ImportError:
        print("âŒ è¯»å–å¤±è´¥ï¼è¯·è¿è¡Œ 'pip install openpyxl'")
        exit(1)

    rename_map = {
        "image": "image_name", "method": "algo_name",
        "network_bw": "bandwidth_mbps", "network_delay": "network_rtt",
        "mem_limit": "mem_limit_mb"
    }
    df_exp = df_exp.rename(columns=rename_map)
    
    if 'total_time' not in df_exp.columns:
        possible_cols = [c for c in df_exp.columns if 'total_tim' in c]
        if possible_cols: df_exp = df_exp.rename(columns={possible_cols[0]: 'total_time'})

    df_exp = df_exp[(df_exp['status'] == 'SUCCESS') & (df_exp['total_time'] > 0)]
    
    if 'mem_limit_mb' not in df_exp.columns: df_exp['mem_limit_mb'] = 1024.0
    
    print(f"ğŸ”„ 2. è¯»å–é•œåƒç‰¹å¾: {CONFIG['feature_path']} ...")
    df_feat = pd.read_csv(CONFIG["feature_path"])
    
    df = pd.merge(df_exp, df_feat, on="image_name", how="inner")
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ ·æœ¬æ•°: {len(df)}")
    return df

class CTSDataset(Dataset):
    def __init__(self, client_x, image_x, algo_x, y):
        self.cx = torch.FloatTensor(client_x)
        self.ix = torch.FloatTensor(image_x)
        self.ax = torch.LongTensor(algo_x)
        self.y = torch.FloatTensor(y)
    def __len__(self): return len(self.y)
    def __getitem__(self, idx): return self.cx[idx], self.ix[idx], self.ax[idx], self.y[idx]

# ==============================================================================
# 4. ä¸»è®­ç»ƒæµç¨‹ (å« EDL è®­ç»ƒé€»è¾‘)
# ==============================================================================
if __name__ == "__main__":
    # --- Step 1: å‡†å¤‡æ•°æ® ---
    df = load_data()
    
    col_client = ['bandwidth_mbps', 'cpu_limit', 'network_rtt', 'mem_limit_mb']
    col_image = ['total_size_mb', 'avg_layer_entropy', 'text_ratio', 'layer_count', 'zero_ratio']
    
    scaler_c = StandardScaler()
    X_client = scaler_c.fit_transform(df[col_client].values)
    
    scaler_i = StandardScaler()
    X_image = scaler_i.fit_transform(df[col_image].values)
    
    enc_algo = LabelEncoder()
    X_algo = enc_algo.fit_transform(df['algo_name'].values)
    
    y_target = np.log1p(df['total_time'].values) # Log å˜æ¢

    Xc_train, Xc_test, Xi_train, Xi_test, Xa_train, Xa_test, y_train, y_test = train_test_split(
        X_client, X_image, X_algo, y_target, test_size=0.2, random_state=42
    )
    
    train_loader = DataLoader(CTSDataset(Xc_train, Xi_train, Xa_train, y_train), batch_size=CONFIG["batch_size"], shuffle=True)
    test_loader = DataLoader(CTSDataset(Xc_test, Xi_test, Xa_test, y_test), batch_size=CONFIG["batch_size"])
    
    # --- Step 2: æ¨¡å‹åˆå§‹åŒ– ---
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸ è®­ç»ƒè®¾å¤‡: {device}")
    
    model = CTSDualTowerModel(
        client_feats=len(col_client),
        image_feats=len(col_image),
        num_algos=len(enc_algo.classes_)
    ).to(device)
    
    optimizer = optim.Adam(model.parameters(), lr=CONFIG["lr"])
    
    # --- Step 3: è®­ç»ƒ (EDL Loop) ---
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ (è¯æ®æ·±åº¦å­¦ä¹ ç‰ˆ - Uncertainty Aware)...")
    best_loss = float('inf')
    
    for epoch in range(CONFIG["epochs"]):
        model.train()
        train_loss = 0
        
        for cx, ix, ax, y in train_loader:
            cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ (è¾“å‡º4ä¸ªå‚æ•°)
            preds = model(cx, ix, ax)
            
            # è®¡ç®— EDL æŸå¤± (NLL + Regularization)
            loss = evidential_loss(preds, y, epoch, CONFIG["epochs"])
            
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            
        # éªŒè¯é›† (åªçœ‹ NLL å³å¯ï¼ŒéªŒè¯é¢„æµ‹å‡†ä¸å‡†)
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for cx, ix, ax, y in test_loader:
                cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
                preds = model(cx, ix, ax)
                gamma, v, alpha, beta = preds[:, 0], preds[:, 1], preds[:, 2], preds[:, 3]
                # éªŒè¯é›†ä¸éœ€è¦æ­£åˆ™é¡¹ï¼Œåªç®— NLL
                val_loss += nig_nll_loss(y, gamma, v, alpha, beta).item()
        
        avg_train = train_loss / len(train_loader)
        avg_val = val_loss / len(test_loader)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1:03d} | Train Loss: {avg_train:.4f} | Val NLL: {avg_val:.4f}")
        
        if avg_val < best_loss:
            best_loss = avg_val
            torch.save(model.state_dict(), CONFIG["model_save_path"])

    print(f"\nğŸ’¾ è®­ç»ƒç»“æŸï¼æ¨¡å‹ä¿å­˜è‡³: {os.path.abspath(CONFIG['model_save_path'])}")
    
    # --- Step 4: æ¼”ç¤º (å«ä¸ç¡®å®šæ€§) ---
    print("\nğŸ”® é¢„æµ‹æ•ˆæœä¸ä¸ç¡®å®šæ€§æ¼”ç¤º:")
    model.load_state_dict(torch.load(CONFIG["model_save_path"]))
    model.eval()
    
    with torch.no_grad():
        cx, ix, ax, y = next(iter(test_loader))
        cx, ix, ax, y = cx.to(device), ix.to(device), ax.to(device), y.to(device)
        
        # é¢„æµ‹
        preds = model(cx, ix, ax)
        gamma, v, alpha, beta = preds[:, 0], preds[:, 1], preds[:, 2], preds[:, 3]
        
        print(f"{'ç®—æ³•':<12} | {'é¢„æµ‹(s)':<10} | {'ä¸ç¡®å®šæ€§(U)':<12} | {'çœŸå®(s)':<10}")
        print("-" * 60)
        
        for i in range(5):
            pred_s = np.expm1(gamma[i].item())
            real_s = np.expm1(y[i].item())
            
            # è®¡ç®—ä¸ç¡®å®šæ€§: Aleatoric + Epistemic
            # Uncertainty = Beta / (v * (Alpha - 1))
            uncertainty = beta[i] / (v[i] * (alpha[i] - 1))
            
            algo = enc_algo.inverse_transform([ax[i].item()])[0]
            
            print(f"{algo:<12} | {pred_s:<10.2f} | {uncertainty.item():<12.4f} | {real_s:<10.2f}")
    
    print("-" * 60)
    print("âœ… æ³¨æ„: 'ä¸ç¡®å®šæ€§(U)' è¶Šå¤§ï¼Œä»£è¡¨æ¨¡å‹å¯¹è¯¥é¢„æµ‹è¶Šæ²¡æŠŠæ¡ (CAGS å°†å› æ­¤è§¦å‘é£é™©æ”¾å¤§æœºåˆ¶)ã€‚")