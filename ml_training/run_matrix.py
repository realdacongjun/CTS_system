import os
import sys
import time
import json
import sqlite3
import logging
import subprocess
import shutil
import docker
import numpy as np
from datetime import datetime
from config import CLIENT_PROFILES, TARGET_IMAGES, COMPRESSION_METHODS, REPETITIONS, DB_PATH, TEMP_DIR, CLIENT_IMAGE

# === æ—¥å¿—é…ç½® ===
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("experiment.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class ExperimentOrchestrator:
    def __init__(self):
        self.docker_client = docker.from_env()
        self.conn = sqlite3.connect(DB_PATH)
        self._init_db()
        self._check_dependencies()
        
        if not os.path.exists(TEMP_DIR):
            os.makedirs(TEMP_DIR)

    def _init_db(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        cursor = self.conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS experiments (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                image TEXT,
                client_profile TEXT,
                method TEXT,
                rep_id INTEGER,
                status TEXT, -- 'SUCCESS', 'FAILED', 'ABNORMAL'
                download_time REAL,
                decomp_time REAL,
                total_time REAL,
                cpu_usage REAL,
                mem_usage REAL,
                compressed_size INTEGER,
                original_size INTEGER,
                bandwidth_measured REAL,
                is_noise BOOLEAN,
                error_msg TEXT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(image, client_profile, method, rep_id)
            )
        ''')
        self.conn.commit()

    def _check_dependencies(self):
        """æ£€æŸ¥çŽ¯å¢ƒä¾èµ–"""
        try:
            subprocess.run(['pumba', '--version'], check=True, stdout=subprocess.PIPE)
            subprocess.run(['tc', '-V'], check=True, stdout=subprocess.PIPE)
            logger.info("âœ… çŽ¯å¢ƒä¾èµ–æ£€æŸ¥é€šè¿‡ (Docker, Pumba, tc)")
        except (subprocess.CalledProcessError, FileNotFoundError):
            logger.error("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–å·¥å…· (Pumba æˆ– tc)ï¼Œè¯·å…ˆå®‰è£…ã€‚")
            sys.exit(1)

    def _clear_system_cache(self):
        """æ¸…ç†ç³»ç»Ÿç¼“å­˜ä»¥ä¿è¯å®žéªŒå‡†ç¡®æ€§"""
        try:
            subprocess.run('sync', shell=True)
            subprocess.run('echo 3 > /proc/sys/vm/drop_caches', shell=True)
        except Exception as e:
            logger.warning(f"æ— æ³•æ¸…ç†ç³»ç»Ÿç¼“å­˜ (å¯èƒ½éœ€è¦sudo): {e}")

    def is_experiment_done(self, image, profile, method, rep):
        """æ£€æŸ¥å®žéªŒæ˜¯å¦å·²ç»å®Œæˆï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰"""
        cursor = self.conn.cursor()
        cursor.execute('''
            SELECT count(*) FROM experiments 
            WHERE image=? AND client_profile=? AND method=? AND rep_id=? AND status='SUCCESS'
        ''', (image, profile, method, rep))
        return cursor.fetchone()[0] > 0

    def prepare_image_payload(self, image_name, method_name):
        """
        1. æ‹‰å–é•œåƒ
        2. å¯¼å‡ºä¸ºTar
        3. åŽ‹ç¼©
        è¿”å›ž: (åŽ‹ç¼©æ–‡ä»¶è·¯å¾„, åŽŸå§‹å¤§å°, åŽ‹ç¼©åŽå¤§å°)
        """
        safe_img_name = image_name.replace(':', '_').replace('/', '_')
        raw_tar_path = os.path.join(TEMP_DIR, f"{safe_img_name}.tar")
        
        # 1. æ‹‰å–é•œåƒ
        logger.info(f"æ­£åœ¨æ‹‰å–é•œåƒ: {image_name}")
        self.docker_client.images.pull(image_name)
        
        # 2. å¯¼å‡ºä¸ºTar (æ¨¡æ‹Ÿæå–é•œåƒå±‚)
        # æ³¨æ„ï¼šçœŸå®žåœºæ™¯å¯èƒ½éœ€è¦æå–ç‰¹å®šLayerï¼Œè¿™é‡Œä¸ºäº†ç®€åŒ–æ¨¡æ‹Ÿï¼Œå¯¼å‡ºæ•´ä¸ªImage Tarä½œä¸ºpayload
        image = self.docker_client.images.get(image_name)
        with open(raw_tar_path, 'wb') as f:
            for chunk in image.save():
                f.write(chunk)
        
        original_size = os.path.getsize(raw_tar_path)
        
        # 3. åŽ‹ç¼©
        cmd_args = COMPRESSION_METHODS[method_name]
        # æž„é€ è¾“å‡ºæ–‡ä»¶å (ä¾‹å¦‚ .tar.gz, .tar.zst)
        if 'gzip' in method_name: ext = '.gz'
        elif 'zstd' in method_name: ext = '.zst'
        elif 'lz4' in method_name: ext = '.lz4'
        elif 'brotli' in method_name: ext = '.br'
        else: ext = '.dat'
        
        compressed_path = raw_tar_path + ext
        
        # æ‰§è¡ŒåŽ‹ç¼©å‘½ä»¤
        logger.info(f"æ­£åœ¨åŽ‹ç¼© ({method_name}): {raw_tar_path} -> {compressed_path}")
        start_time = time.time()
        
        # é’ˆå¯¹ä¸åŒå·¥å…·çš„å‘½ä»¤é€‚é…
        if 'gzip' in method_name:
            with open(raw_tar_path, 'rb') as f_in, open(compressed_path, 'wb') as f_out:
                subprocess.run(cmd_args, stdin=f_in, stdout=f_out, check=True)
        elif 'brotli' in method_name:
             with open(raw_tar_path, 'rb') as f_in, open(compressed_path, 'wb') as f_out:
                subprocess.run(cmd_args, stdin=f_in, stdout=f_out, check=True)
        else:
            # zstd å’Œ lz4 æ”¯æŒç›´æŽ¥æ–‡ä»¶å‚æ•°
            subprocess.run(cmd_args + [raw_tar_path, '-o', compressed_path], check=True)
            
        compressed_size = os.path.getsize(compressed_path)
        
        # æ¸…ç†åŽŸå§‹tarï¼Œåªä¿ç•™åŽ‹ç¼©åŒ…
        if os.path.exists(raw_tar_path):
            os.remove(raw_tar_path)
            
        return compressed_path, original_size, compressed_size


    def setup_client_container(self, profile_name):
        """å¯åŠ¨ç‰¹å®šé…ç½®çš„å®¢æˆ·ç«¯å®¹å™¨"""
        config = CLIENT_PROFILES[profile_name]
        container_name = f"cts_worker_{profile_name}"
        
        # æ¸…ç†æ—§å®¹å™¨
        try:
            old = self.docker_client.containers.get(container_name)
            old.remove(force=True)
        except docker.errors.NotFound:
            pass

        # å¯åŠ¨æ–°å®¹å™¨ (åº”ç”¨ CPU/Mem é™åˆ¶)
        # å¯åŠ¨æ–°å®¹å™¨ (åº”ç”¨ CPU/Mem é™åˆ¶ + ç½‘ç»œæƒé™)
        container = self.docker_client.containers.run(
            CLIENT_IMAGE,
            name=container_name,
            detach=True,
            tty=True,
            nano_cpus=int(config['cpu'] * 1e9),
            mem_limit=config['mem'],
            # === ã€æ–°å¢žä¸‹é¢è¿™ä¸€è¡Œã€‘ ===
            cap_add=['NET_ADMIN'], 
            # =========================
            volumes={TEMP_DIR: {'bind': '/data', 'mode': 'rw'}}, 
            command="tail -f /dev/null"
        )
        
        # åº”ç”¨ç½‘ç»œä»¿çœŸ (Pumba)
        # æ³¨æ„: éœ€è¦åœ¨å®¿ä¸»æœºå®‰è£… pumba äºŒè¿›åˆ¶æ–‡ä»¶
        logger.info(f"åº”ç”¨ç½‘ç»œé™åˆ¶ ({profile_name}): BW={config['bw']}, Delay={config['delay']}")
        pumba_cmd = [
            "pumba", "netem",
            "--interface", "eth0",
            "--duration", "5m", 
            "rate", "--rate", config['bw'],
            "delay", "--time", config['delay'], "--jitter", "5ms", "--correlation", "0",
            container_name
        ]
        subprocess.run(pumba_cmd, check=True)
        
        return container



    def run_agent_in_container(self, container, compressed_file, method_name):
        """åœ¨å®¹å™¨å†…æ‰§è¡Œè§£åŽ‹æµ‹è¯•"""
        filename = os.path.basename(compressed_file)
        container_path = f"/data/{filename}"
        
        # === æ ¸å¿ƒä¿®æ”¹å¼€å§‹ï¼šå‚æ•°æ¸…æ´— ===
        # ç›®çš„ï¼šæŠŠ 'gzip-1' å˜æˆ 'gzip'ï¼ŒæŠŠ 'lz4-slow' å˜æˆ 'lz4'
        if 'lz4' in method_name:
            base_method = 'lz4'
        elif 'brotli' in method_name:
            base_method = 'brotli'
        else:
            # é’ˆå¯¹ gzip-1, zstd-3 è¿™ç§æ ¼å¼ï¼Œå–æ¨ªæ å‰çš„éƒ¨åˆ†
            base_method = method_name.split('-')[0]
        # === æ ¸å¿ƒä¿®æ”¹ç»“æŸ ===

        # æž„é€ å®¹å™¨å†…å‘½ä»¤
        cmd = f"python3 /app/client_agent.py --file {container_path} --method {base_method}"
        
        # æ‰§è¡Œå‘½ä»¤
        exec_result = container.exec_run(cmd)
        output = exec_result.output.decode('utf-8')
        
        if exec_result.exit_code != 0:
            logger.error(f"Agent Error Output: {output}")
            raise Exception(f"Agent Execution Failed: {output}")
            
        try:
            json_str = output.strip().split('\n')[-1]
            return json.loads(json_str)
        except json.JSONDecodeError:
            raise Exception(f"Invalid JSON output: {output}")

    def save_result(self, image, profile, method, rep, data, error=None):
        """ä¿å­˜ç»“æžœåˆ°æ•°æ®åº“"""
        is_noise = False
        status = 'SUCCESS'
        
        # ... (å‰ç•¥)
        if error:
            status = 'FAILED'
        else:
            # === ä¿®æ”¹è¿™é‡Œ ===
            # åŽŸä»£ç : target_bw_mbps = float(CLIENT_PROFILES[profile]['bw'].replace('m', '')) 
            # æ–°ä»£ç : åŽ»æŽ‰ 'mbit' åŽè½¬æµ®ç‚¹æ•°
            target_bw_mbps = float(CLIENT_PROFILES[profile]['bw'].replace('mbit', '')) 
            
            measured_bw = data.get('bandwidth_measured', 0)
            # ... (åŽç•¥)
            
            # 1. å¸¦å®½åå·®æ£€æŸ¥ (>50%)
            if abs(measured_bw - target_bw_mbps) / target_bw_mbps > 0.5:
                is_noise = True
                status = 'ABNORMAL'
            
            # 2. è§£åŽ‹æ—¶é—´è¿‡çŸ­ (<10ms)
            if data.get('decomp_time', 0) < 0.01:
                is_noise = True
                status = 'ABNORMAL'

        cursor = self.conn.cursor()
        cursor.execute('''
            INSERT OR REPLACE INTO experiments 
            (image, client_profile, method, rep_id, status, download_time, decomp_time, 
             total_time, cpu_usage, mem_usage, compressed_size, original_size, 
             bandwidth_measured, is_noise, error_msg)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            image, profile, method, rep, status,
            data.get('download_time', 0), data.get('decomp_time', 0),
            data.get('total_time', 0), data.get('cpu_usage', 0),
            data.get('mem_usage', 0), data.get('compressed_size', 0),
            data.get('original_size', 0), data.get('bandwidth_measured', 0),
            is_noise, str(error) if error else None
        ))
        self.conn.commit()
        logger.info(f"å®žéªŒç»“æžœå·²ä¿å­˜: {status}")

    def run_matrix(self):
        """æ‰§è¡Œå®Œæ•´å®žéªŒçŸ©é˜µ"""
        logger.info(f"å¼€å§‹è¿è¡Œå®žéªŒçŸ©é˜µ: {len(TARGET_IMAGES)}é•œåƒ x {len(CLIENT_PROFILES)}å®¢æˆ·ç«¯ x {len(COMPRESSION_METHODS)}ç®—æ³•")
        
        # 1. å¤–å±‚å¾ªçŽ¯ï¼šé•œåƒ (æœ€è€—æ—¶çš„èµ„æºï¼Œå°½é‡å°‘åˆ‡æ¢)
        for image in TARGET_IMAGES:
            try:
                # 2. ä¸­å±‚å¾ªçŽ¯ï¼šå®¢æˆ·ç«¯ç”»åƒ
                for profile_name in CLIENT_PROFILES.keys():
                    
                    container = None
                    try:
                        # å¯åŠ¨ç‰¹å®šçŽ¯å¢ƒçš„å®¹å™¨
                        container = self.setup_client_container(profile_name)
                        
                        # 3. å†…å±‚å¾ªçŽ¯ï¼šåŽ‹ç¼©ç®—æ³•
                        for method in COMPRESSION_METHODS.keys():
                            
                            # å‡†å¤‡æ•°æ® payload (å®¿ä¸»æœºåŽ‹ç¼©)
                            # ä¼˜åŒ–: å¯ä»¥åœ¨Repå¾ªçŽ¯å¤–åšï¼Œä½†ä¸ºäº†æ¨¡æ‹Ÿæ¯æ¬¡è¯·æ±‚ï¼Œæ”¾åœ¨è¿™é‡Œ
                            comp_path, orig_size, comp_size = self.prepare_image_payload(image, method)
                            
                            # 4. é‡å¤å®žéªŒ
                            for rep in range(REPETITIONS):
                                if self.is_experiment_done(image, profile_name, method, rep):
                                    logger.info(f"â­ï¸ è·³è¿‡å·²å®Œæˆå®žéªŒ: {image} | {profile_name} | {method} | Rep{rep}")
                                    continue
                                
                                logger.info(f"â–¶ï¸ æ‰§è¡Œå®žéªŒ: {image} | {profile_name} | {method} | Rep{rep}")
                                self._clear_system_cache()
                                
                                try:
                                    # æ‰§è¡Œæ ¸å¿ƒæµ‹è¯•
                                    result_data = self.run_agent_in_container(container, comp_path, method)
                                    
                                    # è¡¥å……å®¿ä¸»æœºå·²çŸ¥çš„æ•°æ®
                                    result_data['original_size'] = orig_size
                                    result_data['compressed_size'] = comp_size
                                    
                                    self.save_result(image, profile_name, method, rep, result_data)
                                    
                                except Exception as e:
                                    logger.error(f"âŒ å®žéªŒå¤±è´¥: {e}")
                                    self.save_result(image, profile_name, method, rep, {}, error=e)
                                
                                time.sleep(1) # å†·å´
                            
                            # æ¸…ç†å½“æ¬¡åŽ‹ç¼©æ–‡ä»¶
                            if os.path.exists(comp_path):
                                os.remove(comp_path)
                                
                    finally:
                        if container:
                            container.remove(force=True)
                            
                # é•œåƒå±‚çº§æ¸…ç†: å®Œæˆä¸€ä¸ªé•œåƒçš„æ‰€æœ‰å®žéªŒåŽï¼Œåˆ é™¤æœ¬åœ°é•œåƒä»¥é‡Šæ”¾ç©ºé—´
                self.docker_client.images.remove(image, force=True)
                logger.info(f"ðŸ§¹ æ¸…ç†æœ¬åœ°é•œåƒ: {image}")
                
            except Exception as e:
                logger.critical(f"ðŸ”¥ é•œåƒå±‚çº§ä¸¥é‡é”™è¯¯ ({image}): {e}")

if __name__ == "__main__":
    if os.geteuid() != 0:
        logger.warning("å»ºè®®ä»¥ root æƒé™è¿è¡Œï¼Œå¦åˆ™ Pumba å’Œ ç¼“å­˜æ¸…ç† å¯èƒ½å¤±æ•ˆã€‚")
    
    orchestrator = ExperimentOrchestrator()
    orchestrator.run_matrix()